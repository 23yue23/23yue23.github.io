<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  
    
      
    

    
  

  
    
    
    <link href="https://fonts.cat.net/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="spark-调优,">





  <link rel="alternate" href="/atom.xml" title="蜗牛笔记" type="application/atom+xml">






<meta name="description" content="性能调优分配资源：123456789101112131415161718192021222324252627spark-submit \--class cn.spark.sparktest.core.WordCountCluster \--num-executors 80 \  配置executor的数量--driver-memory 6g \  配置driver的内存（影响不大）--execut">
<meta name="keywords" content="spark-调优">
<meta property="og:type" content="article">
<meta property="og:title" content="spark系列之--调优记录">
<meta property="og:url" content="https://23yue23.github.io/2019/04/28/spark系列之-调优记录/index.html">
<meta property="og:site_name" content="蜗牛笔记">
<meta property="og:description" content="性能调优分配资源：123456789101112131415161718192021222324252627spark-submit \--class cn.spark.sparktest.core.WordCountCluster \--num-executors 80 \  配置executor的数量--driver-memory 6g \  配置driver的内存（影响不大）--execut">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-04-29T08:32:42.494Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark系列之--调优记录">
<meta name="twitter:description" content="性能调优分配资源：123456789101112131415161718192021222324252627spark-submit \--class cn.spark.sparktest.core.WordCountCluster \--num-executors 80 \  配置executor的数量--driver-memory 6g \  配置driver的内存（影响不大）--execut">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://23yue23.github.io/2019/04/28/spark系列之-调优记录/">





  <title>spark系列之--调优记录 | 蜗牛笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">蜗牛笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">骑士的心</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://23yue23.github.io/2019/04/28/spark系列之-调优记录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Brady">
      <meta itemprop="description" content>
      <meta itemprop="image" content="http://static.oschina.net/uploads/space/2015/0629/170157_AOda_1767531.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="蜗牛笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">spark系列之--调优记录</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-28T14:23:46+08:00">
                2019-04-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  5.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  24
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h3><h4 id="分配资源："><a href="#分配资源：" class="headerlink" title="分配资源："></a>分配资源：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class cn.spark.sparktest.core.WordCountCluster \</span><br><span class="line">--num-executors 80 \  配置executor的数量</span><br><span class="line">--driver-memory 6g \  配置driver的内存（影响不大）</span><br><span class="line">--executor-memory 6g \  配置每个executor的内存大小</span><br><span class="line">--executor-cores 3 \  配置每个executor的cpu core数量(RDD cache/shuffle/task执行)</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--queue root.default \</span><br><span class="line">--conf spark.yarn.executor.memoryOverhead=2048 \  executor堆外内存</span><br><span class="line">--conf spark.core.connection.ack.wait.timeout=300 \ 连接的超时时长</span><br><span class="line">/usr/local/spark/spark.jar \</span><br><span class="line">$&#123;1&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-submit \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-cores 3 \</span><br><span class="line">  --executor-memory 10G \</span><br><span class="line">  --driver-memory 4G \</span><br><span class="line">  --conf spark.dynamicAllocation.enabled=true \</span><br><span class="line">  --conf spark.shuffle.service.enabled=true \</span><br><span class="line">  --conf spark.dynamicAllocation.initialExecutors=5 \</span><br><span class="line">  --conf spark.dynamicAllocation.maxExecutors=40 \</span><br><span class="line">  --conf spark.dynamicAllocation.minExecutors=0 \</span><br><span class="line">  --conf spark.dynamicAllocation.executorIdleTimeout=30s \</span><br><span class="line">  --conf spark.dynamicAllocation.schedulerBacklogTimeout=10s \</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="SparkStreaming-优雅退出"><a href="#SparkStreaming-优雅退出" class="headerlink" title="SparkStreaming 优雅退出"></a>SparkStreaming 优雅退出</h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">		Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR);</span><br><span class="line">//String checkpointPath = PropertiesUtil.getProperty(&quot;streaming.checkpoint.path&quot;);</span><br><span class="line">		JavaStreamingContext javaStreamingContext = JavaStreamingContext.getOrCreate(&quot;hdfs://Master:9000/streaming_checkpoint&quot;, createContext());</span><br><span class="line">		javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">		每隔20秒钟监控是否有停止指令,如果有则优雅退出streaming</span><br><span class="line">		final Properties serverProps = PropertiesUtil.properties;</span><br><span class="line">		Thread thread = new Thread(new MonitorStopThread(javaStreamingContext,serverProps));</span><br><span class="line">		thread.start();</span><br><span class="line">		javaStreamingContext.awaitTermination();</span><br><span class="line">	   &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="调节并行度："><a href="#调节并行度：" class="headerlink" title="调节并行度："></a>调节并行度：</h4><blockquote>
<ol>
<li>并行度：其实就是指的是，Spark作业中，各个stage的task数量，也就代表了Spark作业的在各个阶段（stage）的并行度。</li>
<li>官方是推荐，task数量，设置成spark application总cpu core数量的2~3倍，比如150个cpu core，基本要设置task数量为300~500；</li>
<li>SparkConf conf = new SparkConf().set(“spark.default.parallelism”, “500”)</li>
</ol>
</blockquote>
<hr>
<h4 id="InputDStream并行化数据接收"><a href="#InputDStream并行化数据接收" class="headerlink" title="InputDStream并行化数据接收"></a>InputDStream并行化数据接收</h4><blockquote>
<p>创建多个InputDStream来接收同一数据源,把多个topic数据细化为单一的kafkaStream来接收</p>
<blockquote>
<ol>
<li>创建kafkaStream</li>
</ol>
</blockquote>
</blockquote>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;();</span><br><span class="line"> kafkaParams.put(&quot;metadata.broker.list&quot;, &quot;192.168.1.164:9092,192.168.1.165:9092,192.168.1.166:9092&quot;);</span><br><span class="line"> kafkaParams.put(&quot;zookeeper.connect&quot;,&quot;master:2181,data1:2181,data2:2181&quot;);</span><br><span class="line"> </span><br><span class="line"> 构建topic set</span><br><span class="line"> String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);</span><br><span class="line"> String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line"> Set&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line"> for(String kafkaTopic : kafkaTopicsSplited) &#123;</span><br><span class="line"> 	topics.add(kafkaTopic);</span><br><span class="line"> 	</span><br><span class="line"> JavaPairInputDStream&lt;String, String&gt; kafkaStream = KafkaUtils.createDirectStream(</span><br><span class="line"> 	jssc, </span><br><span class="line"> 	String.class, </span><br><span class="line"> 	String.class, </span><br><span class="line"> 	StringDecoder.class, </span><br><span class="line"> 	StringDecoder.class, </span><br><span class="line"> 	kafkaParams, </span><br><span class="line"> 	topics);</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<ol start="2">
<li>InputDStream并行化数据接收</li>
</ol>
</blockquote>
</blockquote>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> int numStreams = 5;</span><br><span class="line"> List&lt;JavaPairDStream&lt;String, String&gt;&gt; kafkaStreams = new</span><br><span class="line"> ArrayList&lt;JavaPairDStream&lt;String,String&gt;&gt;(numStreams);</span><br><span class="line"> for (int i = 0; i &lt; numStreams; i++) &#123;</span><br><span class="line"> 	kafkaStreams.add(KafkaUtils.createStream(...));</span><br><span class="line">	 &#125;</span><br><span class="line"> JavaPairDStream&lt;String, String&gt; unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));</span><br><span class="line">unifiedStream.print();</span><br></pre></td></tr></table></figure>
</code></pre><hr>
<h4 id="增加block数量，增加每个batch-rdd的partition数量，增加处理并行度"><a href="#增加block数量，增加每个batch-rdd的partition数量，增加处理并行度" class="headerlink" title="增加block数量，增加每个batch rdd的partition数量，增加处理并行度"></a>增加block数量，增加每个batch rdd的partition数量，增加处理并行度</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">第一步：receiver从数据源源源不断地获取到数据，首先是会按照block interval，将指定时间间隔的数据，收集为一个block；默认时间是200ms，官方推荐不要小于50ms；</span><br><span class="line">第二步：根据指定batch interval时间间隔合并为一个batch，创建为一个rdd，</span><br><span class="line">第三步：启动一个job，去处理这个batch rdd中的数据。</span><br><span class="line">第四步：batch rdd 的partition数量是多少呢？一个batch有多少个block，就有多少个partition；就意味着并行度是多少；就意味着每个batch rdd有多少个task会并行计算和处理。</span><br><span class="line">调优：如果希望可以比默认的task数量和并行度再多一些，可以手动调节blockinterval，减少block interval。每个batch可以包含更多的block。因此也就有更多的partition，因此就会有更多的task并行处理每个batch rdd。</span><br></pre></td></tr></table></figure>
<h4 id="重分区，增加每个batch-rdd的partition数量"><a href="#重分区，增加每个batch-rdd的partition数量" class="headerlink" title="重分区，增加每个batch rdd的partition数量"></a>重分区，增加每个batch rdd的partition数量</h4><p>inputStream.repartition()：重分区，增加每个batch rdd的partition数量<br>对dstream中的rdd进行重分区为指定数量的分区，就可以提高指定dstream的rdd的计算并行度<br>调节并行度</p>
<hr>
<h4 id="重构RDD架构以及RDD持久化："><a href="#重构RDD架构以及RDD持久化：" class="headerlink" title="重构RDD架构以及RDD持久化："></a>重构RDD架构以及RDD持久化：</h4><blockquote>
<ol>
<li>RDD架构重构与优化</li>
<li>公共RDD一定要实现持久化,对于要多次计算和使用的公共RDD，一定要进行持久化。</li>
<li>持久化，是可以进行序列化的<br>sessionid2actionRDD=sessionid2actionRDD.persist(StorageLevel.MEMORY_ONLY());<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MEMORY_ONLY    直接以Java对象的形式存储于JVM的内存中</span><br><span class="line">MYMORY_AND_DISK    存储于JVM的内存+磁盘</span><br><span class="line">MEMORY_ONLY_SER    序列化存储于内存中</span><br><span class="line">MEMORY_AND_DISK_SER    序列化存储于内存+磁盘</span><br></pre></td></tr></table></figure>
</code></pre></li>
</ol>
</blockquote>
<blockquote>
<ol start="4">
<li>为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化</li>
</ol>
</blockquote>
<hr>
<h4 id="实现RDD高可用性：启动WAL预写日志机制"><a href="#实现RDD高可用性：启动WAL预写日志机制" class="headerlink" title="实现RDD高可用性：启动WAL预写日志机制"></a>实现RDD高可用性：启动WAL预写日志机制</h4><blockquote>
<p>spark streaming，从原理上来说，是通过receiver来进行数据接收的；接收到的数据，会被划分成一个一个的block；block会被组合成一个batch；针对一个batch，会创建一个rdd；<br>receiver接收到数据后，就会立即将数据写入一份到容错文件系统（比如hdfs）上的checkpoint目录中的，另一份写入到磁盘文件中去；作为数据的冗余副本。无论你的程序怎么挂掉，或者是数据丢失，那么数据都不肯能会永久性的丢失；因为肯定有副本。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf()       </span><br><span class="line">    		.setMaster(&quot;local[2]&quot;)</span><br><span class="line">    		.setAppName(&quot;StreamingSpark&quot;);</span><br><span class="line">	.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;);</span><br><span class="line">   		.set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);</span><br><span class="line">    		.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;);    </span><br><span class="line">    		.set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);   </span><br><span class="line">JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5)); </span><br><span class="line">jssc.checkpoint(&quot;hdfs://192.168.1.164:9000/checkpoint&quot;);</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="广播大变量（1m-100m）："><a href="#广播大变量（1m-100m）：" class="headerlink" title="广播大变量（1m~100m）："></a>广播大变量（1m~100m）：</h4><blockquote>
<ol>
<li>默认的情况下，task执行的算子中，使用了外部的变量，每个task都会获取一份变量的副本，有什么缺点呢？<pre><code>网络传输的开销、耗费内存、RDD持久化到内存（内存不够，持续到磁盘）、task创建对象导致gc；
</code></pre></li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li>广播变量，初始的时候，就在Drvier上有一份副本。</li>
</ol>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；</span><br><span class="line">如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中；</span><br><span class="line">此后这个executor上的task，都会直接使用本地的BlockManager中的副本。</span><br><span class="line">executor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，举例越近越好。</span><br><span class="line">sc.boradcast();</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="使用Kryo序列化"><a href="#使用Kryo序列化" class="headerlink" title="使用Kryo序列化:"></a>使用Kryo序列化:</h4><blockquote>
<ol>
<li>默认情况下，Spark内部是使用Java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。</li>
<li>Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。</li>
<li>Kryo序列化机制，一旦启用以后，会生效的几个地方：<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、算子函数中使用到的外部变量</span><br><span class="line">2、持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER</span><br><span class="line">3、shuffle</span><br><span class="line">   .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">   .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);</span><br><span class="line">.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;);    </span><br><span class="line">.set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);   </span><br><span class="line">   .registerKryoClasses(new Class[]&#123;CategorySortKey.class&#125;)</span><br></pre></td></tr></table></figure>
</code></pre></li>
</ol>
</blockquote>
<blockquote>
<ol start="4">
<li>序列化<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2、注册你使用到的，需要通过Kryo序列化的，一些自定义类，SparkConf.registerKryoClasses()</span><br></pre></td></tr></table></figure>
</code></pre></li>
</ol>
</blockquote>
<hr>
<h4 id="使用fastutil优化数据格式"><a href="#使用fastutil优化数据格式" class="headerlink" title="使用fastutil优化数据格式:"></a>使用fastutil优化数据格式:</h4><blockquote>
<ol>
<li>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；</li>
<li>fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度；</li>
</ol>
</blockquote>
<p>fastutil也提供了64位的array、set和list，以及高性能快速的，以及实用的IO类，来处理二进制和文本类型的文件；fastutil最新版本要求Java 7以及以上版本；</p>
<p>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</p>
<p>fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。<br>fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。</p>
<blockquote>
<ol start="3">
<li>maven 依赖<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;fastutil&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;fastutil&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;5.0.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<hr>
<h4 id="调节数据本地化等待时长："><a href="#调节数据本地化等待时长：" class="headerlink" title="调节数据本地化等待时长："></a>调节数据本地化等待时长：</h4><blockquote>
<ol>
<li>PROCESS_LOCAL：进程本地化；NODE_LOCAL：节点本地化；NO_PREF：对于task来说，没有好坏之分；RACK_LOCAL：机架本地化；ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差；</li>
<li>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL（不用调节）、NODE LOCAL、ANY（调节一下数据本地化的等待时长），反复调节，每次调节完以后，再来运行，观察日志</li>
<li>怎么调节？<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait，默认是3s；6s，10s</span><br><span class="line">默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</span><br><span class="line">spark.locality.wait.process</span><br><span class="line">spark.locality.wait.node</span><br><span class="line">spark.locality.wait.rack</span><br><span class="line">new SparkConf()</span><br><span class="line">  .set(&quot;spark.locality.wait&quot;, &quot;10&quot;)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<hr>
<h4 id="定时清除不需要的数据"><a href="#定时清除不需要的数据" class="headerlink" title="定时清除不需要的数据"></a>定时清除不需要的数据</h4><blockquote>
<ol>
<li>通过配置spark.cleaner.ttl为一个合理的值，但是这个值不能过小，因为如果后面计算需要用的数据被清除会带来不必要的麻烦。</li>
<li>另外通过配置spark.streaming.unpersist为true(默认就是true)来更智能地去持久化（unpersist）RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为。</li>
</ol>
</blockquote>
<hr>
<h4 id="去除压缩-内存充足的情况下"><a href="#去除压缩-内存充足的情况下" class="headerlink" title="去除压缩 (内存充足的情况下)"></a>去除压缩 (内存充足的情况下)</h4><p>在内存充足的情况下，可以设置spark.rdd.compress 设置为false.</p>
<hr>
<h3 id="Yarn-优化"><a href="#Yarn-优化" class="headerlink" title="Yarn 优化"></a>Yarn 优化</h3><h4 id="Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配"><a href="#Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配" class="headerlink" title="Executors和cpu核心数设置和Spark On Yarn 动态资源分配"></a>Executors和cpu核心数设置和Spark On Yarn 动态资源分配</h4><blockquote>
<p>首先需要对YARN的NodeManager进行配置，使其支持Spark的Shuffle Service。</p>
</blockquote>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#修改</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">#增加</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.shuffle.service.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;7337&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>将spark中对应jar包拷贝到hadoop的目录下：<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先找到spark版本的spark-&lt;version&gt;-yarn-shuffle.jar</span><br><span class="line">shuffle包，并将该包放到集群所有NodeManager的classpath下，</span><br><span class="line">比如放到HADOOP_HOME/share/hadoop/yarn/lib</span><br></pre></td></tr></table></figure></p>
</blockquote>
<hr>
<h3 id="JVM-调优"><a href="#JVM-调优" class="headerlink" title="JVM 调优"></a>JVM 调优</h3><h4 id="原理概述以及降低cache操作的内存占比"><a href="#原理概述以及降低cache操作的内存占比" class="headerlink" title="原理概述以及降低cache操作的内存占比:"></a>原理概述以及降低cache操作的内存占比:</h4><blockquote>
<ol>
<li>full gc / minor gc，无论是快，还是慢，都会导致jvm的工作线程停止工作，stop the world。简而言之，就是说，gc的时候，spark停止工作了。等着垃圾回收结束。</li>
<li>spark中，堆内存又被划分成了两块儿，存储内存和执行内存；<pre><code>一句话，让task执行算子函数时，有更多的内存可以使用。
</code></pre></li>
</ol>
</blockquote>
<hr>
<h4 id="GC优化策略-暂时不确定"><a href="#GC优化策略-暂时不确定" class="headerlink" title="GC优化策略(暂时不确定)"></a>GC优化策略(暂时不确定)</h4><p>建议用并行Mark-Sweep垃圾回收机制，虽然它消耗更多的资源，但是我们还是建议开启。<br>在spark-submit中使用<br>–driver-java-options “-XX:+UseConcMarkSweepGC”<br>–conf “spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC”</p>
<h4 id="调节executor堆外内存与连接等待时长-在spark-sbmit中修改-："><a href="#调节executor堆外内存与连接等待时长-在spark-sbmit中修改-：" class="headerlink" title="调节executor堆外内存与连接等待时长(在spark-sbmit中修改)："></a>调节executor堆外内存与连接等待时长(在spark-sbmit中修改)：</h4><blockquote>
<ol>
<li>有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行，时不时的报错，shuffle file cannot find，executor、task lost，out of memory（内存溢出）；–conf spark.yarn.executor.memoryOverhead=2048</li>
<li>有时候，无法建立网络连接；会卡住；ok，spark默认的网络连接的超时时长，是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了:一串file id。uuid（dsfsfd-2342vs–sdf–sdfsd）。not found。file lost。<br> –conf spark.core.connection.ack.wait.timeout=300</li>
</ol>
</blockquote>
<hr>
<h3 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h3><h4 id="原理概述："><a href="#原理概述：" class="headerlink" title="原理概述："></a>原理概述：</h4><blockquote>
<ol>
<li>在spark中，主要是以下几个算子：groupByKey、reduceByKey、countByKey、join，等等。</li>
<li>shuffle，一定是分为两个stage来完成的。因为这其实是个逆向的过程，不是stage决定shuffle，是shuffle决定stage。</li>
<li>shuffle前半部分的task在写入数据到磁盘文件之前，都会先写入一个一个的内存缓冲，内存缓冲满溢之后，再spill溢写到磁盘文件中。</li>
</ol>
</blockquote>
<hr>
<h4 id="合并map端输出文件："><a href="#合并map端输出文件：" class="headerlink" title="合并map端输出文件："></a>合并map端输出文件：</h4><blockquote>
<ol>
<li>开启shuffle map端输出文件合并的机制；默认情况下，是不开启的，就是会发生如上所述的大量map端输出文件的操作，严重影响性能。</li>
<li>new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)<br>new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)</li>
</ol>
</blockquote>
<hr>
<h4 id="合并map端输出文件：-1"><a href="#合并map端输出文件：-1" class="headerlink" title="合并map端输出文件："></a>合并map端输出文件：</h4><blockquote>
<ol>
<li>map端内存缓冲：spark.shuffle.file.buffer，默认32k  <pre><code>reduce端内存占比：spark.shuffle.memoryFraction，0.2
</code></pre></li>
<li>调节的时候的原则。spark.shuffle.file.buffer，每次扩大一倍，然后看看效果，64，128；<br>spark.shuffle.memoryFraction，每次提高0.1，看看效果。<br>不能调节的太大，太大了以后过犹不及，因为内存资源是有限的，你这里调节的太大了，其他环节的内存使用就会有问题了。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new SparkConf().set(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)</span><br><span class="line">new SparkConf().set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.3&quot;)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<hr>
<h4 id="HashShuffleManager与SortShuffleManager"><a href="#HashShuffleManager与SortShuffleManager" class="headerlink" title="HashShuffleManager与SortShuffleManager"></a>HashShuffleManager与SortShuffleManager</h4><blockquote>
<ol>
<li>spark.shuffle.manager：hash、sort、tungsten-sort（自己实现内存管理），spark 1.2.x版本以后，默认的shuffle manager，是SortShuffleManager。<br>   spark.shuffle.sort.bypassMergeThreshold：200（默认值为200）</li>
<li>SortShuffleManager会避免像HashShuffleManager那样，默认就去创建多份磁盘文件。一个task，只会写入一个磁盘文件，不同reduce task的数据，用offset来划分界定。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)</span><br><span class="line">new SparkConf().set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<hr>
<h3 id="算子调优"><a href="#算子调优" class="headerlink" title="算子调优"></a>算子调优</h3><h4 id="MapPartitions提升Map类操作性能"><a href="#MapPartitions提升Map类操作性能" class="headerlink" title="MapPartitions提升Map类操作性能:"></a>MapPartitions提升Map类操作性能:</h4><blockquote>
<ol>
<li>如果是普通的map，比如一个partition中有1万条数据；function要执行和计算1万次。但是，使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。</li>
<li>但是，可能就OOM，内存溢出。   </li>
</ol>
</blockquote>
<hr>
<h4 id="filter过后使用coalesce减少分区数量："><a href="#filter过后使用coalesce减少分区数量：" class="headerlink" title="filter过后使用coalesce减少分区数量："></a>filter过后使用coalesce减少分区数量：</h4><blockquote>
<ol>
<li>就会导致有些task运行的速度很快；有些task运行的速度很慢。这就是数据倾斜。</li>
<li>coalesce算子：主要就是用于在filter操作之后，针对每个partition的数据量各不相同的情况，来压缩partition的数量。减少partition的数量，而且让每个partition的数据量都尽量均匀紧凑。</li>
</ol>
</blockquote>
<hr>
<h4 id="foreachPartition优化写数据库性能："><a href="#foreachPartition优化写数据库性能：" class="headerlink" title="foreachPartition优化写数据库性能："></a>foreachPartition优化写数据库性能：</h4><pre><code>&gt; 1. 用了foreachPartition算子之后，好处在哪里？
    1、对于我们写的function函数，就调用一次，一次传入一个partition所有数据；
    2、主要创建或者获取一个数据库连接就可以；
    3、只要向数据库发送一次SQL语句和多组参数即可；
&gt; 2. 很有可能会发生OOM，内存溢出的问题。
    一个partition大概是1千条左右用foreach，跟用foreachPartition，性能的提升达到了2~3分钟。
</code></pre><hr>
<h4 id="repartition解决Spark-SQL低并行度的性能问题："><a href="#repartition解决Spark-SQL低并行度的性能问题：" class="headerlink" title="repartition解决Spark SQL低并行度的性能问题："></a>repartition解决Spark SQL低并行度的性能问题：</h4><pre><code>repartition算子，你用Spark SQL这一步的并行度和task数量，肯定是没有办法去改变了。但是呢，可以将你用Spark SQL查询出来的RDD，使用repartition算子，去重新进行分区，此时可以分区成多个partition，比如从20个partition，分区成100个。
</code></pre><hr>
<h4 id="reduceByKey本地聚合介绍："><a href="#reduceByKey本地聚合介绍：" class="headerlink" title="reduceByKey本地聚合介绍："></a>reduceByKey本地聚合介绍：</h4><pre><code>reduceByKey，相较于普通的shuffle操作（比如groupByKey），它的一个特点，就是说，会进行map端的本地聚合
</code></pre><h3 id="代码-调优"><a href="#代码-调优" class="headerlink" title="代码 调优"></a>代码 调优</h3><h4 id="进行HA机制处理-针对Driver高可用性"><a href="#进行HA机制处理-针对Driver高可用性" class="headerlink" title="进行HA机制处理-针对Driver高可用性"></a>进行HA机制处理-针对Driver高可用性</h4><blockquote>
<p>在创建和启动StreamingContext的时候，将元数据写入容错的文件系统（比如hdfs）。保证在driver挂掉之后，spark集群可以自己将driver重新启动起来；而且driver在启动的时候，不会重新创建一个streaming context，而是从容错文件系统（比如hdfs）中读取之前的元数据信息，包括job的执行进度，继续接着之前的进度，继续执行。使用这种机制，就必须使用cluster模式提交，确保driver运行在某个worker上面；</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() &#123;</span><br><span class="line">     @Override</span><br><span class="line">    public JavaStreamingContext create() &#123;</span><br><span class="line">      	JavaStreamingContext jssc = new JavaStreamingContext(...);</span><br><span class="line">     	JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...);</span><br><span class="line">     	jssc.checkpoint(checkpointDirectory);</span><br><span class="line">     	return jssc;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;;</span><br><span class="line">JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);</span><br><span class="line">context.start();</span><br><span class="line">context.awaitTermination();</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">JavaStreamingContext.getOrCreate 基于Function0&lt; JavaStreamingContext &gt; 进行Driver高可用</span><br><span class="line"> Function0&lt;JavaStreamingContext&gt; createContextFunc = new Function0&lt;JavaStreamingContext&gt;()&#123;</span><br><span class="line"> 	@Override</span><br><span class="line"> 	public JavaStreamingContext call() throws Exception</span><br><span class="line"> 	&#123;</span><br><span class="line"> 		conf = new SparkConf()</span><br><span class="line"> 				.setMaster(&quot;local[4]&quot;)</span><br><span class="line"> 				.setAppName(&quot;java/RealTimeStreaming&quot;)</span><br><span class="line"> 				.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line"> 				.set(&quot;spark.default.parallelism&quot;, &quot;10&quot;)</span><br><span class="line"> 				.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;)</span><br><span class="line"> 				.set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);</span><br><span class="line"> 		Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();</span><br><span class="line"> 		kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);</span><br><span class="line"> 		kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);</span><br><span class="line"> 		kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);</span><br><span class="line"> 		kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);</span><br><span class="line"> 		kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span><br><span class="line"> 		kafkaParams.put(&quot;enable.auto.commit&quot;,true);</span><br><span class="line"></span><br><span class="line"> 		JavaStreamingContext jssc = new JavaStreamingContext(</span><br><span class="line"> 				conf, Durations.seconds(30));</span><br><span class="line"> 		jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;);</span><br><span class="line"></span><br><span class="line"> 		// 构建topic set</span><br><span class="line"> 		String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);</span><br><span class="line"> 		String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);</span><br><span class="line"> 		Set&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line"> 		for(String kafkaTopic : kafkaTopicsSplited) &#123;</span><br><span class="line"> 			topics.add(kafkaTopic);</span><br><span class="line"> 		&#125;</span><br><span class="line"></span><br><span class="line"> 		JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc,</span><br><span class="line"> 				LocationStrategies.PreferConsistent(),</span><br><span class="line"> 				ConsumerStrategies.Subscribe(topics, kafkaParams));</span><br><span class="line"></span><br><span class="line"> 		hostMap = adRealTimeLogDStream.mapToPair(record -&gt; new Tuple2&lt;String, String&gt;(record.key(), record.value()));</span><br><span class="line"></span><br><span class="line"> 		logPeakDstream  = hostMap.mapToPair(new PairFunction&lt;Tuple2&lt;String, String&gt;, String, Long&gt;() &#123;</span><br><span class="line"> 			@Override</span><br><span class="line"> 			public Tuple2&lt;String,Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123;</span><br><span class="line"> 				String log = tuple._2;</span><br><span class="line"> 				String[] logSplited = log.split(&quot;\\|&quot;);</span><br><span class="line"> 				String eventTime= logSplited[1];</span><br><span class="line"> 				String todayDate = DATE_FORMAT.format(new Date()).trim();</span><br><span class="line"> 				String cutTime= eventTime.substring(13,eventTime.length()-7);</span><br><span class="line"></span><br><span class="line"> 				String ip = logSplited[0].trim();</span><br><span class="line"> 				String host = logSplited[14].trim();</span><br><span class="line"> 				return new Tuple2&lt;String, Long&gt;(host+&quot;-&quot;+ip, 1L);</span><br><span class="line"> 			&#125;</span><br><span class="line"> 		&#125;);</span><br><span class="line"></span><br><span class="line"> 		hostReduce = logPeakDstream.reduceByKeyAndWindow(new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line"> 			@Override</span><br><span class="line"> 			public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line"> 				return v1 + v2;</span><br><span class="line"> 			&#125;</span><br><span class="line"> 		&#125;, Durations.minutes(10),Durations.seconds(30));</span><br><span class="line"></span><br><span class="line"> 		JavaPairDStream&lt;String, Long&gt; topNPairRdd = hostReduce.transformToPair(new Function&lt;JavaPairRDD&lt;String, Long&gt;, JavaPairRDD&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line"> 			@Override</span><br><span class="line"> 			public JavaPairRDD&lt;String, Long&gt; call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception &#123;</span><br><span class="line"> 				JavaPairRDD&lt;Long, String&gt; sortRDD = (JavaPairRDD&lt;Long, String&gt;) rdd.mapToPair(record -&gt; new Tuple2&lt;Long, String&gt;(record._2, record._1));</span><br><span class="line"> 				JavaPairRDD&lt;String, Long&gt; sortedRdd = (JavaPairRDD&lt;String, Long&gt;) sortRDD.sortByKey(false).mapToPair(record -&gt; new Tuple2&lt;String, Long&gt;(record._2, record._1));</span><br><span class="line"></span><br><span class="line"> 				List&lt;Tuple2&lt;String, Long&gt;&gt; topNs = sortedRdd.take(5);//取前5个输出</span><br><span class="line"> 				System.out.println(&quot;                                                 &quot;);</span><br><span class="line"> 				System.out.println(&quot;*****************峰值访问窗统计*******************&quot;);</span><br><span class="line"> 				for (Tuple2&lt;String, Long&gt; topN : topNs) &#123;</span><br><span class="line"> 					System.out.println(topN);</span><br><span class="line"> 				&#125;</span><br><span class="line"> 				System.out.println(&quot;**********************END***********************&quot;);</span><br><span class="line"> 				System.out.println(&quot;                                                 &quot;);</span><br><span class="line"> 				return sortedRdd;</span><br><span class="line"> 			&#125;</span><br><span class="line"> 		&#125;);</span><br><span class="line"></span><br><span class="line"> 		topNPairRdd.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line"> 			@Override</span><br><span class="line"> 			public void call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception &#123;</span><br><span class="line"></span><br><span class="line"> 			&#125;</span><br><span class="line"> 		&#125;);</span><br><span class="line"></span><br><span class="line"> 		logDetailDstream = hostMap.map(new Function&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123;</span><br><span class="line"> 			@Override</span><br><span class="line"> 			public String call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123;</span><br><span class="line"> 				String log = tuple._2;</span><br><span class="line"> 				String[] logSplited = log.split(&quot;\\|&quot;);</span><br><span class="line"> 				String eventTime= logSplited[1];</span><br><span class="line"> 				String todayDate = DATE_FORMAT.format(new Date()).trim();</span><br><span class="line"> 				String cutTime= eventTime.substring(13,eventTime.length()-7);</span><br><span class="line"> 				String[] urlDetails = logSplited[7].split(&quot;/&quot;);</span><br><span class="line"> 				String ip = logSplited[0].trim();</span><br><span class="line"></span><br><span class="line"> 				String url =&quot;&quot;;</span><br><span class="line"> 				if(urlDetails.length==4)&#123;</span><br><span class="line"> 					url = urlDetails[3];</span><br><span class="line"> 				&#125;else if(urlDetails.length==5)&#123;</span><br><span class="line"> 					url = urlDetails[3] + &quot;/&quot; + urlDetails[4];</span><br><span class="line"> 				&#125;else if(urlDetails.length&gt;=6)&#123;</span><br><span class="line"> 					url = urlDetails[3] + &quot;/&quot; + urlDetails[4]+ &quot;/&quot; + urlDetails[5];</span><br><span class="line"> 				&#125;</span><br><span class="line"> 				String host = logSplited[14].trim();</span><br><span class="line"> 				String dataTime =todayDate +&quot; &quot;+ cutTime;</span><br><span class="line"> 				String bytesSent = logSplited[5].trim();</span><br><span class="line"> 				return  dataTime+&quot; &quot;+host+&quot; &quot;+ip+&quot; &quot;+url+&quot; &quot;+bytesSent;</span><br><span class="line"> 			&#125;</span><br><span class="line"> 		&#125;);</span><br><span class="line"> 		//logDetailDstream.print();</span><br><span class="line"> 		return jssc;</span><br><span class="line"> 	&#125;</span><br><span class="line"> &#125;;</span><br><span class="line"> return createContextFunc;</span><br></pre></td></tr></table></figure>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">提交方式</span><br><span class="line"> spark-submit</span><br><span class="line">         --deploy-mode cluster</span><br><span class="line">         --supervise</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="SparkStreaming-与kafka整合调优"><a href="#SparkStreaming-与kafka整合调优" class="headerlink" title="SparkStreaming 与kafka整合调优"></a>SparkStreaming 与kafka整合调优</h4><blockquote>
<p>LocationStrategies 位置策略：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The new Kafka consumer API will pre-fetch messages into buffers. Therefore it is important for performance </span><br><span class="line">reasons that the Spark integration keep cached consumers on executors (rather than recreating them for each </span><br><span class="line">batch), and prefer to schedule partitions on the host locations that have the appropriate consumers.</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>新的Kafka消费者API可以预获取消息缓存到缓冲区，因此Spark整合Kafka让消费者在executor上进行缓存对性能是非常有助的，可以调度消费者所在主机位置的分区。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In most cases, you should use LocationStrategies.PreferConsistent as shown above. This will distribute partitions </span><br><span class="line">evenly across available executors. If your executors are on the same hosts as your Kafka brokers, use PreferBrokers,</span><br><span class="line">which will prefer to schedule partitions on the Kafka leader for that partition. Finally, if you have a significant </span><br><span class="line">skew in load among partitions, use PreferFixed. This allows you to specify an explicit mapping of partitions to </span><br><span class="line">hosts (any unspecified partitions will use a consistent location).</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>通常，你可以使用 LocationStrategies.PreferConsistent，这个策略会将分区分布到所有可获得的executor上。如果你的executor和kafkabroker在同一主机上的话，可以使用PreferBrokers，这样kafka leader会为此分区进行调度。最后，如果你加载数据有倾斜的话可以使用PreferFixed，这将允许你制定一个分区和主机的映射（没有指定的分区将使用PreferConsistent 策略）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The cache for consumers has a default maximum size of 64. If you expect to be handling more than </span><br><span class="line">(64 * number of executors) Kafka partitions, you can change this setting</span><br><span class="line">via spark.streaming.kafka.consumer.cache.maxCapacity</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>消费者默认缓存大小是64，如果你期望处理较大的Kafka分区的话，你可以使用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.kafka.consumer.cache.maxCapacity设置大小。</span><br><span class="line">The cache is keyed by topicpartition and group.id, so use a separate group.id for each call to createDirectStream.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>缓存是使用key为topic partition 和组id的，因此对于每一次调用 createDirectStream 可以使用不同的 group . id</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public  static SparkConf  conf = new SparkConf()</span><br><span class="line">				.setMaster(&quot;local[4]&quot;)</span><br><span class="line">				.setAppName(&quot;java/RealTimeStreaming&quot;)</span><br><span class="line">				.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">				.set(&quot;spark.default.parallelism&quot;, &quot;10&quot;)</span><br><span class="line">				.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;)</span><br><span class="line">				.set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);</span><br><span class="line">Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();</span><br><span class="line">kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);</span><br><span class="line">kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);</span><br><span class="line">kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);</span><br><span class="line">kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);</span><br><span class="line">kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span><br><span class="line">kafkaParams.put(&quot;enable.auto.commit&quot;,true);</span><br><span class="line"></span><br><span class="line">JavaStreamingContext jssc = new JavaStreamingContext(</span><br><span class="line">		conf, Durations.seconds(30));</span><br><span class="line">jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>构建topic set</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);</span><br><span class="line">String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);</span><br><span class="line">Set&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line">for(String kafkaTopic : kafkaTopicsSplited) &#123;</span><br><span class="line">	topics.add(kafkaTopic);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc,</span><br><span class="line">		LocationStrategies.PreferConsistent(),</span><br><span class="line">		ConsumerStrategies.Subscribe(topics, kafkaParams));</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    
   <div>
    
      <div>
    
        <div style="text-align:center;color: #555;font-size:14px;">-------------本文结束,感谢您的阅读-------------</div>
    
</div>

    
   </div>
    <div>
      
        

      
    </div>

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Brady
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://23yue23.github.io/2019/04/28/spark系列之-调优记录/" title="spark系列之--调优记录">https://23yue23.github.io/2019/04/28/spark系列之-调优记录/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark-调优/" rel="tag"><i class="fa fa-tag"></i> spark-调优</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/28/1-grafana系列之-调研/" rel="next" title="1.grafana系列之--调研">
                <i class="fa fa-chevron-left"></i> 1.grafana系列之--调研
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/29/mysql系列之-问题汇总/" rel="prev" title="mysql系列之-问题汇总">
                mysql系列之-问题汇总 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
             <a href="/">
              <img class="site-author-image" itemprop="image" src="http://static.oschina.net/uploads/space/2015/0629/170157_AOda_1767531.jpg" alt="Brady">
	      </a>
            
              <p class="site-author-name" itemprop="name">Brady</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/yuexianchang" target="_blank" title="csdn">
                      
                        <i class="fa fa-fw fa-google"></i>csdn</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://spring.io" target="_blank" title="spring">
                      
                        <i class="fa fa-fw fa-google"></i>spring</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://hadoop.apache.org" target="_blank" title="hadoop">
                      
                        <i class="fa fa-fw fa-google"></i>hadoop</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://spark.apache.org" target="_blank" title="spark">
                      
                        <i class="fa fa-fw fa-google"></i>spark</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://kafka.apache.org" target="_blank" title="kafka">
                      
                        <i class="fa fa-fw fa-google"></i>kafka</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://flink.apache.org" target="_blank" title="flink">
                      
                        <i class="fa fa-fw fa-google"></i>flink</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://cassandra.apache.org" target="_blank" title="cassandra">
                      
                        <i class="fa fa-fw fa-google"></i>cassandra</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://grafana.com/docs" target="_blank" title="grafana">
                      
                        <i class="fa fa-fw fa-google"></i>grafana</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#性能调优"><span class="nav-number">1.</span> <span class="nav-text">性能调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#分配资源："><span class="nav-number">1.1.</span> <span class="nav-text">分配资源：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkStreaming-优雅退出"><span class="nav-number">1.2.</span> <span class="nav-text">SparkStreaming 优雅退出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调节并行度："><span class="nav-number">1.3.</span> <span class="nav-text">调节并行度：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InputDStream并行化数据接收"><span class="nav-number">1.4.</span> <span class="nav-text">InputDStream并行化数据接收</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#增加block数量，增加每个batch-rdd的partition数量，增加处理并行度"><span class="nav-number">1.5.</span> <span class="nav-text">增加block数量，增加每个batch rdd的partition数量，增加处理并行度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重分区，增加每个batch-rdd的partition数量"><span class="nav-number">1.6.</span> <span class="nav-text">重分区，增加每个batch rdd的partition数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重构RDD架构以及RDD持久化："><span class="nav-number">1.7.</span> <span class="nav-text">重构RDD架构以及RDD持久化：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现RDD高可用性：启动WAL预写日志机制"><span class="nav-number">1.8.</span> <span class="nav-text">实现RDD高可用性：启动WAL预写日志机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广播大变量（1m-100m）："><span class="nav-number">1.9.</span> <span class="nav-text">广播大变量（1m~100m）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用Kryo序列化"><span class="nav-number">1.10.</span> <span class="nav-text">使用Kryo序列化:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用fastutil优化数据格式"><span class="nav-number">1.11.</span> <span class="nav-text">使用fastutil优化数据格式:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调节数据本地化等待时长："><span class="nav-number">1.12.</span> <span class="nav-text">调节数据本地化等待时长：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#定时清除不需要的数据"><span class="nav-number">1.13.</span> <span class="nav-text">定时清除不需要的数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#去除压缩-内存充足的情况下"><span class="nav-number">1.14.</span> <span class="nav-text">去除压缩 (内存充足的情况下)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn-优化"><span class="nav-number">2.</span> <span class="nav-text">Yarn 优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配"><span class="nav-number">2.1.</span> <span class="nav-text">Executors和cpu核心数设置和Spark On Yarn 动态资源分配</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JVM-调优"><span class="nav-number">3.</span> <span class="nav-text">JVM 调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原理概述以及降低cache操作的内存占比"><span class="nav-number">3.1.</span> <span class="nav-text">原理概述以及降低cache操作的内存占比:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GC优化策略-暂时不确定"><span class="nav-number">3.2.</span> <span class="nav-text">GC优化策略(暂时不确定)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调节executor堆外内存与连接等待时长-在spark-sbmit中修改-："><span class="nav-number">3.3.</span> <span class="nav-text">调节executor堆外内存与连接等待时长(在spark-sbmit中修改)：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle调优"><span class="nav-number">4.</span> <span class="nav-text">Shuffle调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原理概述："><span class="nav-number">4.1.</span> <span class="nav-text">原理概述：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#合并map端输出文件："><span class="nav-number">4.2.</span> <span class="nav-text">合并map端输出文件：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#合并map端输出文件：-1"><span class="nav-number">4.3.</span> <span class="nav-text">合并map端输出文件：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HashShuffleManager与SortShuffleManager"><span class="nav-number">4.4.</span> <span class="nav-text">HashShuffleManager与SortShuffleManager</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算子调优"><span class="nav-number">5.</span> <span class="nav-text">算子调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MapPartitions提升Map类操作性能"><span class="nav-number">5.1.</span> <span class="nav-text">MapPartitions提升Map类操作性能:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#filter过后使用coalesce减少分区数量："><span class="nav-number">5.2.</span> <span class="nav-text">filter过后使用coalesce减少分区数量：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#foreachPartition优化写数据库性能："><span class="nav-number">5.3.</span> <span class="nav-text">foreachPartition优化写数据库性能：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#repartition解决Spark-SQL低并行度的性能问题："><span class="nav-number">5.4.</span> <span class="nav-text">repartition解决Spark SQL低并行度的性能问题：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reduceByKey本地聚合介绍："><span class="nav-number">5.5.</span> <span class="nav-text">reduceByKey本地聚合介绍：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码-调优"><span class="nav-number">6.</span> <span class="nav-text">代码 调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#进行HA机制处理-针对Driver高可用性"><span class="nav-number">6.1.</span> <span class="nav-text">进行HA机制处理-针对Driver高可用性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkStreaming-与kafka整合调优"><span class="nav-number">6.2.</span> <span class="nav-text">SparkStreaming 与kafka整合调优</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-sun-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Brady</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">16.7k</span>
  
</div>







<div><span> --不忘初心<>方得始终--</span></div>

        
<div class="busuanzi-count">
<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
    <span class="site-uv">
      <i class="fa fa-user"></i>访客
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>访问总量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":2,"width":100,"height":100,"position":"left"},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.5},"log":false});</script></body>
</html>
