<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蜗牛笔记</title>
  
  <subtitle>骑士的心</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://23yue23.github.io/"/>
  <updated>2019-09-25T02:42:03.495Z</updated>
  <id>https://23yue23.github.io/</id>
  
  <author>
    <name>Brady</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>指导思想</title>
    <link href="https://23yue23.github.io/2019/08/05/%E6%8C%87%E5%AF%BC%E6%80%9D%E6%83%B3/"/>
    <id>https://23yue23.github.io/2019/08/05/指导思想/</id>
    <published>2019-08-05T06:49:05.000Z</published>
    <updated>2019-09-25T02:42:03.495Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>学习方法：<br>  <a href="http://xugongli.club/2018/10/26/how_to_be_proficient_in_some_field/" target="_blank" rel="noopener">3 w 学习</a></p></blockquote><blockquote><p>提问的智慧：<br><a href="https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/master/README-zh_CN.md" target="_blank" rel="noopener">连接</a></p></blockquote><blockquote><p>一万小时定论：<br><a href="https://mp.weixin.qq.com/s/Z05VcYfXp5tExvdqidmkgw" target="_blank" rel="noopener">一万小时定论</a></p></blockquote><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><blockquote><ol><li><a href="https://tech.meituan.com/2018/04/16/study-vs-work.html" target="_blank" rel="noopener">工作中如何做好技术积累</a></li><li><a href="http://openskill.cn/article/488" target="_blank" rel="noopener">学习新技术的10个技巧</a></li><li><a href></a></li><li><a href></a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;学习方法：&lt;br&gt;  &lt;a href=&quot;http://xugongli.club/2018/10/26/how_to_be_proficient_in_some_field/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;3 w 
      
    
    </summary>
    
      <category term="第三世界" scheme="https://23yue23.github.io/categories/%E7%AC%AC%E4%B8%89%E4%B8%96%E7%95%8C/"/>
    
    
      <category term="指导思想" scheme="https://23yue23.github.io/tags/%E6%8C%87%E5%AF%BC%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>Pulsar-学习篇</title>
    <link href="https://23yue23.github.io/2019/07/17/Pulsar-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/07/17/Pulsar-学习篇/</id>
    <published>2019-07-17T08:03:32.000Z</published>
    <updated>2019-09-25T02:37:54.019Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.infoq.cn/article/1UaxFKWUhUKTY1t_5gPq" target="_blank" rel="noopener"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.infoq.cn/article/1UaxFKWUhUKTY1t_5gPq&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="Pulsar" scheme="https://23yue23.github.io/categories/Pulsar/"/>
    
    
      <category term="Pulsar-学习篇" scheme="https://23yue23.github.io/tags/Pulsar-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>广告-学习篇</title>
    <link href="https://23yue23.github.io/2019/07/12/%E5%B9%BF%E5%91%8A-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/07/12/广告-学习篇/</id>
    <published>2019-07-12T09:30:15.000Z</published>
    <updated>2019-09-25T02:40:02.550Z</updated>
    
    <content type="html"><![CDATA[<h4 id="术语理解"><a href="#术语理解" class="headerlink" title="术语理解"></a>术语理解</h4><p>1.<a href="https://cloud.tencent.com/developer/article/1351052" target="_blank" rel="noopener">dsp系统架构</a></p><h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><blockquote><ol><li>dsp 业务</li></ol></blockquote><blockquote><p>1.1 <a href="https://blog.csdn.net/LW_GHY/article/details/71455535" target="_blank" rel="noopener">美团DSP广告策略实践</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;术语理解&quot;&gt;&lt;a href=&quot;#术语理解&quot; class=&quot;headerlink&quot; title=&quot;术语理解&quot;&gt;&lt;/a&gt;术语理解&lt;/h4&gt;&lt;p&gt;1.&lt;a href=&quot;https://cloud.tencent.com/developer/article/1351052
      
    
    </summary>
    
      <category term="广告" scheme="https://23yue23.github.io/categories/%E5%B9%BF%E5%91%8A/"/>
    
    
      <category term="广告-学习篇" scheme="https://23yue23.github.io/tags/%E5%B9%BF%E5%91%8A-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>spark-学习篇</title>
    <link href="https://23yue23.github.io/2019/07/12/spark-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/07/12/spark-学习篇/</id>
    <published>2019-07-12T08:53:29.000Z</published>
    <updated>2019-09-25T02:39:01.773Z</updated>
    
    <content type="html"><![CDATA[<h4 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h4><blockquote><ol><li><a href="https://blog.csdn.net/LW_GHY/article/details/50859346" target="_blank" rel="noopener">Spark on yarn有分为两种模式yarn-cluster和yarn-client</a></li></ol></blockquote><blockquote><ol start="2"><li><a href="https://blog.csdn.net/LW_GHY/article/details/52946136" target="_blank" rel="noopener">spark参数总结</a></li><li><a href="https://blog.csdn.net/LW_GHY/article/details/51477065" target="_blank" rel="noopener">spark 常用算子</a></li><li><a href="https://blog.csdn.net/LW_GHY/article/details/51470073" target="_blank" rel="noopener">Spark on YARN客户端模式作业运行全过程分析</a></li></ol></blockquote><hr><h4 id="spark-batch"><a href="#spark-batch" class="headerlink" title="spark batch"></a>spark batch</h4><blockquote><ol><li><a href="https://blog.csdn.net/LW_GHY/article/details/51477100" target="_blank" rel="noopener">Spark多文件输出(MultipleOutputFormat)</a></li></ol></blockquote><blockquote><p>2.spark-mysql 操作</p><blockquote><p>2.1 <a href="https://blog.csdn.net/LW_GHY/article/details/50939091" target="_blank" rel="noopener">Spark读取数据库(Mysql)的四种方式讲解</a></p></blockquote></blockquote><blockquote><blockquote><p>2.2 <a href="https://blog.csdn.net/LW_GHY/article/details/51465125" target="_blank" rel="noopener">Spark与Mysql(JdbcRDD)整合开发</a></p></blockquote></blockquote><blockquote><blockquote><p>2.3 <a href="https://blog.csdn.net/LW_GHY/article/details/51477072" target="_blank" rel="noopener">spark 计算结果写入mysql</a></p></blockquote></blockquote><hr><h4 id="spark-Streaming"><a href="#spark-Streaming" class="headerlink" title="spark Streaming"></a>spark Streaming</h4><blockquote><ol><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">官网stream</a></li></ol></blockquote><blockquote><ol start="2"><li><a href="https://blog.csdn.net/LW_GHY/article/details/88867174" target="_blank" rel="noopener">Spark Streaming 实现思路与模块概述</a></li></ol></blockquote><blockquote><ol start="3"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51673081" target="_blank" rel="noopener">SparkStreaming向Hbase中写数据</a></li></ol></blockquote><blockquote><ol start="4"><li><a href="https://blog.csdn.net/LW_GHY/article/details/52143659" target="_blank" rel="noopener">Spark Streaming kafka实现数据零丢失的几种方式</a></li></ol></blockquote><blockquote><ol start="5"><li><a href="https://blog.csdn.net/LW_GHY/article/details/50926956" target="_blank" rel="noopener">Kafka+Spark Streaming+Redis实时系统实践</a></li></ol></blockquote><blockquote><ol start="6"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51477355" target="_blank" rel="noopener">Spark Streaming中空batches处理的两种方法</a></li></ol></blockquote><hr><h4 id="spark-调优"><a href="#spark-调优" class="headerlink" title="spark 调优"></a>spark 调优</h4><blockquote><ol><li><a href="https://blog.csdn.net/LW_GHY/article/details/51419760" target="_blank" rel="noopener">spark性能优化：shuffle调优</a></li></ol></blockquote><blockquote><ol start="2"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51420027" target="_blank" rel="noopener">spark性能调优：开发调优</a></li></ol></blockquote><blockquote><ol start="3"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51419977" target="_blank" rel="noopener">spark性能调优：资源优化</a></li></ol></blockquote><blockquote><ol start="4"><li><a href="https://blog.csdn.net/LW_GHY/article/details/50780940" target="_blank" rel="noopener">Saprk Streaming性能调优</a></li></ol></blockquote><blockquote><ol start="5"><li><a href="https://blog.csdn.net/LW_GHY/article/details/52373749" target="_blank" rel="noopener">GC调优在Spark应用中的实践</a></li></ol></blockquote><blockquote><ol start="6"><li><a href="https://blog.csdn.net/LW_GHY/article/details/88232471" target="_blank" rel="noopener">JVM的GC调优-上</a></li></ol></blockquote><blockquote><ol start="7"><li><a href="https://blog.csdn.net/LW_GHY/article/details/88232638" target="_blank" rel="noopener">JVM的GC调优-下</a></li></ol></blockquote><hr><h4 id="sparkMLib"><a href="#sparkMLib" class="headerlink" title="sparkMLib"></a>sparkMLib</h4><blockquote><p>1.<a href="https://blog.csdn.net/LW_GHY/article/details/54426443" target="_blank" rel="noopener">Spark MLlib训练的广告点击率预测模型</a></p></blockquote><hr><h4 id="案例实践"><a href="#案例实践" class="headerlink" title="案例实践"></a>案例实践</h4><blockquote><ol><li><a href="https://blog.csdn.net/xwc35047/article/details/75309350" target="_blank" rel="noopener">基于Spark streaming的SQL服务实时自动化运维</a></li><li></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;spark&quot;&gt;&lt;a href=&quot;#spark&quot; class=&quot;headerlink&quot; title=&quot;spark&quot;&gt;&lt;/a&gt;spark&lt;/h4&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/LW_GHY/a
      
    
    </summary>
    
      <category term="spark" scheme="https://23yue23.github.io/categories/spark/"/>
    
    
      <category term="spark-学习篇" scheme="https://23yue23.github.io/tags/spark-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>hexo-常用篇</title>
    <link href="https://23yue23.github.io/2019/07/10/hexo-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/07/10/hexo-常用篇/</id>
    <published>2019-07-10T03:23:08.000Z</published>
    <updated>2019-09-25T02:46:05.682Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建新文件"><a href="#创建新文件" class="headerlink" title="创建新文件"></a>创建新文件</h4> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在当前总目录下</span><br><span class="line">hexo new &quot;hexo之基本操作&quot;</span><br></pre></td></tr></table></figure><h4 id="清除缓存更新部署"><a href="#清除缓存更新部署" class="headerlink" title="清除缓存更新部署"></a>清除缓存更新部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g -d</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;创建新文件&quot;&gt;&lt;a href=&quot;#创建新文件&quot; class=&quot;headerlink&quot; title=&quot;创建新文件&quot;&gt;&lt;/a&gt;创建新文件&lt;/h4&gt; &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter
      
    
    </summary>
    
      <category term="hexo" scheme="https://23yue23.github.io/categories/hexo/"/>
    
    
      <category term="hexo-常用篇" scheme="https://23yue23.github.io/tags/hexo-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>linux-常用篇</title>
    <link href="https://23yue23.github.io/2019/07/10/linux-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/07/10/linux-常用篇/</id>
    <published>2019-07-10T03:22:41.000Z</published>
    <updated>2019-09-25T02:38:27.260Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Bash-快捷键"><a href="#Bash-快捷键" class="headerlink" title="Bash 快捷键"></a>Bash 快捷键</h4><p>Ctl-U   删除光标到行首的所有字符,在某些设置下,删除全行<br>Ctl-W   删除当前光标到前边的最近一个空格之间的字符</p><p>文件</p><ol><li>删除文件： rm -rf  <em>log  等加于$ find ./ -name “</em>log” -exec rm {} </li><li>搜寻文件或目录: $find ./ -name “core*” | xargs file</li><li>查找所有非txt文本: find . ! -name “*.txt” -print</li><li>统计文本中123 出现的个数： grep -c “123” filename</li><li>sort 排序：-n 按数字进行排序 VS -d 按字典序进行排序 -r 逆序排序  -k N 指定按第N列排序 ：sort -n -1k -2k    sort -bd data // 忽略像空格之类的前导空白字符</li><li>拼接文本 paste file1 file2</li></ol><p>系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># uname -a               # 查看内核/操作系统/CPU信息</span><br><span class="line"># head -n 1 /etc/issue   # 查看操作系统版本</span><br><span class="line"># cat /proc/cpuinfo      # 查看CPU信息</span><br><span class="line"># hostname               # 查看计算机名</span><br><span class="line"># lspci -tv              # 列出所有PCI设备</span><br><span class="line"># lsusb -tv              # 列出所有USB设备</span><br><span class="line"># lsmod                  # 列出加载的内核模块</span><br><span class="line"># env                    # 查看环境变量</span><br><span class="line">资源</span><br><span class="line"># free -m                # 查看内存使用量和交换区使用量</span><br><span class="line"># df -h                  # 查看各分区使用情况</span><br><span class="line"># du -sh &lt;目录名&gt;        # 查看指定目录的大小</span><br><span class="line"># grep MemTotal /proc/meminfo   # 查看内存总量</span><br><span class="line"># grep MemFree /proc/meminfo    # 查看空闲内存量</span><br><span class="line"># uptime                 # 查看系统运行时间、用户数、负载</span><br><span class="line"># cat /proc/loadavg      # 查看系统负载</span><br></pre></td></tr></table></figure><h4 id="磁盘和分区"><a href="#磁盘和分区" class="headerlink" title="磁盘和分区"></a>磁盘和分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># mount | column -t      # 查看挂接的分区状态</span><br><span class="line"># fdisk -l               # 查看所有分区</span><br><span class="line"># swapon -s              # 查看所有交换分区</span><br><span class="line"># hdparm -i /dev/hda     # 查看磁盘参数(仅适用于IDE设备)</span><br><span class="line"># dmesg | grep IDE       # 查看启动时IDE设备检测状况</span><br></pre></td></tr></table></figure><h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># ifconfig               # 查看所有网络接口的属性</span><br><span class="line"># iptables -L            # 查看防火墙设置</span><br><span class="line"># route -n               # 查看路由表</span><br><span class="line"># netstat -lntp          # 查看所有监听端口</span><br><span class="line"># netstat -antp          # 查看所有已经建立的连接</span><br><span class="line"># netstat -s             # 查看网络统计信息</span><br></pre></td></tr></table></figure><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ps -ef                 # 查看所有进程</span><br><span class="line"># top                    # 实时显示进程状态</span><br></pre></td></tr></table></figure><h4 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># w                      # 查看活动用户</span><br><span class="line"># id &lt;用户名&gt;            # 查看指定用户信息</span><br><span class="line"># last                   # 查看用户登录日志</span><br><span class="line"># cut -d: -f1 /etc/passwd   # 查看系统所有用户</span><br><span class="line"># cut -d: -f1 /etc/group    # 查看系统所有组</span><br><span class="line"># crontab -l             # 查看当前用户的计划任务</span><br></pre></td></tr></table></figure><h4 id="服务"><a href="#服务" class="headerlink" title="服务"></a>服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># chkconfig --list       # 列出所有系统服务</span><br><span class="line"># chkconfig --list | grep on    # 列出所有启动的系统服务</span><br></pre></td></tr></table></figure><h4 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># rpm -qa                # 查看所有安装的软件包</span><br></pre></td></tr></table></figure><h4 id="Shell-脚本执行返回状态码："><a href="#Shell-脚本执行返回状态码：" class="headerlink" title="Shell 脚本执行返回状态码："></a>Shell 脚本执行返回状态码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">0 命令成功完成</span><br><span class="line">1通常的未知错误</span><br><span class="line">2误用shell命令</span><br><span class="line">126命令无法执行</span><br><span class="line">127没有找到命令</span><br><span class="line">128无效的退出参数</span><br><span class="line">128+x使用Linux信号x的致命错误。</span><br><span class="line">130使用Ctrl-C终止的命令</span><br><span class="line">255规范外的退出状态</span><br></pre></td></tr></table></figure><h4 id="技巧："><a href="#技巧：" class="headerlink" title="技巧："></a>技巧：</h4><blockquote><p>1.查看变量是否被声明：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用 :- 来测试是否一个变量是否被声明过。</span><br><span class="line">如：if [ &quot;$&#123;NAME:-&#125;&quot; = &quot;Kevin&quot; ] 如果 $&#123;NAME&#125;变量未声明则会变为空字符，你也可以设置为其他默认值.</span><br><span class="line">例如：如果不存在，默认值设为：noname ，if [ &quot;$&#123;NAME:-noname&#125;&quot; = &quot;Kevin” ]</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>2.自定义shell命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc 中设置命令别名:</span><br><span class="line">alias lsl=&apos;ls -lrt&apos;</span><br><span class="line">alias lm=&apos;ls -al|more’</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>3.shell文本格式化代码：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gg=G</span><br></pre></td></tr></table></figure></p></blockquote><hr><h4 id="vi-快捷建"><a href="#vi-快捷建" class="headerlink" title="vi 快捷建"></a>vi 快捷建</h4><p>A               移动光标到当前行尾，并进入 insert 状态<br>a               在当前位置后进入 insert 状态<br>dd              删除当前行<br>D               删除光标之后的内容<br>p               粘贴刚删除的文本<br>ctrl+r          搜索历史命令<br>ctrl+X Ctrl+E   调用默认编辑器去编辑一个特别长的命令</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Bash-快捷键&quot;&gt;&lt;a href=&quot;#Bash-快捷键&quot; class=&quot;headerlink&quot; title=&quot;Bash 快捷键&quot;&gt;&lt;/a&gt;Bash 快捷键&lt;/h4&gt;&lt;p&gt;Ctl-U   删除光标到行首的所有字符,在某些设置下,删除全行&lt;br&gt;Ctl-W   删除
      
    
    </summary>
    
      <category term="linux" scheme="https://23yue23.github.io/categories/linux/"/>
    
    
      <category term="linux-常用篇" scheme="https://23yue23.github.io/tags/linux-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>linux-问题篇</title>
    <link href="https://23yue23.github.io/2019/05/17/linux-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/05/17/linux-问题篇/</id>
    <published>2019-05-17T06:51:25.000Z</published>
    <updated>2019-09-25T02:36:52.723Z</updated>
    
    <content type="html"><![CDATA[<h4 id="lsof"><a href="#lsof" class="headerlink" title="lsof"></a>lsof</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i:80</span><br></pre></td></tr></table></figure><h4 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -anp|grep port</span><br></pre></td></tr></table></figure><h4 id="ip"><a href="#ip" class="headerlink" title="ip"></a>ip</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr</span><br></pre></td></tr></table></figure><h4 id="flind"><a href="#flind" class="headerlink" title="flind"></a>flind</h4><h4 id="journalctl"><a href="#journalctl" class="headerlink" title="journalctl"></a>journalctl</h4><p><a href></a></p><h4 id="curl"><a href="#curl" class="headerlink" title="curl"></a>curl</h4><p><a href="http://man.linuxde.net/curl" target="_blank" rel="noopener">curl</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;lsof&quot;&gt;&lt;a href=&quot;#lsof&quot; class=&quot;headerlink&quot; title=&quot;lsof&quot;&gt;&lt;/a&gt;lsof&lt;/h4&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr
      
    
    </summary>
    
      <category term="linux" scheme="https://23yue23.github.io/categories/linux/"/>
    
    
      <category term="linux-问题篇" scheme="https://23yue23.github.io/tags/linux-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>Druid-学习篇</title>
    <link href="https://23yue23.github.io/2019/05/17/Druid-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/05/17/Druid-学习篇/</id>
    <published>2019-05-17T05:49:39.000Z</published>
    <updated>2019-09-25T02:37:16.186Z</updated>
    
    <content type="html"><![CDATA[<ol><li><a href="https://blog.csdn.net/bigtree_3721/article/category/6956082" target="_blank" rel="noopener">druid 系列csdn</a></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/bigtree_3721/article/category/6956082&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;druid 系列csdn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li
      
    
    </summary>
    
      <category term="Druid" scheme="https://23yue23.github.io/categories/Druid/"/>
    
    
      <category term="Druid-学习篇" scheme="https://23yue23.github.io/tags/Druid-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>Homebrew-使用篇</title>
    <link href="https://23yue23.github.io/2019/05/17/Homebrew-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/05/17/Homebrew-使用篇/</id>
    <published>2019-05-17T02:56:28.000Z</published>
    <updated>2019-09-25T02:34:24.410Z</updated>
    
    <content type="html"><![CDATA[<p>安装路径：/usr/local/Cellar/</p><blockquote><p>查找软件包</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew search wget</span><br></pre></td></tr></table></figure><blockquote><p>安装软件包</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install wget</span><br></pre></td></tr></table></figure><blockquote><p>列出已安装的软件包</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew list</span><br></pre></td></tr></table></figure><blockquote><p>删除软件包</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew remove wget</span><br></pre></td></tr></table></figure><blockquote><p>查看软件包信息</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew info wget</span><br></pre></td></tr></table></figure><blockquote><p>列出软件包的依赖关系</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew deps wget</span><br></pre></td></tr></table></figure><blockquote><p>更新brew</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew update</span><br></pre></td></tr></table></figure><blockquote><p>列出过时的软件包（已安装但不是最新版本）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew outdated</span><br></pre></td></tr></table></figure><blockquote><p>更新过时的软件包（全部或指定）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew upgrade 或 brew upgrade wget</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;安装路径：/usr/local/Cellar/&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;查找软件包&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;s
      
    
    </summary>
    
      <category term="工具" scheme="https://23yue23.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Homebrew-使用篇" scheme="https://23yue23.github.io/tags/Homebrew-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>代码库-邮件发送</title>
    <link href="https://23yue23.github.io/2019/05/06/%E4%BB%A3%E7%A0%81%E5%BA%93-%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81/"/>
    <id>https://23yue23.github.io/2019/05/06/代码库-邮件发送/</id>
    <published>2019-05-06T04:02:06.000Z</published>
    <updated>2019-09-25T02:39:38.093Z</updated>
    
    <content type="html"><![CDATA[<h4 id="scala-版本示例："><a href="#scala-版本示例：" class="headerlink" title="scala 版本示例："></a>scala 版本示例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.slf4j.LoggerFactory</span><br><span class="line"></span><br><span class="line">import javax.mail._</span><br><span class="line">import javax.mail.internet.InternetAddress</span><br><span class="line">import javax.mail.internet.MimeMessage</span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object Mail &#123;</span><br><span class="line">  val logger = LoggerFactory.getLogger(Mail.getClass)</span><br><span class="line">  val bodyHtml = &quot;&lt;!DOCTYPE html PUBLIC -//W3C//DTD HTML 4.01 Transitional//ENhttp://www.w3.org/TR/html4/loose.dtd&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=Content-Type content=text/html; charset=utf-8 pageEncoding=UTF-8&gt;&lt;/head&gt;&lt;body&gt;%s&lt;/body&gt;&lt;/html&gt;&quot;</span><br><span class="line">  val prop = new Properties()</span><br><span class="line">  prop.put(&quot;mail.smtp.host&quot;,&quot;smtp.exmail.qq.com&quot;)</span><br><span class="line">  prop.put(&quot;mail.smtp.auth&quot;,&quot;true&quot;)</span><br><span class="line">  prop.put(&quot;mail.smtp.connectiontimeout&quot;,&quot;10000&quot;)</span><br><span class="line">  prop.put(&quot;mail.smtp.timeout&quot;,&quot;20000&quot;)</span><br><span class="line"></span><br><span class="line">  def send(address: String, title: String, content: String) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      val addresses = address.split(&quot;,&quot;).map(new InternetAddress(_).asInstanceOf[Address])</span><br><span class="line">      val authenticator = new SMTPAuthenticator(&quot;username@qq.com&quot;, &quot;password&quot;)</span><br><span class="line">      val sendMailSession = Session.getDefaultInstance(prop, authenticator)</span><br><span class="line">      val newMessage = new MimeMessage(sendMailSession)</span><br><span class="line">      newMessage.setFrom(new InternetAddress(&quot;username@qq.com&quot;))</span><br><span class="line">      newMessage.setRecipients(Message.RecipientType.TO, addresses)</span><br><span class="line">      newMessage.setSubject(title)</span><br><span class="line">      val html = String.format(bodyHtml, content)</span><br><span class="line">      newMessage.setContent(html, &quot;text/html;charset=utf-8&quot;)</span><br><span class="line">      Transport.send(newMessage)</span><br><span class="line">      logger.info(&quot;send an email to address[&#123;&#125;] title[&#123;&#125;] content[&#123;&#125;]&quot;, addresses, title, content);</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: MessagingException =&gt; logger.info(&quot;error occur when mail&quot;, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class SMTPAuthenticator(username: String, password: String) extends Authenticator &#123;</span><br><span class="line">    override def getPasswordAuthentication: PasswordAuthentication = new PasswordAuthentication(username, password)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;scala-版本示例：&quot;&gt;&lt;a href=&quot;#scala-版本示例：&quot; class=&quot;headerlink&quot; title=&quot;scala 版本示例：&quot;&gt;&lt;/a&gt;scala 版本示例：&lt;/h4&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tabl
      
    
    </summary>
    
      <category term="代码库" scheme="https://23yue23.github.io/categories/%E4%BB%A3%E7%A0%81%E5%BA%93/"/>
    
    
      <category term="代码库-邮件发送" scheme="https://23yue23.github.io/tags/%E4%BB%A3%E7%A0%81%E5%BA%93-%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81/"/>
    
  </entry>
  
  <entry>
    <title>spark-问题篇</title>
    <link href="https://23yue23.github.io/2019/04/29/spark-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/04/29/spark-问题篇/</id>
    <published>2019-04-29T12:38:35.000Z</published>
    <updated>2019-09-25T02:39:26.405Z</updated>
    
    <content type="html"><![CDATA[<h5 id="org-apache-spark-SparkException-Could-not-find-CoarseGrainedScheduler"><a href="#org-apache-spark-SparkException-Could-not-find-CoarseGrainedScheduler" class="headerlink" title="org.apache.spark.SparkException: Could not find CoarseGrainedScheduler."></a>org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.</h5><blockquote><ol><li>这个可能是一个资源问题，应该给任务分配更多的 cores 和Executors，并且分配更多的内存。并且需要给RDD分配更多的分区 </li><li>在配置资源中加入这句话也许能解决你的问题：<br> –conf spark.dynamicAllocation.enabled=false</li><li>经过一般调试，发现原来是因为spark任务生成task任务过少，而任务提交时所指定的Excutor 数过多导致，故调小 –num-executors 参数问题得以解决。</li></ol></blockquote><hr><p>##### </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;org-apache-spark-SparkException-Could-not-find-CoarseGrainedScheduler&quot;&gt;&lt;a href=&quot;#org-apache-spark-SparkException-Could-not-find-Coar
      
    
    </summary>
    
      <category term="spark" scheme="https://23yue23.github.io/categories/spark/"/>
    
    
      <category term="spark-问题篇" scheme="https://23yue23.github.io/tags/spark-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>MacOS-使用篇</title>
    <link href="https://23yue23.github.io/2019/04/29/MacOS-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/04/29/MacOS-使用篇/</id>
    <published>2019-04-29T12:27:28.000Z</published>
    <updated>2019-09-25T02:37:44.220Z</updated>
    
    <content type="html"><![CDATA[<h4 id="快捷键："><a href="#快捷键：" class="headerlink" title="快捷键："></a>快捷键：</h4><p>退出关闭应用：Command+Q</p><hr><h4 id="目录地址："><a href="#目录地址：" class="headerlink" title="目录地址："></a>目录地址：</h4><blockquote><p>软件的安装目录</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/Library/Application Support</span><br></pre></td></tr></table></figure><blockquote><p>brew 安装路径</p></blockquote><pre><code>/usr/local/Cellar/</code></pre><h4 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h4><blockquote><p><a href="https://blog.csdn.net/k12104/article/details/84104261" target="_blank" rel="noopener">Mac Beyond Compare4 破解方法</a></p></blockquote><blockquote><p>[]</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;快捷键：&quot;&gt;&lt;a href=&quot;#快捷键：&quot; class=&quot;headerlink&quot; title=&quot;快捷键：&quot;&gt;&lt;/a&gt;快捷键：&lt;/h4&gt;&lt;p&gt;退出关闭应用：Command+Q&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;目录地址：&quot;&gt;&lt;a href=&quot;#目录地址：&quot; clas
      
    
    </summary>
    
      <category term="工具" scheme="https://23yue23.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="MacOS-使用篇" scheme="https://23yue23.github.io/tags/MacOS-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>技术文章</title>
    <link href="https://23yue23.github.io/2019/04/29/%E7%BB%8F%E5%85%B8%E6%96%87%E7%AB%A0/"/>
    <id>https://23yue23.github.io/2019/04/29/经典文章/</id>
    <published>2019-04-29T05:37:39.000Z</published>
    <updated>2019-09-25T02:41:51.520Z</updated>
    
    <content type="html"><![CDATA[<h4 id="技术文章"><a href="#技术文章" class="headerlink" title="技术文章"></a>技术文章</h4><blockquote><ol><li><a href="https://tech.meituan.com/2014/06/30/mysql-index.html" target="_blank" rel="noopener">MySQL索引原理及慢查询优化</a></li><li><a href="https://tech.meituan.com/2018/10/18/meishi-data-flink.html" target="_blank" rel="noopener">美团点评基于 Flink 的实时数仓建设实践</a></li><li><a href="https://mp.weixin.qq.com/s/lhP4B6jA0CQpjT1PdlYmQw" target="_blank" rel="noopener">从零开始入门推荐算法工程师</a></li><li><a href="https://yq.aliyun.com/articles/691816?spm=a2c4e.11153940.blogcont691499.9.2be51d53KvnU8f" target="_blank" rel="noopener">计算广告与流处理技术综述</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;技术文章&quot;&gt;&lt;a href=&quot;#技术文章&quot; class=&quot;headerlink&quot; title=&quot;技术文章&quot;&gt;&lt;/a&gt;技术文章&lt;/h4&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://tech.meituan.com/2014/06/3
      
    
    </summary>
    
      <category term="技术文章" scheme="https://23yue23.github.io/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="技术文章" scheme="https://23yue23.github.io/tags/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
  </entry>
  
  <entry>
    <title>spark-调优篇</title>
    <link href="https://23yue23.github.io/2019/04/28/spark-%E8%B0%83%E4%BC%98%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/04/28/spark-调优篇/</id>
    <published>2019-04-28T06:23:46.000Z</published>
    <updated>2019-09-25T02:39:10.857Z</updated>
    
    <content type="html"><![CDATA[<h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h3><h4 id="分配资源："><a href="#分配资源：" class="headerlink" title="分配资源："></a>分配资源：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class cn.spark.sparktest.core.WordCountCluster \</span><br><span class="line">--num-executors 80 \  配置executor的数量</span><br><span class="line">--driver-memory 6g \  配置driver的内存（影响不大）</span><br><span class="line">--executor-memory 6g \  配置每个executor的内存大小</span><br><span class="line">--executor-cores 3 \  配置每个executor的cpu core数量(RDD cache/shuffle/task执行)</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--queue root.default \</span><br><span class="line">--conf spark.yarn.executor.memoryOverhead=2048 \  executor堆外内存</span><br><span class="line">--conf spark.core.connection.ack.wait.timeout=300 \ 连接的超时时长</span><br><span class="line">/usr/local/spark/spark.jar \</span><br><span class="line">$&#123;1&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-submit \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-cores 3 \</span><br><span class="line">  --executor-memory 10G \</span><br><span class="line">  --driver-memory 4G \</span><br><span class="line">  --conf spark.dynamicAllocation.enabled=true \</span><br><span class="line">  --conf spark.shuffle.service.enabled=true \</span><br><span class="line">  --conf spark.dynamicAllocation.initialExecutors=5 \</span><br><span class="line">  --conf spark.dynamicAllocation.maxExecutors=40 \</span><br><span class="line">  --conf spark.dynamicAllocation.minExecutors=0 \</span><br><span class="line">  --conf spark.dynamicAllocation.executorIdleTimeout=30s \</span><br><span class="line">  --conf spark.dynamicAllocation.schedulerBacklogTimeout=10s \</span><br></pre></td></tr></table></figure><hr><h4 id="SparkStreaming-优雅退出"><a href="#SparkStreaming-优雅退出" class="headerlink" title="SparkStreaming 优雅退出"></a>SparkStreaming 优雅退出</h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR);</span><br><span class="line">//String checkpointPath = PropertiesUtil.getProperty(&quot;streaming.checkpoint.path&quot;);</span><br><span class="line">JavaStreamingContext javaStreamingContext = JavaStreamingContext.getOrCreate(&quot;hdfs://Master:9000/streaming_checkpoint&quot;, createContext());</span><br><span class="line">javaStreamingContext.start();</span><br><span class="line"></span><br><span class="line">每隔20秒钟监控是否有停止指令,如果有则优雅退出streaming</span><br><span class="line">final Properties serverProps = PropertiesUtil.properties;</span><br><span class="line">Thread thread = new Thread(new MonitorStopThread(javaStreamingContext,serverProps));</span><br><span class="line">thread.start();</span><br><span class="line">javaStreamingContext.awaitTermination();</span><br><span class="line">   &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><hr><h4 id="调节并行度："><a href="#调节并行度：" class="headerlink" title="调节并行度："></a>调节并行度：</h4><blockquote><ol><li>并行度：其实就是指的是，Spark作业中，各个stage的task数量，也就代表了Spark作业的在各个阶段（stage）的并行度。</li><li>官方是推荐，task数量，设置成spark application总cpu core数量的2~3倍，比如150个cpu core，基本要设置task数量为300~500；</li><li>SparkConf conf = new SparkConf().set(“spark.default.parallelism”, “500”)</li></ol></blockquote><hr><h4 id="InputDStream并行化数据接收"><a href="#InputDStream并行化数据接收" class="headerlink" title="InputDStream并行化数据接收"></a>InputDStream并行化数据接收</h4><blockquote><p>创建多个InputDStream来接收同一数据源,把多个topic数据细化为单一的kafkaStream来接收</p><blockquote><ol><li>创建kafkaStream</li></ol></blockquote></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;();</span><br><span class="line"> kafkaParams.put(&quot;metadata.broker.list&quot;, &quot;192.168.1.164:9092,192.168.1.165:9092,192.168.1.166:9092&quot;);</span><br><span class="line"> kafkaParams.put(&quot;zookeeper.connect&quot;,&quot;master:2181,data1:2181,data2:2181&quot;);</span><br><span class="line"> </span><br><span class="line"> 构建topic set</span><br><span class="line"> String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);</span><br><span class="line"> String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line"> Set&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line"> for(String kafkaTopic : kafkaTopicsSplited) &#123;</span><br><span class="line"> topics.add(kafkaTopic);</span><br><span class="line"> </span><br><span class="line"> JavaPairInputDStream&lt;String, String&gt; kafkaStream = KafkaUtils.createDirectStream(</span><br><span class="line"> jssc, </span><br><span class="line"> String.class, </span><br><span class="line"> String.class, </span><br><span class="line"> StringDecoder.class, </span><br><span class="line"> StringDecoder.class, </span><br><span class="line"> kafkaParams, </span><br><span class="line"> topics);</span><br></pre></td></tr></table></figure><blockquote><blockquote><ol start="2"><li>InputDStream并行化数据接收</li></ol></blockquote></blockquote><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> int numStreams = 5;</span><br><span class="line"> List&lt;JavaPairDStream&lt;String, String&gt;&gt; kafkaStreams = new</span><br><span class="line"> ArrayList&lt;JavaPairDStream&lt;String,String&gt;&gt;(numStreams);</span><br><span class="line"> for (int i = 0; i &lt; numStreams; i++) &#123;</span><br><span class="line"> kafkaStreams.add(KafkaUtils.createStream(...));</span><br><span class="line"> &#125;</span><br><span class="line"> JavaPairDStream&lt;String, String&gt; unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));</span><br><span class="line">unifiedStream.print();</span><br></pre></td></tr></table></figure></code></pre><hr><h4 id="增加block数量，增加每个batch-rdd的partition数量，增加处理并行度"><a href="#增加block数量，增加每个batch-rdd的partition数量，增加处理并行度" class="headerlink" title="增加block数量，增加每个batch rdd的partition数量，增加处理并行度"></a>增加block数量，增加每个batch rdd的partition数量，增加处理并行度</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">第一步：receiver从数据源源源不断地获取到数据，首先是会按照block interval，将指定时间间隔的数据，收集为一个block；默认时间是200ms，官方推荐不要小于50ms；</span><br><span class="line">第二步：根据指定batch interval时间间隔合并为一个batch，创建为一个rdd，</span><br><span class="line">第三步：启动一个job，去处理这个batch rdd中的数据。</span><br><span class="line">第四步：batch rdd 的partition数量是多少呢？一个batch有多少个block，就有多少个partition；就意味着并行度是多少；就意味着每个batch rdd有多少个task会并行计算和处理。</span><br><span class="line">调优：如果希望可以比默认的task数量和并行度再多一些，可以手动调节blockinterval，减少block interval。每个batch可以包含更多的block。因此也就有更多的partition，因此就会有更多的task并行处理每个batch rdd。</span><br></pre></td></tr></table></figure><h4 id="重分区，增加每个batch-rdd的partition数量"><a href="#重分区，增加每个batch-rdd的partition数量" class="headerlink" title="重分区，增加每个batch rdd的partition数量"></a>重分区，增加每个batch rdd的partition数量</h4><p>inputStream.repartition()：重分区，增加每个batch rdd的partition数量<br>对dstream中的rdd进行重分区为指定数量的分区，就可以提高指定dstream的rdd的计算并行度<br>调节并行度</p><hr><h4 id="重构RDD架构以及RDD持久化："><a href="#重构RDD架构以及RDD持久化：" class="headerlink" title="重构RDD架构以及RDD持久化："></a>重构RDD架构以及RDD持久化：</h4><blockquote><ol><li>RDD架构重构与优化</li><li>公共RDD一定要实现持久化,对于要多次计算和使用的公共RDD，一定要进行持久化。</li><li>持久化，是可以进行序列化的<br>sessionid2actionRDD=sessionid2actionRDD.persist(StorageLevel.MEMORY_ONLY());<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MEMORY_ONLY    直接以Java对象的形式存储于JVM的内存中</span><br><span class="line">MYMORY_AND_DISK    存储于JVM的内存+磁盘</span><br><span class="line">MEMORY_ONLY_SER    序列化存储于内存中</span><br><span class="line">MEMORY_AND_DISK_SER    序列化存储于内存+磁盘</span><br></pre></td></tr></table></figure></code></pre></li></ol></blockquote><blockquote><ol start="4"><li>为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化</li></ol></blockquote><hr><h4 id="实现RDD高可用性：启动WAL预写日志机制"><a href="#实现RDD高可用性：启动WAL预写日志机制" class="headerlink" title="实现RDD高可用性：启动WAL预写日志机制"></a>实现RDD高可用性：启动WAL预写日志机制</h4><blockquote><p>spark streaming，从原理上来说，是通过receiver来进行数据接收的；接收到的数据，会被划分成一个一个的block；block会被组合成一个batch；针对一个batch，会创建一个rdd；<br>receiver接收到数据后，就会立即将数据写入一份到容错文件系统（比如hdfs）上的checkpoint目录中的，另一份写入到磁盘文件中去；作为数据的冗余副本。无论你的程序怎么挂掉，或者是数据丢失，那么数据都不肯能会永久性的丢失；因为肯定有副本。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf()       </span><br><span class="line">    .setMaster(&quot;local[2]&quot;)</span><br><span class="line">    .setAppName(&quot;StreamingSpark&quot;);</span><br><span class="line">.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;);</span><br><span class="line">   .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);</span><br><span class="line">    .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;);    </span><br><span class="line">    .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);   </span><br><span class="line">JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5)); </span><br><span class="line">jssc.checkpoint(&quot;hdfs://192.168.1.164:9000/checkpoint&quot;);</span><br></pre></td></tr></table></figure><hr><h4 id="广播大变量（1m-100m）："><a href="#广播大变量（1m-100m）：" class="headerlink" title="广播大变量（1m~100m）："></a>广播大变量（1m~100m）：</h4><blockquote><ol><li>默认的情况下，task执行的算子中，使用了外部的变量，每个task都会获取一份变量的副本，有什么缺点呢？<pre><code>网络传输的开销、耗费内存、RDD持久化到内存（内存不够，持续到磁盘）、task创建对象导致gc；</code></pre></li></ol></blockquote><blockquote><ol start="2"><li>广播变量，初始的时候，就在Drvier上有一份副本。</li></ol></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；</span><br><span class="line">如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中；</span><br><span class="line">此后这个executor上的task，都会直接使用本地的BlockManager中的副本。</span><br><span class="line">executor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，举例越近越好。</span><br><span class="line">sc.boradcast();</span><br></pre></td></tr></table></figure><hr><h4 id="使用Kryo序列化"><a href="#使用Kryo序列化" class="headerlink" title="使用Kryo序列化:"></a>使用Kryo序列化:</h4><blockquote><ol><li>默认情况下，Spark内部是使用Java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。</li><li>Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。</li><li>Kryo序列化机制，一旦启用以后，会生效的几个地方：<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、算子函数中使用到的外部变量</span><br><span class="line">2、持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER</span><br><span class="line">3、shuffle</span><br><span class="line">   .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">   .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);</span><br><span class="line">.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;);    </span><br><span class="line">.set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);   </span><br><span class="line">   .registerKryoClasses(new Class[]&#123;CategorySortKey.class&#125;)</span><br></pre></td></tr></table></figure></code></pre></li></ol></blockquote><blockquote><ol start="4"><li>序列化<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2、注册你使用到的，需要通过Kryo序列化的，一些自定义类，SparkConf.registerKryoClasses()</span><br></pre></td></tr></table></figure></code></pre></li></ol></blockquote><hr><h4 id="使用fastutil优化数据格式"><a href="#使用fastutil优化数据格式" class="headerlink" title="使用fastutil优化数据格式:"></a>使用fastutil优化数据格式:</h4><blockquote><ol><li>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；</li><li>fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度；</li></ol></blockquote><p>fastutil也提供了64位的array、set和list，以及高性能快速的，以及实用的IO类，来处理二进制和文本类型的文件；fastutil最新版本要求Java 7以及以上版本；</p><p>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</p><p>fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。<br>fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。</p><blockquote><ol start="3"><li>maven 依赖<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;fastutil&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;fastutil&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;5.0.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li></ol></blockquote><hr><h4 id="调节数据本地化等待时长："><a href="#调节数据本地化等待时长：" class="headerlink" title="调节数据本地化等待时长："></a>调节数据本地化等待时长：</h4><blockquote><ol><li>PROCESS_LOCAL：进程本地化；NODE_LOCAL：节点本地化；NO_PREF：对于task来说，没有好坏之分；RACK_LOCAL：机架本地化；ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差；</li><li>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL（不用调节）、NODE LOCAL、ANY（调节一下数据本地化的等待时长），反复调节，每次调节完以后，再来运行，观察日志</li><li>怎么调节？<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait，默认是3s；6s，10s</span><br><span class="line">默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</span><br><span class="line">spark.locality.wait.process</span><br><span class="line">spark.locality.wait.node</span><br><span class="line">spark.locality.wait.rack</span><br><span class="line">new SparkConf()</span><br><span class="line">  .set(&quot;spark.locality.wait&quot;, &quot;10&quot;)</span><br></pre></td></tr></table></figure></li></ol></blockquote><hr><h4 id="定时清除不需要的数据"><a href="#定时清除不需要的数据" class="headerlink" title="定时清除不需要的数据"></a>定时清除不需要的数据</h4><blockquote><ol><li>通过配置spark.cleaner.ttl为一个合理的值，但是这个值不能过小，因为如果后面计算需要用的数据被清除会带来不必要的麻烦。</li><li>另外通过配置spark.streaming.unpersist为true(默认就是true)来更智能地去持久化（unpersist）RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为。</li></ol></blockquote><hr><h4 id="去除压缩-内存充足的情况下"><a href="#去除压缩-内存充足的情况下" class="headerlink" title="去除压缩 (内存充足的情况下)"></a>去除压缩 (内存充足的情况下)</h4><p>在内存充足的情况下，可以设置spark.rdd.compress 设置为false.</p><hr><h3 id="Yarn-优化"><a href="#Yarn-优化" class="headerlink" title="Yarn 优化"></a>Yarn 优化</h3><h4 id="Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配"><a href="#Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配" class="headerlink" title="Executors和cpu核心数设置和Spark On Yarn 动态资源分配"></a>Executors和cpu核心数设置和Spark On Yarn 动态资源分配</h4><blockquote><p>首先需要对YARN的NodeManager进行配置，使其支持Spark的Shuffle Service。</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#修改</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">#增加</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;spark.shuffle.service.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;7337&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><blockquote><p>将spark中对应jar包拷贝到hadoop的目录下：<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先找到spark版本的spark-&lt;version&gt;-yarn-shuffle.jar</span><br><span class="line">shuffle包，并将该包放到集群所有NodeManager的classpath下，</span><br><span class="line">比如放到HADOOP_HOME/share/hadoop/yarn/lib</span><br></pre></td></tr></table></figure></p></blockquote><hr><h3 id="JVM-调优"><a href="#JVM-调优" class="headerlink" title="JVM 调优"></a>JVM 调优</h3><h4 id="原理概述以及降低cache操作的内存占比"><a href="#原理概述以及降低cache操作的内存占比" class="headerlink" title="原理概述以及降低cache操作的内存占比:"></a>原理概述以及降低cache操作的内存占比:</h4><blockquote><ol><li>full gc / minor gc，无论是快，还是慢，都会导致jvm的工作线程停止工作，stop the world。简而言之，就是说，gc的时候，spark停止工作了。等着垃圾回收结束。</li><li>spark中，堆内存又被划分成了两块儿，存储内存和执行内存；<pre><code>一句话，让task执行算子函数时，有更多的内存可以使用。</code></pre></li></ol></blockquote><hr><h4 id="GC优化策略-暂时不确定"><a href="#GC优化策略-暂时不确定" class="headerlink" title="GC优化策略(暂时不确定)"></a>GC优化策略(暂时不确定)</h4><p>建议用并行Mark-Sweep垃圾回收机制，虽然它消耗更多的资源，但是我们还是建议开启。<br>在spark-submit中使用<br>–driver-java-options “-XX:+UseConcMarkSweepGC”<br>–conf “spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC”</p><h4 id="调节executor堆外内存与连接等待时长-在spark-sbmit中修改-："><a href="#调节executor堆外内存与连接等待时长-在spark-sbmit中修改-：" class="headerlink" title="调节executor堆外内存与连接等待时长(在spark-sbmit中修改)："></a>调节executor堆外内存与连接等待时长(在spark-sbmit中修改)：</h4><blockquote><ol><li>有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行，时不时的报错，shuffle file cannot find，executor、task lost，out of memory（内存溢出）；–conf spark.yarn.executor.memoryOverhead=2048</li><li>有时候，无法建立网络连接；会卡住；ok，spark默认的网络连接的超时时长，是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了:一串file id。uuid（dsfsfd-2342vs–sdf–sdfsd）。not found。file lost。<br> –conf spark.core.connection.ack.wait.timeout=300</li></ol></blockquote><hr><h3 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h3><h4 id="原理概述："><a href="#原理概述：" class="headerlink" title="原理概述："></a>原理概述：</h4><blockquote><ol><li>在spark中，主要是以下几个算子：groupByKey、reduceByKey、countByKey、join，等等。</li><li>shuffle，一定是分为两个stage来完成的。因为这其实是个逆向的过程，不是stage决定shuffle，是shuffle决定stage。</li><li>shuffle前半部分的task在写入数据到磁盘文件之前，都会先写入一个一个的内存缓冲，内存缓冲满溢之后，再spill溢写到磁盘文件中。</li></ol></blockquote><hr><h4 id="合并map端输出文件："><a href="#合并map端输出文件：" class="headerlink" title="合并map端输出文件："></a>合并map端输出文件：</h4><blockquote><ol><li>开启shuffle map端输出文件合并的机制；默认情况下，是不开启的，就是会发生如上所述的大量map端输出文件的操作，严重影响性能。</li><li>new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)<br>new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)</li></ol></blockquote><hr><h4 id="合并map端输出文件：-1"><a href="#合并map端输出文件：-1" class="headerlink" title="合并map端输出文件："></a>合并map端输出文件：</h4><blockquote><ol><li>map端内存缓冲：spark.shuffle.file.buffer，默认32k  <pre><code>reduce端内存占比：spark.shuffle.memoryFraction，0.2</code></pre></li><li>调节的时候的原则。spark.shuffle.file.buffer，每次扩大一倍，然后看看效果，64，128；<br>spark.shuffle.memoryFraction，每次提高0.1，看看效果。<br>不能调节的太大，太大了以后过犹不及，因为内存资源是有限的，你这里调节的太大了，其他环节的内存使用就会有问题了。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new SparkConf().set(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)</span><br><span class="line">new SparkConf().set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.3&quot;)</span><br></pre></td></tr></table></figure></li></ol></blockquote><hr><h4 id="HashShuffleManager与SortShuffleManager"><a href="#HashShuffleManager与SortShuffleManager" class="headerlink" title="HashShuffleManager与SortShuffleManager"></a>HashShuffleManager与SortShuffleManager</h4><blockquote><ol><li>spark.shuffle.manager：hash、sort、tungsten-sort（自己实现内存管理），spark 1.2.x版本以后，默认的shuffle manager，是SortShuffleManager。<br>   spark.shuffle.sort.bypassMergeThreshold：200（默认值为200）</li><li>SortShuffleManager会避免像HashShuffleManager那样，默认就去创建多份磁盘文件。一个task，只会写入一个磁盘文件，不同reduce task的数据，用offset来划分界定。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)</span><br><span class="line">new SparkConf().set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;)</span><br></pre></td></tr></table></figure></li></ol></blockquote><hr><h3 id="算子调优"><a href="#算子调优" class="headerlink" title="算子调优"></a>算子调优</h3><h4 id="MapPartitions提升Map类操作性能"><a href="#MapPartitions提升Map类操作性能" class="headerlink" title="MapPartitions提升Map类操作性能:"></a>MapPartitions提升Map类操作性能:</h4><blockquote><ol><li>如果是普通的map，比如一个partition中有1万条数据；function要执行和计算1万次。但是，使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。</li><li>但是，可能就OOM，内存溢出。   </li></ol></blockquote><hr><h4 id="filter过后使用coalesce减少分区数量："><a href="#filter过后使用coalesce减少分区数量：" class="headerlink" title="filter过后使用coalesce减少分区数量："></a>filter过后使用coalesce减少分区数量：</h4><blockquote><ol><li>就会导致有些task运行的速度很快；有些task运行的速度很慢。这就是数据倾斜。</li><li>coalesce算子：主要就是用于在filter操作之后，针对每个partition的数据量各不相同的情况，来压缩partition的数量。减少partition的数量，而且让每个partition的数据量都尽量均匀紧凑。</li></ol></blockquote><hr><h4 id="foreachPartition优化写数据库性能："><a href="#foreachPartition优化写数据库性能：" class="headerlink" title="foreachPartition优化写数据库性能："></a>foreachPartition优化写数据库性能：</h4><pre><code>&gt; 1. 用了foreachPartition算子之后，好处在哪里？    1、对于我们写的function函数，就调用一次，一次传入一个partition所有数据；    2、主要创建或者获取一个数据库连接就可以；    3、只要向数据库发送一次SQL语句和多组参数即可；&gt; 2. 很有可能会发生OOM，内存溢出的问题。    一个partition大概是1千条左右用foreach，跟用foreachPartition，性能的提升达到了2~3分钟。</code></pre><hr><h4 id="repartition解决Spark-SQL低并行度的性能问题："><a href="#repartition解决Spark-SQL低并行度的性能问题：" class="headerlink" title="repartition解决Spark SQL低并行度的性能问题："></a>repartition解决Spark SQL低并行度的性能问题：</h4><pre><code>repartition算子，你用Spark SQL这一步的并行度和task数量，肯定是没有办法去改变了。但是呢，可以将你用Spark SQL查询出来的RDD，使用repartition算子，去重新进行分区，此时可以分区成多个partition，比如从20个partition，分区成100个。</code></pre><hr><h4 id="reduceByKey本地聚合介绍："><a href="#reduceByKey本地聚合介绍：" class="headerlink" title="reduceByKey本地聚合介绍："></a>reduceByKey本地聚合介绍：</h4><pre><code>reduceByKey，相较于普通的shuffle操作（比如groupByKey），它的一个特点，就是说，会进行map端的本地聚合</code></pre><h3 id="代码-调优"><a href="#代码-调优" class="headerlink" title="代码 调优"></a>代码 调优</h3><h4 id="进行HA机制处理-针对Driver高可用性"><a href="#进行HA机制处理-针对Driver高可用性" class="headerlink" title="进行HA机制处理-针对Driver高可用性"></a>进行HA机制处理-针对Driver高可用性</h4><blockquote><p>在创建和启动StreamingContext的时候，将元数据写入容错的文件系统（比如hdfs）。保证在driver挂掉之后，spark集群可以自己将driver重新启动起来；而且driver在启动的时候，不会重新创建一个streaming context，而是从容错文件系统（比如hdfs）中读取之前的元数据信息，包括job的执行进度，继续接着之前的进度，继续执行。使用这种机制，就必须使用cluster模式提交，确保driver运行在某个worker上面；</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() &#123;</span><br><span class="line">     @Override</span><br><span class="line">    public JavaStreamingContext create() &#123;</span><br><span class="line">      JavaStreamingContext jssc = new JavaStreamingContext(...);</span><br><span class="line">     JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...);</span><br><span class="line">     jssc.checkpoint(checkpointDirectory);</span><br><span class="line">     return jssc;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;;</span><br><span class="line">JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);</span><br><span class="line">context.start();</span><br><span class="line">context.awaitTermination();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">JavaStreamingContext.getOrCreate 基于Function0&lt; JavaStreamingContext &gt; 进行Driver高可用</span><br><span class="line"> Function0&lt;JavaStreamingContext&gt; createContextFunc = new Function0&lt;JavaStreamingContext&gt;()&#123;</span><br><span class="line"> @Override</span><br><span class="line"> public JavaStreamingContext call() throws Exception</span><br><span class="line"> &#123;</span><br><span class="line"> conf = new SparkConf()</span><br><span class="line"> .setMaster(&quot;local[4]&quot;)</span><br><span class="line"> .setAppName(&quot;java/RealTimeStreaming&quot;)</span><br><span class="line"> .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line"> .set(&quot;spark.default.parallelism&quot;, &quot;10&quot;)</span><br><span class="line"> .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;)</span><br><span class="line"> .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);</span><br><span class="line"> Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();</span><br><span class="line"> kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);</span><br><span class="line"> kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);</span><br><span class="line"> kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);</span><br><span class="line"> kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);</span><br><span class="line"> kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span><br><span class="line"> kafkaParams.put(&quot;enable.auto.commit&quot;,true);</span><br><span class="line"></span><br><span class="line"> JavaStreamingContext jssc = new JavaStreamingContext(</span><br><span class="line"> conf, Durations.seconds(30));</span><br><span class="line"> jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;);</span><br><span class="line"></span><br><span class="line"> // 构建topic set</span><br><span class="line"> String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);</span><br><span class="line"> String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);</span><br><span class="line"> Set&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line"> for(String kafkaTopic : kafkaTopicsSplited) &#123;</span><br><span class="line"> topics.add(kafkaTopic);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc,</span><br><span class="line"> LocationStrategies.PreferConsistent(),</span><br><span class="line"> ConsumerStrategies.Subscribe(topics, kafkaParams));</span><br><span class="line"></span><br><span class="line"> hostMap = adRealTimeLogDStream.mapToPair(record -&gt; new Tuple2&lt;String, String&gt;(record.key(), record.value()));</span><br><span class="line"></span><br><span class="line"> logPeakDstream  = hostMap.mapToPair(new PairFunction&lt;Tuple2&lt;String, String&gt;, String, Long&gt;() &#123;</span><br><span class="line"> @Override</span><br><span class="line"> public Tuple2&lt;String,Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123;</span><br><span class="line"> String log = tuple._2;</span><br><span class="line"> String[] logSplited = log.split(&quot;\\|&quot;);</span><br><span class="line"> String eventTime= logSplited[1];</span><br><span class="line"> String todayDate = DATE_FORMAT.format(new Date()).trim();</span><br><span class="line"> String cutTime= eventTime.substring(13,eventTime.length()-7);</span><br><span class="line"></span><br><span class="line"> String ip = logSplited[0].trim();</span><br><span class="line"> String host = logSplited[14].trim();</span><br><span class="line"> return new Tuple2&lt;String, Long&gt;(host+&quot;-&quot;+ip, 1L);</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;);</span><br><span class="line"></span><br><span class="line"> hostReduce = logPeakDstream.reduceByKeyAndWindow(new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line"> @Override</span><br><span class="line"> public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line"> return v1 + v2;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;, Durations.minutes(10),Durations.seconds(30));</span><br><span class="line"></span><br><span class="line"> JavaPairDStream&lt;String, Long&gt; topNPairRdd = hostReduce.transformToPair(new Function&lt;JavaPairRDD&lt;String, Long&gt;, JavaPairRDD&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line"> @Override</span><br><span class="line"> public JavaPairRDD&lt;String, Long&gt; call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception &#123;</span><br><span class="line"> JavaPairRDD&lt;Long, String&gt; sortRDD = (JavaPairRDD&lt;Long, String&gt;) rdd.mapToPair(record -&gt; new Tuple2&lt;Long, String&gt;(record._2, record._1));</span><br><span class="line"> JavaPairRDD&lt;String, Long&gt; sortedRdd = (JavaPairRDD&lt;String, Long&gt;) sortRDD.sortByKey(false).mapToPair(record -&gt; new Tuple2&lt;String, Long&gt;(record._2, record._1));</span><br><span class="line"></span><br><span class="line"> List&lt;Tuple2&lt;String, Long&gt;&gt; topNs = sortedRdd.take(5);//取前5个输出</span><br><span class="line"> System.out.println(&quot;                                                 &quot;);</span><br><span class="line"> System.out.println(&quot;*****************峰值访问窗统计*******************&quot;);</span><br><span class="line"> for (Tuple2&lt;String, Long&gt; topN : topNs) &#123;</span><br><span class="line"> System.out.println(topN);</span><br><span class="line"> &#125;</span><br><span class="line"> System.out.println(&quot;**********************END***********************&quot;);</span><br><span class="line"> System.out.println(&quot;                                                 &quot;);</span><br><span class="line"> return sortedRdd;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;);</span><br><span class="line"></span><br><span class="line"> topNPairRdd.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line"> @Override</span><br><span class="line"> public void call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception &#123;</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"> &#125;);</span><br><span class="line"></span><br><span class="line"> logDetailDstream = hostMap.map(new Function&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123;</span><br><span class="line"> @Override</span><br><span class="line"> public String call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123;</span><br><span class="line"> String log = tuple._2;</span><br><span class="line"> String[] logSplited = log.split(&quot;\\|&quot;);</span><br><span class="line"> String eventTime= logSplited[1];</span><br><span class="line"> String todayDate = DATE_FORMAT.format(new Date()).trim();</span><br><span class="line"> String cutTime= eventTime.substring(13,eventTime.length()-7);</span><br><span class="line"> String[] urlDetails = logSplited[7].split(&quot;/&quot;);</span><br><span class="line"> String ip = logSplited[0].trim();</span><br><span class="line"></span><br><span class="line"> String url =&quot;&quot;;</span><br><span class="line"> if(urlDetails.length==4)&#123;</span><br><span class="line"> url = urlDetails[3];</span><br><span class="line"> &#125;else if(urlDetails.length==5)&#123;</span><br><span class="line"> url = urlDetails[3] + &quot;/&quot; + urlDetails[4];</span><br><span class="line"> &#125;else if(urlDetails.length&gt;=6)&#123;</span><br><span class="line"> url = urlDetails[3] + &quot;/&quot; + urlDetails[4]+ &quot;/&quot; + urlDetails[5];</span><br><span class="line"> &#125;</span><br><span class="line"> String host = logSplited[14].trim();</span><br><span class="line"> String dataTime =todayDate +&quot; &quot;+ cutTime;</span><br><span class="line"> String bytesSent = logSplited[5].trim();</span><br><span class="line"> return  dataTime+&quot; &quot;+host+&quot; &quot;+ip+&quot; &quot;+url+&quot; &quot;+bytesSent;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;);</span><br><span class="line"> //logDetailDstream.print();</span><br><span class="line"> return jssc;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;;</span><br><span class="line"> return createContextFunc;</span><br></pre></td></tr></table></figure> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">提交方式</span><br><span class="line"> spark-submit</span><br><span class="line">         --deploy-mode cluster</span><br><span class="line">         --supervise</span><br></pre></td></tr></table></figure><hr><h4 id="SparkStreaming-与kafka整合调优"><a href="#SparkStreaming-与kafka整合调优" class="headerlink" title="SparkStreaming 与kafka整合调优"></a>SparkStreaming 与kafka整合调优</h4><blockquote><p>LocationStrategies 位置策略：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The new Kafka consumer API will pre-fetch messages into buffers. Therefore it is important for performance </span><br><span class="line">reasons that the Spark integration keep cached consumers on executors (rather than recreating them for each </span><br><span class="line">batch), and prefer to schedule partitions on the host locations that have the appropriate consumers.</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>新的Kafka消费者API可以预获取消息缓存到缓冲区，因此Spark整合Kafka让消费者在executor上进行缓存对性能是非常有助的，可以调度消费者所在主机位置的分区。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In most cases, you should use LocationStrategies.PreferConsistent as shown above. This will distribute partitions </span><br><span class="line">evenly across available executors. If your executors are on the same hosts as your Kafka brokers, use PreferBrokers,</span><br><span class="line">which will prefer to schedule partitions on the Kafka leader for that partition. Finally, if you have a significant </span><br><span class="line">skew in load among partitions, use PreferFixed. This allows you to specify an explicit mapping of partitions to </span><br><span class="line">hosts (any unspecified partitions will use a consistent location).</span><br></pre></td></tr></table></figure><blockquote><p>通常，你可以使用 LocationStrategies.PreferConsistent，这个策略会将分区分布到所有可获得的executor上。如果你的executor和kafkabroker在同一主机上的话，可以使用PreferBrokers，这样kafka leader会为此分区进行调度。最后，如果你加载数据有倾斜的话可以使用PreferFixed，这将允许你制定一个分区和主机的映射（没有指定的分区将使用PreferConsistent 策略）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The cache for consumers has a default maximum size of 64. If you expect to be handling more than </span><br><span class="line">(64 * number of executors) Kafka partitions, you can change this setting</span><br><span class="line">via spark.streaming.kafka.consumer.cache.maxCapacity</span><br></pre></td></tr></table></figure><blockquote><p>消费者默认缓存大小是64，如果你期望处理较大的Kafka分区的话，你可以使用</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.kafka.consumer.cache.maxCapacity设置大小。</span><br><span class="line">The cache is keyed by topicpartition and group.id, so use a separate group.id for each call to createDirectStream.</span><br></pre></td></tr></table></figure><blockquote><p>缓存是使用key为topic partition 和组id的，因此对于每一次调用 createDirectStream 可以使用不同的 group . id</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public  static SparkConf  conf = new SparkConf()</span><br><span class="line">.setMaster(&quot;local[4]&quot;)</span><br><span class="line">.setAppName(&quot;java/RealTimeStreaming&quot;)</span><br><span class="line">.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">.set(&quot;spark.default.parallelism&quot;, &quot;10&quot;)</span><br><span class="line">.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;)</span><br><span class="line">.set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);</span><br><span class="line">Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();</span><br><span class="line">kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);</span><br><span class="line">kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);</span><br><span class="line">kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);</span><br><span class="line">kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);</span><br><span class="line">kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span><br><span class="line">kafkaParams.put(&quot;enable.auto.commit&quot;,true);</span><br><span class="line"></span><br><span class="line">JavaStreamingContext jssc = new JavaStreamingContext(</span><br><span class="line">conf, Durations.seconds(30));</span><br><span class="line">jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>构建topic set</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);</span><br><span class="line">String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);</span><br><span class="line">Set&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line">for(String kafkaTopic : kafkaTopicsSplited) &#123;</span><br><span class="line">topics.add(kafkaTopic);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc,</span><br><span class="line">LocationStrategies.PreferConsistent(),</span><br><span class="line">ConsumerStrategies.Subscribe(topics, kafkaParams));</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;性能调优&quot;&gt;&lt;a href=&quot;#性能调优&quot; class=&quot;headerlink&quot; title=&quot;性能调优&quot;&gt;&lt;/a&gt;性能调优&lt;/h3&gt;&lt;h4 id=&quot;分配资源：&quot;&gt;&lt;a href=&quot;#分配资源：&quot; class=&quot;headerlink&quot; title=&quot;分配资源：&quot;&gt;
      
    
    </summary>
    
      <category term="spark" scheme="https://23yue23.github.io/categories/spark/"/>
    
    
      <category term="spark-调优篇" scheme="https://23yue23.github.io/tags/spark-%E8%B0%83%E4%BC%98%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>grafana-学习篇</title>
    <link href="https://23yue23.github.io/2019/04/28/grafana-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    <id>https://23yue23.github.io/2019/04/28/grafana-学习篇/</id>
    <published>2019-04-28T03:38:55.000Z</published>
    <updated>2019-09-25T02:38:02.575Z</updated>
    
    <content type="html"><![CDATA[<h5 id="参考地址："><a href="#参考地址：" class="headerlink" title="参考地址："></a>参考地址：</h5><blockquote><ol><li><a href="https://grafana.com" target="_blank" rel="noopener">官网</a></li><li><a href="https://grafana.com/docs/tutorials/screencasts/" target="_blank" rel="noopener">官网视频</a></li></ol></blockquote><h5 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h5><blockquote><ol><li><a href="https://grafana.com/docs/plugins/installation/" target="_blank" rel="noopener">插件安装步骤</a></li><li><a href="https://grafana.com/plugins" target="_blank" rel="noopener">插件搜索</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;参考地址：&quot;&gt;&lt;a href=&quot;#参考地址：&quot; class=&quot;headerlink&quot; title=&quot;参考地址：&quot;&gt;&lt;/a&gt;参考地址：&lt;/h5&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://grafana.com&quot; target=&quot;_
      
    
    </summary>
    
      <category term="grafana" scheme="https://23yue23.github.io/categories/grafana/"/>
    
    
      <category term="grafana-学习篇" scheme="https://23yue23.github.io/tags/grafana-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
    
  </entry>
  
  <entry>
    <title>markDown-使用</title>
    <link href="https://23yue23.github.io/2019/04/25/markDown-%E4%BD%BF%E7%94%A8/"/>
    <id>https://23yue23.github.io/2019/04/25/markDown-使用/</id>
    <published>2019-04-25T07:37:41.000Z</published>
    <updated>2019-09-25T02:38:48.412Z</updated>
    
    <content type="html"><![CDATA[<h6 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># h1  最大</span><br><span class="line">## h2</span><br><span class="line">### h3</span><br><span class="line">#### h4</span><br><span class="line">##### h5</span><br><span class="line">###### h6 最小</span><br></pre></td></tr></table></figure><hr><h6 id="段落及区块引用"><a href="#段落及区块引用" class="headerlink" title="段落及区块引用"></a>段落及区块引用</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;</span><br></pre></td></tr></table></figure><hr><h6 id="插入链接和图片"><a href="#插入链接和图片" class="headerlink" title="插入链接和图片"></a>插入链接和图片</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">链接 []()</span><br><span class="line">[点击跳转至百度](http://www.baidu.com)</span><br><span class="line">图片 ![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)</span><br><span class="line">![图片](https://user-gold-cdn.xitu.io/2018/4/18/162d75d959444389?w=1240&amp;h=703&amp;f=jpeg&amp;s=56927)</span><br></pre></td></tr></table></figure><hr><h6 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">* | + | - 是无序列表</span><br><span class="line">1. 数字点加空格 是有序列表</span><br><span class="line"></span><br><span class="line">列表中加入了区块引用，区域引用标记符也需要缩进4个空格</span><br><span class="line">上一级和下一级之间敲三个空格即可</span><br><span class="line">示例：</span><br><span class="line">* 段落一</span><br><span class="line">    &gt; 区块标记一</span><br><span class="line">      &gt;&gt;区块标记二</span><br><span class="line">* 段落二</span><br><span class="line">    &gt; 区块标记二</span><br></pre></td></tr></table></figure><hr><h6 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">***</span><br></pre></td></tr></table></figure><hr><h6 id="强调"><a href="#强调" class="headerlink" title="强调"></a>强调</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*这里是斜体*</span><br><span class="line">**这里是加粗**</span><br><span class="line">***这里是斜线加粗***</span><br><span class="line">～～这里是删除线～～</span><br></pre></td></tr></table></figure><hr><h6 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">单行代码：单反引号包裹</span><br><span class="line">代码块：三个反引号包裹。</span><br></pre></td></tr></table></figure><hr><h6 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">表头|条目一|条目二</span><br><span class="line">:---:|:---:|:---:</span><br><span class="line">项目|项目一|项目二</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">第二行分割表头和内容。</span><br><span class="line">- 有一个就行，为了对齐，多加了几个</span><br><span class="line">文字默认居左</span><br><span class="line">-两边加：表示文字居中</span><br><span class="line">-右边加：表示文字居右</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h6 id=&quot;标签&quot;&gt;&lt;a href=&quot;#标签&quot; class=&quot;headerlink&quot; title=&quot;标签&quot;&gt;&lt;/a&gt;标签&lt;/h6&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span 
      
    
    </summary>
    
      <category term="工具" scheme="https://23yue23.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="markDown-使用" scheme="https://23yue23.github.io/tags/markDown-%E4%BD%BF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>启程</title>
    <link href="https://23yue23.github.io/2019/04/24/%E5%90%AF%E7%A8%8B/"/>
    <id>https://23yue23.github.io/2019/04/24/启程/</id>
    <published>2019-04-24T06:02:32.000Z</published>
    <updated>2019-04-24T08:36:28.646Z</updated>
    
    <content type="html"><![CDATA[<hr><p>我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，<strong>Cmd Markdown</strong> 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：</p><blockquote><ul><li>整理知识，学习笔记</li><li>发布日记，杂文，所见所想</li><li>撰写发布技术文稿（代码支持）</li><li>撰写发布学术论文（LaTeX 公式支持）</li></ul></blockquote><p><img src="https://www.zybuluo.com/static/img/logo.png" alt="cmd-markdown-logo"></p><p>除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载：</p><h3 id="Windows-Mac-Linux-全平台客户端"><a href="#Windows-Mac-Linux-全平台客户端" class="headerlink" title="Windows/Mac/Linux 全平台客户端"></a><a href="https://www.zybuluo.com/cmd/" target="_blank" rel="noopener">Windows/Mac/Linux 全平台客户端</a></h3><blockquote><p>请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 <i class="icon-file"></i> <strong>新文稿</strong> 或者使用快捷键 <code>Ctrl+Alt+N</code>。</p></blockquote><hr><h2 id="什么是-Markdown"><a href="#什么是-Markdown" class="headerlink" title="什么是 Markdown"></a>什么是 Markdown</h2><p>Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，<strong>粗体</strong> 或者 <em>斜体</em> 某些文字，更棒的是，它还可以</p><h3 id="1-制作一份待办事宜-Todo-列表"><a href="#1-制作一份待办事宜-Todo-列表" class="headerlink" title="1. 制作一份待办事宜 Todo 列表"></a>1. 制作一份待办事宜 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#13-待办事宜-todo-列表" target="_blank" rel="noopener">Todo 列表</a></h3><ul><li style="list-style: none"><input type="checkbox"> 支持以 PDF 格式导出文稿</li><li style="list-style: none"><input type="checkbox"> 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率</li><li style="list-style: none"><input type="checkbox" checked> 新增 Todo 列表功能</li><li style="list-style: none"><input type="checkbox" checked> 修复 LaTex 公式渲染问题</li><li style="list-style: none"><input type="checkbox" checked> 新增 LaTex 公式编号功能</li></ul><h3 id="2-书写一个质能守恒公式-LaTeX"><a href="#2-书写一个质能守恒公式-LaTeX" class="headerlink" title="2. 书写一个质能守恒公式[^LaTeX]"></a>2. 书写一个质能守恒公式[^LaTeX]</h3><p>$$E=mc^2$$</p><h3 id="3-高亮一段代码-code"><a href="#3-高亮一段代码-code" class="headerlink" title="3. 高亮一段代码[^code]"></a>3. 高亮一段代码[^code]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@requires_authorization</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeClass</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># A comment</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'hello world'</span></span><br></pre></td></tr></table></figure><h3 id="4-高效绘制-流程图"><a href="#4-高效绘制-流程图" class="headerlink" title="4. 高效绘制 流程图"></a>4. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#7-流程图" target="_blank" rel="noopener">流程图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: Start</span><br><span class="line">op=&gt;operation: Your Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line"></span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure><h3 id="5-高效绘制-序列图"><a href="#5-高效绘制-序列图" class="headerlink" title="5. 高效绘制 序列图"></a>5. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#8-序列图" target="_blank" rel="noopener">序列图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Alice-&gt;Bob: Hello Bob, how are you?</span><br><span class="line">Note right of Bob: Bob thinks</span><br><span class="line">Bob--&gt;Alice: I am good thanks!</span><br></pre></td></tr></table></figure><h3 id="6-高效绘制-甘特图"><a href="#6-高效绘制-甘特图" class="headerlink" title="6. 高效绘制 甘特图"></a>6. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#9-甘特图" target="_blank" rel="noopener">甘特图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">title 项目开发流程</span><br><span class="line">section 项目确定</span><br><span class="line">    需求分析       :a1, 2016-06-22, 3d</span><br><span class="line">    可行性报告     :after a1, 5d</span><br><span class="line">    概念验证       : 5d</span><br><span class="line">section 项目实施</span><br><span class="line">    概要设计      :2016-07-05  , 5d</span><br><span class="line">    详细设计      :2016-07-08, 10d</span><br><span class="line">    编码          :2016-07-15, 10d</span><br><span class="line">    测试          :2016-07-22, 5d</span><br><span class="line">section 发布验收</span><br><span class="line">    发布: 2d</span><br><span class="line">    验收: 3d</span><br></pre></td></tr></table></figure><h3 id="7-绘制表格"><a href="#7-绘制表格" class="headerlink" title="7. 绘制表格"></a>7. 绘制表格</h3><table><thead><tr><th>项目</th><th style="text-align:right">价格</th><th style="text-align:center">数量</th></tr></thead><tbody><tr><td>计算机</td><td style="text-align:right">\$1600</td><td style="text-align:center">5</td></tr><tr><td>手机</td><td style="text-align:right">\$12</td><td style="text-align:center">12</td></tr><tr><td>管线</td><td style="text-align:right">\$1</td><td style="text-align:center">234</td></tr></tbody></table><h3 id="8-更详细语法说明"><a href="#8-更详细语法说明" class="headerlink" title="8. 更详细语法说明"></a>8. 更详细语法说明</h3><p>想要查看更详细的语法说明，可以参考我们准备的 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown" target="_blank" rel="noopener">Cmd Markdown 简明语法手册</a>，进阶用户可以参考 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#cmd-markdown-高阶语法手册" target="_blank" rel="noopener">Cmd Markdown 高阶语法手册</a> 了解更多高级功能。</p><p>总而言之，不同于其它 <em>所见即所得</em> 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。<strong>Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。</strong> 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。</p><hr><h2 id="什么是-Cmd-Markdown"><a href="#什么是-Cmd-Markdown" class="headerlink" title="什么是 Cmd Markdown"></a>什么是 Cmd Markdown</h2><p>您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 <strong>编辑/发布/阅读</strong> Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。</p><h3 id="1-实时同步预览"><a href="#1-实时同步预览" class="headerlink" title="1. 实时同步预览"></a>1. 实时同步预览</h3><p>我们将 Cmd Markdown 的主界面一分为二，左边为<strong>编辑区</strong>，右边为<strong>预览区</strong>，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！</p><h3 id="2-编辑工具栏"><a href="#2-编辑工具栏" class="headerlink" title="2. 编辑工具栏"></a>2. 编辑工具栏</h3><p>也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 <strong>编辑区</strong> 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。</p><p><img src="https://www.zybuluo.com/static/img/toolbar-editor.png" alt="tool-editor"></p><h3 id="3-编辑模式"><a href="#3-编辑模式" class="headerlink" title="3. 编辑模式"></a>3. 编辑模式</h3><p>完全心无旁骛的方式编辑文字：点击 <strong>编辑工具栏</strong> 最右侧的拉伸按钮或者按下 <code>Ctrl + M</code>，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！</p><h3 id="4-实时的云端文稿"><a href="#4-实时的云端文稿" class="headerlink" title="4. 实时的云端文稿"></a>4. 实时的云端文稿</h3><p>为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 <strong>编辑工具栏</strong> 的最右侧提示 <code>已保存</code> 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。</p><h3 id="5-离线模式"><a href="#5-离线模式" class="headerlink" title="5. 离线模式"></a>5. 离线模式</h3><p>在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。</p><h3 id="6-管理工具栏"><a href="#6-管理工具栏" class="headerlink" title="6. 管理工具栏"></a>6. 管理工具栏</h3><p>为了便于管理您的文稿，在 <strong>预览区</strong> 的顶部放置了如下所示的 <strong>管理工具栏</strong>：</p><p><img src="https://www.zybuluo.com/static/img/toolbar-manager.jpg" alt="tool-manager"></p><p>通过管理工具栏可以：</p><p><i class="icon-share"></i> 发布：将当前的文稿生成固定链接，在网络上发布，分享<br><i class="icon-file"></i> 新建：开始撰写一篇新的文稿<br><i class="icon-trash"></i> 删除：删除当前的文稿<br><i class="icon-cloud"></i> 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地<br><i class="icon-reorder"></i> 列表：所有新增和过往的文稿都可以在这里查看、操作<br><i class="icon-pencil"></i> 模式：切换 普通/Vim/Emacs 编辑模式</p><h3 id="7-阅读工具栏"><a href="#7-阅读工具栏" class="headerlink" title="7. 阅读工具栏"></a>7. 阅读工具栏</h3><p><img src="https://www.zybuluo.com/static/img/toolbar-reader.jpg" alt="tool-manager"></p><p>通过 <strong>预览区</strong> 右上角的 <strong>阅读工具栏</strong>，可以查看当前文稿的目录并增强阅读体验。</p><p>工具栏上的五个图标依次为：</p><p><i class="icon-list"></i> 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落<br><i class="icon-chevron-sign-left"></i> 视图：互换左边编辑区和右边预览区的位置<br><i class="icon-adjust"></i> 主题：内置了黑白两种模式的主题，试试 <strong>黑色主题</strong>，超炫！<br><i class="icon-desktop"></i> 阅读：心无旁骛的阅读模式提供超一流的阅读体验<br><i class="icon-fullscreen"></i> 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境</p><h3 id="8-阅读模式"><a href="#8-阅读模式" class="headerlink" title="8. 阅读模式"></a>8. 阅读模式</h3><p>在 <strong>阅读工具栏</strong> 点击 <i class="icon-desktop"></i> 或者按下 <code>Ctrl+Alt+M</code> 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。</p><h3 id="9-标签、分类和搜索"><a href="#9-标签、分类和搜索" class="headerlink" title="9. 标签、分类和搜索"></a>9. 标签、分类和搜索</h3><p>在编辑区任意行首位置输入以下格式的文字可以标签当前文档：</p><p>标签： 未分类</p><p>标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示：</p><p><img src="https://www.zybuluo.com/static/img/file-list.png" alt="file-list"></p><h3 id="10-文稿发布和分享"><a href="#10-文稿发布和分享" class="headerlink" title="10. 文稿发布和分享"></a>10. 文稿发布和分享</h3><p>在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 <i class="icon-share"></i> (Ctrl+Alt+P) 发布这份文档给好友吧！</p><hr><p>再一次感谢您花费时间阅读这份欢迎稿，点击 <i class="icon-file"></i> (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！</p><p>作者 <a href="http://weibo.com/ghosert" target="_blank" rel="noopener">@ghosert</a><br>2016 年 07月 07日    </p><p>[^LaTeX]: 支持 <strong>LaTeX</strong> 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 <a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">MathJax</a> 参考更多使用方法。</p><p>[^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，<strong>四十一</strong>种主流编程语言。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，&lt;strong&gt;Cmd Markdown&lt;/strong&gt; 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：&lt;/p&gt;

      
    
    </summary>
    
      <category term="其他" scheme="https://23yue23.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="其他" scheme="https://23yue23.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
</feed>
