<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[面试资料]]></title>
    <url>%2F2019%2F09%2F25%2F%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[1.语言类 scala scala sql 知识扩展：1.推荐系统实现 2.用户行为数据收集系统-(1)简介 3.用户行为数据收集系统–(2)客户端SDK设计 4.用户行为数据收集系统–(3)数据接收端设计 5.大数据世界]]></content>
      <tags>
        <tag>面试资料</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[promethus-问题篇]]></title>
    <url>%2F2019%2F09%2F25%2Fpromethus-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[address already in use 保存信息：1error starting web server: listen tcp 0.0.0.0:9090: bind: address already in use 解决方法：12lsof -i :9090sudo kill -9 prot ####]]></content>
      <categories>
        <category>promethus</category>
      </categories>
      <tags>
        <tag>promethus-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[promethus-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Fpromethus-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[####grafana 安装和常用配置 #####常用配置12345678配置： 配置文件应位于/usr/local/etc/grafana/grafana.ini。日志： 日志文件应位于/usr/local/var/log/grafana/grafana.log。插件： 如果你想手动安装一个插件，请点击此处：/usr/local/var/lib/grafana/plugins。数据库： 默认的sqlite数据库位于 /usr/local/var/lib/grafana 下载安装1234wget https://dl.grafana.com/oss/release/grafana-6.3.5.linux-amd64.tar.gztar -zxvf grafana-6.3.5.linux-amd64.tar.gzconf/grafana.ini./bin/grafana-server web mac 安装方式123456789101112131415安装：brew install grafana执行：brew services start grafana重启：brew services restart grafana#Log/usr/local/var/log/grafana/grafana.log#conf/usr/local/etc/grafana/grafana.ini#plugins/usr/local/var/lib/grafana/plugins#database/usr/local/var/lib/grafana Metrics 类型Counter（计数器）123456#获取样本数据node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle”&#125; #通过rate()函数获取HTTP请求量的增长率rate(http_requests_total[5m])# 查询当前系统中，访问量前10的HTTP地址：topk(10, http_requests_total) Gauge 可增可减的仪表盘12预测系统磁盘空间在4个小时之后的剩余情况：predict_linear(node_filesystem_free&#123;job=&quot;node&quot;&#125;[1h], 4 * 3600) Histogram和Summary分析数据分布情况 PromQL基础基本函数语法123prometheus_http_requests_total 等价于prometheus_http_requests_total&#123;&#125; #####条件筛选123prometheus_http_requests_total&#123;instance=&quot;localhost:9090”&#125;不满足条件prometheus_http_requests_total&#123;instance!=&quot;localhost:9090”&#125; 123支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离：使用label=~regx表示选择那些标签符合正则表达式定义的时间序列； 反之使用label!~regx进行排除http_requests_total&#123;environment=~&quot;staging|testing|development&quot;,method!=&quot;GET”&#125; 范围查询123例如：查询最近5分钟http_request_total&#123;&#125;[5m] s - 秒 m - 分钟 h - 小时 d - 天 w - 周 y - 年 123瞬时向量表达式或者区间向量表达式中，都是以当前时间为基准http_request_total&#123;&#125; # 瞬时向量表达式，选择当前最新的数据http_request_total&#123;&#125;[5m] # 区间向量表达式，选择以当前时间为基准，5分钟内的数据 123如果我们想查询，5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据呢? 这个时候我们就可以使用位移操作，位移操作的关键字为offsethttp_request_total&#123;&#125; offset 5m http_request_total&#123;&#125;[1d] offset 1d 聚合操作12 查询系统所有http请求的总量sum(http_request_total) 12 按照mode计算主机CPU的平均使用时间avg(node_cpu) by (mode) 12 按照主机查询各个主机的CPU使用率sum(sum(irate(node_cpu&#123;mode!=&apos;idle&apos;&#125;[5m])) / sum(irate(node_cpu[5m]))) by (instance) 支持的算法： (加法) - (减法) * (乘法) / (除法) % (求余) ^ (幂运算)]]></content>
      <categories>
        <category>promethus</category>
      </categories>
      <tags>
        <tag>promethus-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cassandra-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fcassandra-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Youte 视频12345DataStax Accelerate 2019 — Full Session Recordings： https://www.youtube.com/playlist?list=PLm-EPIkBI3YpJbuKUGDlZVNHzT0umcBSl 大概85个视频，涉及到cassandra各个方面。下次会议是 https://apachecon.com/acna19/s/#/schedule?search=cassandra 可以关注下https://issues.apache.org/jira/browse/CASSANDRA-11815]]></content>
      <categories>
        <category>cassandra</category>
      </categories>
      <tags>
        <tag>cassandra-资料篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zeppelin-问题篇]]></title>
    <url>%2F2019%2F09%2F25%2Fzeppelin-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[hive task 失败 报错信息： 1ERROR [bc60fbe1-ee2a-49e5-8932-231b92250134 main] ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask 问题原因： 1作业被kill 掉后，会出现以上信息。 Apache zeppelin process died 报错信息： 1Apache zeppelin process died 解决方法: 1234服务失败，重启服务ps -ef | grep &quot;zeppelin&quot; kill -9 pid sudo bin/zeppelin-daemon.sh restart 权限问题，无法执行 报错信息： 12 shell_web_path is: http://zeppeline.com:8070/file/. shell/xianchang.yueJob 20190815-140750_2122347662 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-shell::2EHURB1RG-shared_session 解决方法： 1切换当前，shell的目录为可执行的目录 chown -R XXX:XXX shell/ connection refused 错误 报错原因：1可能正在使用过期的开发终端节点。尝试创建新的开发终端节点并重新连接。 解决方法：(推荐第一种，不可以后再选择第二种)1234561.重新创建新的note 进行执行。会建立新的连接。2.连接超时或由于任何原因停止工作，则您可能需要采取以下步骤来还原它： (1). 在 Zeppelin 中，在页面右上角的下拉菜单中，选择 Interpreters (解释器)。在解释器页面上，搜索 spark。选择 edit，然后清除 Connect to existing process 复选框。在页面底部选择 Save。 (2). 如前所述启动 SSH 端口转发。 (3). 在 Zeppelin 中，重新启用 spark 解释器的 Connect to existing process 设置，然后再次保存。像这样重置解释器应该会恢复连接。另一种实现方法是在 Interpreters 页面上为 Spark 解释器选择 restart。然后，等待最多 30 秒，以确保远程解释器已重新启动。 ####]]></content>
      <categories>
        <category>zeppelin</category>
      </categories>
      <tags>
        <tag>zeppelin-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grafana-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fgrafana-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
      <categories>
        <category>grafana</category>
      </categories>
      <tags>
        <tag>grafana-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fnginx-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[zabbix-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fzabbix-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[promethus-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fpromethus-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[promethus-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fpromethus-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[模版案例1234567891011scrape_configs: - job_name: &apos;example-random&apos; # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: [&apos;localhost:8070&apos;, &apos;localhost:8071&apos;] labels: group: &apos;production&apos; - targets: [&apos;localhost:8072&apos;] labels: group: ‘canary&apos; 基本使用123456789查看进程端口： pgrep -f prometheus解决端口占用： lsof -i :9090 sudo kill -9 prot 启动进程： ./prometheus --config.file=prometheus.yml]]></content>
      <categories>
        <category>promethus</category>
      </categories>
      <tags>
        <tag>promethus-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-问题篇]]></title>
    <url>%2F2019%2F09%2F25%2Fhadoop-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[hadoop-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fhadoop-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[hadoop 命令 cat1234将路径指定文件的内容输出到stdout。示例： hadoop fs -cat hdfs://host1:port1/file1hdfs://host2:port2/file2 hadoop fs -cat file:///file3 /user/hadoop/file4 dus1使用方法：hadoop fs -dus &lt;args&gt;显示文件的大小 test1234567使用方法：hadoop fs -test -[ezd] URI选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。 -d 如果路径是个目录，则返回1，否则返回0。示例： hadoop fs -test -e filename 查看.gz 的文件内容123456*无需解压整个文件： hadoop fs -cat /hdfs_location/part-00000.gz | zcat | head -n 20 或者 hadoop fs -cat /hdfs_location/part-00000.gz | zmore*需要解压整个文件： hadoop fs -text /myfolder/part-r-00024.gz | tail 查看.bz2 的文件内容 1类似查看.gz的方法，只需将zcat换为bzcat， 或者将zmore换为bzmore即可 yarn 命令查看 application列表 :1yarn application -list 查看单独的logs 日志：12yarn logs -applicationId application_1493700892407_0007yarn logs -applicationId application_1542870632001_26426 &gt; ./application.log 查看当前作业的状态：1yarn application -status application_1542870632001_26426 kill 当前作业1yarn application -kill application_1542870632001_26426 #####查看yarn-site.xml，确定log配置目录以及集群ip hadoop 集群配置日志清理12345678相关参数：key : mapreduce.jobhistory.max-age-msvalue : 2592000000 (30天)descrption：负责清理hdfs路径下日志 /var/hadoop/mapred/mr-history/donekey : yarn.log-aggregation.retain-secondsvalue : 2592000 （30天）description：负责清理hdfs路径下日志 /var/hadoop/yarn/$&#123;user&#125;/logs服务器上的本地日志在任务执行完进行日志聚合之后会自动进行删除，不过老数据目前还没有清理。 日志相关参数1234567891011121) yarn.log-aggregation-enable 是否开启日志聚合功能2) yarn.log-aggregation.retain-seconds hdfs上的日志保留多久。当前配置路径为：/var/hadoop/yarn/$&#123;user&#125;/logs3) yarn.log-aggregation.retain-check-interval-seconds 多长时间检查一次日志，并将满足条件的删除，如果是0或者负数，则为上一个值的1/10，已经配置为：1296000（15天）4) yarn.nodemanager.remote-app-log-dir 前缀目录：/var/hadoop/yarn5) yarn.nodemanager.remote-app-log-dir-suffix 后缀目录：logs6) yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 每隔一段时间进行日志的聚合，当前配置为：3600。如果配置为-1，则会等待任务执行完还会聚合 日志聚合参数配置123456789101112yarn.log-aggregation-enable参数说明：是否启用日志聚合功能，日志聚合开启后保存到HDFS上。默认值：falseyarn.log-aggregation.retain-seconds参数说明：聚合后的日志在HDFS上保存多长时间，单位为s。默认值：-1（不启用日志聚合），例如设置为86400，24小时yarn.log-aggregation.retain-check-interval-seconds参数说明：删除任务在HDFS上执行的间隔，执行时候将满足条件的日志删除（超过参数2设置的时间的日志），如果是0或者负数，则为参数2设置值的1/10，上例值在此处为8640s。默认值：-1yarn.nodemanager.log.retain-seconds参数说明：当不启用日志聚合此参数生效，日志文件保存在本地的时间，单位为s。默认值：10800yarn.nodemanager.remote-app-log-dir参数说明：当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚集功能时有效），修改为保存的日志文件夹。默认值：/tmp/logsyarn.nodemanager.remote-app-log-dir-suffix参数说明：远程日志目录子目录名称（启用日志聚集功能时有效）。默认值：logs 日志将被转移到目录$&#123;yarn.nodemanager.remote-app-log-dir&#125;/$&#123;user&#125;/$&#123;thisParam&#125;下]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fairflow-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Azkaban-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2FAzkaban-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[kylin-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fkylin-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[kylin-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fkylin-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[kylin-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Fkylin-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[superset-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fsuperset-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[superset-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fsuperset-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[zeppelin-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Fzeppelin-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[zeppelin-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fzeppelin-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[clickhouse-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Fclickhouse-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[maven-问题篇]]></title>
    <url>%2F2019%2F09%2F25%2Fmaven-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[maven thread “main” java.lang.StackOverflowError 保存信息：12maven thread &quot;main&quot; java.lang.StackOverflowErrorat org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext(AbstractArchiver.java:481) 解决方法：123456789方法一（亲测可以）： https://www.jianshu.com/p/5444c660ea87 Maven -&gt; Runner -&gt;VM Options ,设置-Xmx512m -Xms128m -Xss2m即可方法二：在pom.xml 中加上&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fmaven-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[redis-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fredis-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[hive-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fhive-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[redis-常用篇]]></title>
    <url>%2F2019%2F09%2F25%2Fredis-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[java-guava篇]]></title>
    <url>%2F2019%2F09%2F25%2Fjava-guava%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[java-多线程篇]]></title>
    <url>%2F2019%2F09%2F25%2Fjava-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[java-JVM篇]]></title>
    <url>%2F2019%2F09%2F25%2Fjava-JVM%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[java-基础篇]]></title>
    <url>%2F2019%2F09%2F25%2Fjava-%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mysql-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fmysql-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[hive-资料篇]]></title>
    <url>%2F2019%2F09%2F25%2Fhive-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[hive-问题篇]]></title>
    <url>%2F2019%2F09%2F25%2Fhive-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[hive-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Fhive-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mysql-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Fmysql-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[linux-学习篇]]></title>
    <url>%2F2019%2F09%2F25%2Flinux-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[flink-实战篇]]></title>
    <url>%2F2019%2F09%2F25%2Fflink-%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mysql-问题篇]]></title>
    <url>%2F2019%2F09%2F25%2Fmysql-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[RROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/tmp/mysql.sock’ (2)12主要原因是mysql 服务没有启动解决：mysql.server start ###om.mysql.jdbc.exceptions.jdbc4.CommunicationsException:1om.mysql.jdbc.exceptions.jdbc4.CommunicationsException:The last packet successfully received from the server was 44,024,462 milliseconds ago. The last packet sent successfully to the server was 44,024,462 milliseconds ago. is longer than the server configured value of &apos;wait_timeout&apos;. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property &apos;autoReconnect=true&apos; to avoid this problem. 解决方法：1231.在连接上添加这个设置&amp;autoReconnect=true&amp;failOverReadOnly=false2.每次使用完记得归还连接]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zeppelin-常用篇]]></title>
    <url>%2F2019%2F09%2F15%2Fzeppelin-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[hive 使用作业名设置12%hiveset mapred.job.name=zeppelin_hive_xianchang.yue; ##### shell 使用参数优化123%shell set mapred.child.java.opts; mapred.child.java.opts=-Xmx1536m -Xms1536m -Xmn256m -XX:SurvivorRatio=6 -XX:MaxPermSize=128m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80 -XX:GCTimeLimit=90 -XX:GCHeapFreeLimit=10 -XX:ParallelGCThreads=8 执行hive12345%shell export HADOOP_CLIENT_OPTS=&quot;-Xmx4096m $HADOOP_CLIENT_OPTS&quot;hive -e &quot; select * from test &quot;&gt;&gt;./template_size.txt spark 使用spark 执行优化123456789101112 %spark.conf spark.app.name zeppeline_custom_conf spark.driver.memory 3g spark.executor.memory 2g spark.dynamicAllocation.enabled true spark.dynamicAllocation.minExecutors 10 spark.dynamicAllocation.maxExecutors 50 spark.executor.cores 3 spark.shuffle.service.enabled true ``` ##### spark Streaming 执行 %spark import org.apache.spark.streaming._ import org.apache.spark.streaming.StreamingContext import org.apache.spark.streaming.StreamingContext._ import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.Duration import org.apache.spark.streaming.Seconds val ssc = new StreamingContext(sc, Seconds(1)) 12##### 执行后数据保存文件 %sparkimport org.apache.spark.sql.types._val sql_str =”select * from test”val df = sqlContext.sql(sql_str)val out_put_path = “s3://tmp/tag_1”df.repartition(1).write.format(“csv”).option(“delimiter”, “\t”).option(“header”, true).option(“inferSchema”, true).mode(“overwrite”).save(out_put_path) 1234 #### python 使用##### 保存文件 %python import commands print commands.getoutput(&quot;sh /var/tools/hadoop/download_file.sh&quot;+&quot; &quot;+z.input(&quot;s3下载目录:&quot;)+&quot; &quot;+z.input(&quot;保存目录:如 zhangsan/test01&quot;)+&quot; &quot; +z.input(&quot;保存文件名&quot;)) 12##### 读取文件 %pythonimport urllib2data = urllib2.urlopen(“http://zeppeline.com:8060/file/shell/product/shellppp_demo.xls&quot;)for line in data: print line `]]></content>
      <categories>
        <category>zeppelin</category>
      </categories>
      <tags>
        <tag>zeppelin-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指导思想]]></title>
    <url>%2F2019%2F08%2F05%2F%E6%8C%87%E5%AF%BC%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[学习方法： 3 w 学习 提问的智慧：连接 一万小时定论：一万小时定论 其他 工作中如何做好技术积累 学习新技术的10个技巧 如何快速处理线上故障]]></content>
      <categories>
        <category>第三世界</category>
      </categories>
      <tags>
        <tag>指导思想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase-学习篇]]></title>
    <url>%2F2019%2F07%2F29%2Fhbase-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[####HBase的简介 HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”； 就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力； HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库； 另一个不同的是HBase基于列的而不是基于行的模式；一张表的列簇不会超过5个，每个列簇中的列数没有限制； ####HBase的特点 容量大：HBase单表可以有上百亿行、百万列，数据矩阵横向和纵向两个维度所支持的数据量级都非常具有弹性； 面向列：HBase是面向列的存储和权限控制，并支持独立检索。列式存储，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段的时候，能大大减少读取的数据量； 多版本：HBase每一个列的数据存储有多个Version（version）； 稀疏性：为空的列并不占用存储空间，表可以设计的非常稀疏； 扩展性：底层依赖于HDFS； 高可靠性：WAL机制保证了数据写入时不会因集群异常而导致写入数据丢失，Replication机制保证了在集群出现严重的问题时，数据不会发生丢失或损坏。而且HBase底层使用HDFS，HDFS本身也有备份； 高性能：底层的LSM数据结构和Rowkey有序排列等架构上的独特设计，使得HBase具有非常高的写入性能。region切分、主键索引和缓存机制使得HBase在海量数据下具备一定的随机读取性能，该性能针对Rowkey的查询能够达到毫秒级别； ####HBase架构图]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码库-split分割方式]]></title>
    <url>%2F2019%2F07%2F25%2F%E4%BB%A3%E7%A0%81%E5%BA%93-split%E5%88%86%E5%89%B2%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[split 案例1234567891011121314151617181920212223String address=&quot;上海*上海市*闵行区*吴中路&quot;; String[] splitAddress=address.split(&quot;\\*&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海:上海市:闵行区:吴中路&quot;; String[] splitAddress=address.split(&quot;\:&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海.上海市.闵行区.吴中路&quot;; String[] splitAddress=address.split(&quot;\\.&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海^上海市^闵行区^吴中路&quot;; String[] splitAddress=address.split(&quot;\\^&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海@上海市@闵行区@吴中路&quot;; String[] splitAddress=address.split(&quot;@&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海,上海市,闵行区,吴中路&quot;; String[] splitAddress=address.split(&quot;,&quot;);System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);【2】多个符号作为分隔符 String address=&quot;上海^上海市@闵行区#吴中路&quot;; String[] splitAddress=address.split(&quot;\\^|@|#&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);]]></content>
      <categories>
        <category>代码库</category>
      </categories>
      <tags>
        <tag>split</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pulsar-学习篇]]></title>
    <url>%2F2019%2F07%2F17%2FPulsar-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Pulsar</category>
      </categories>
      <tags>
        <tag>Pulsar-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告-学习篇]]></title>
    <url>%2F2019%2F07%2F12%2F%E5%B9%BF%E5%91%8A-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[术语理解1.dsp系统架构 实践 dsp 业务 1.1 美团DSP广告策略实践]]></content>
      <categories>
        <category>广告</category>
      </categories>
      <tags>
        <tag>广告-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-学习篇]]></title>
    <url>%2F2019%2F07%2F12%2Fspark-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[spark Spark on yarn有分为两种模式yarn-cluster和yarn-client spark参数总结 spark 常用算子 Spark on YARN客户端模式作业运行全过程分析 spark 原理解析（重要） spark batch Spark多文件输出(MultipleOutputFormat) 2.spark-mysql 操作 2.1 Spark读取数据库(Mysql)的四种方式讲解 2.2 Spark与Mysql(JdbcRDD)整合开发 2.3 spark 计算结果写入mysql spark Streaming 官网stream Spark Streaming 实现思路与模块概述 SparkStreaming向Hbase中写数据 Spark Streaming kafka实现数据零丢失的几种方式 Kafka+Spark Streaming+Redis实时系统实践 Spark Streaming中空batches处理的两种方法 spark 调优 spark性能优化：shuffle调优 spark性能调优：开发调优 spark性能调优：资源优化 Saprk Streaming性能调优 GC调优在Spark应用中的实践 JVM的GC调优-上 JVM的GC调优-下 sparkMLib 1.Spark MLlib训练的广告点击率预测模型 案例实践 基于Spark streaming的SQL服务实时自动化运维]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka-资料篇]]></title>
    <url>%2F2019%2F07%2F12%2Fkafka-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"><![CDATA[官网 1.官网 kafka 技术内幕 Kafka技术内幕-日志压缩 中文教程 1.kafka kafka 实践 1.kafka broker的常用配置]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka-资料篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka-问题篇]]></title>
    <url>%2F2019%2F07%2F11%2Fkafka-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[No resolvable bootstrap urls given in bootstrap.servers12345678910111213说明当前的bootstrap.servers 填写不正确或者未再配置conf 中设置conf 基础配置val props = parameterTool.getProperties //判断是线上还是线下if (parameterTool.get(ENV_STATE_FLAG) == &quot;online&quot;) props.put(&quot;bootstrap.servers&quot;, parameterTool.get(KAFKA_ONLINE_BROKERS))else props.put(&quot;bootstrap.servers&quot;, parameterTool.get(KAFKA_OFFLINE_BROKERS)) props.put(&quot;group.id&quot;, parameterTool.get(&quot;groupId&quot;)) props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;) props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;) props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;) props 写kafka 超时123Caused by: org.apache.kafka.common.errors.TimeoutException: Expiring 23 record(s) for Topic: 30001 ms has passed since last append 解决方法： 查看 request.timeout.ms=120000 （defalt 30s）]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-常用篇]]></title>
    <url>%2F2019%2F07%2F10%2Fhexo-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[创建新文件 12在当前总目录下hexo new &quot;hexo之基本操作&quot; 清除缓存更新部署1hexo clean &amp;&amp; hexo g -d]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-常用篇]]></title>
    <url>%2F2019%2F07%2F10%2Flinux-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[####基本命令 常用命令： curl 的使用12# 结果会被保存到index.htmlcurl -o index.html http://www.codebelief.com 查看错误日志12tail -n 2000 namenode.log | grep -i -A 30 -B 5 -E &quot;error|exception&quot;读取namenode.log文件的末尾2000行，然后通过grep命令，忽略大小写过滤出包含error或者exception字符的行，并打印匹配行前面5行和后面30行的内容。这样不仅可以找出错误，还可以把异常堆栈信息找出来 Bash 快捷键Ctl-U 删除光标到行首的所有字符,在某些设置下,删除全行Ctl-W 删除当前光标到前边的最近一个空格之间的字符 文件 删除文件： rm -rf log 等加于$ find ./ -name “log” -exec rm {} 搜寻文件或目录: $find ./ -name “core*” | xargs file 查找所有非txt文本: find . ! -name “*.txt” -print 统计文本中123 出现的个数： grep -c “123” filename sort 排序：-n 按数字进行排序 VS -d 按字典序进行排序 -r 逆序排序 -k N 指定按第N列排序 ：sort -n -1k -2k sort -bd data // 忽略像空格之类的前导空白字符 拼接文本 paste file1 file2 系统12345678# uname -a # 查看内核/操作系统/CPU信息# head -n 1 /etc/issue # 查看操作系统版本# cat /proc/cpuinfo # 查看CPU信息# hostname # 查看计算机名# lspci -tv # 列出所有PCI设备# lsusb -tv # 列出所有USB设备# lsmod # 列出加载的内核模块# env # 查看环境变量 资源1234567# free -m # 查看内存使用量和交换区使用量# df -h # 查看各分区使用情况# du -sh &lt;目录名&gt; # 查看指定目录的大小# grep MemTotal /proc/meminfo # 查看内存总量# grep MemFree /proc/meminfo # 查看空闲内存量# uptime # 查看系统运行时间、用户数、负载# cat /proc/loadavg # 查看系统负载 磁盘和分区12345# mount | column -t # 查看挂接的分区状态# fdisk -l # 查看所有分区# swapon -s # 查看所有交换分区# hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备)# dmesg | grep IDE # 查看启动时IDE设备检测状况 网络123456# ifconfig # 查看所有网络接口的属性# iptables -L # 查看防火墙设置# route -n # 查看路由表# netstat -lntp # 查看所有监听端口# netstat -antp # 查看所有已经建立的连接# netstat -s # 查看网络统计信息 进程12# ps -ef # 查看所有进程# top # 实时显示进程状态 用户123456# w # 查看活动用户# id &lt;用户名&gt; # 查看指定用户信息# last # 查看用户登录日志# cut -d: -f1 /etc/passwd # 查看系统所有用户# cut -d: -f1 /etc/group # 查看系统所有组# crontab -l # 查看当前用户的计划任务 服务12# chkconfig --list # 列出所有系统服务# chkconfig --list | grep on # 列出所有启动的系统服务 程序1# rpm -qa # 查看所有安装的软件包 Shell 脚本执行返回状态码：1234567890 命令成功完成1通常的未知错误2误用shell命令126命令无法执行127没有找到命令128无效的退出参数128+x使用Linux信号x的致命错误。130使用Ctrl-C终止的命令255规范外的退出状态 技巧： 1.查看变量是否被声明：123使用 :- 来测试是否一个变量是否被声明过。如：if [ &quot;$&#123;NAME:-&#125;&quot; = &quot;Kevin&quot; ] 如果 $&#123;NAME&#125;变量未声明则会变为空字符，你也可以设置为其他默认值.例如：如果不存在，默认值设为：noname ，if [ &quot;$&#123;NAME:-noname&#125;&quot; = &quot;Kevin” ] 2.自定义shell命令：123vim ~/.bashrc 中设置命令别名:alias lsl=&apos;ls -lrt&apos;alias lm=&apos;ls -al|more’ 3.shell文本格式化代码： 1gg=G 4.查看shell 脚本执行过程1bash -x test.sh vi 快捷建A 移动光标到当前行尾，并进入 insert 状态a 在当前位置后进入 insert 状态dd 删除当前行D 删除光标之后的内容p 粘贴刚删除的文本ctrl+r 搜索历史命令ctrl+X Ctrl+E 调用默认编辑器去编辑一个特别长的命令 shell 编程####运算符 含义1234567891011= 两个字符串相等返回true!= 两个字符串不相等返回true-z 字符串长度为0返回true-n 字符串长度不为0返回true-d file 检测文件是否是目录，如果是，则返回 true-r file 检测文件是否可读，如果是，则返回 true-w file 检测文件是否可写，如果是，则返回 true-x file 检测文件是否可执行，如果是，则返回 true-s file 检测文件是否为空（文件大小是否大于0，不为空返回 true-e file 检测文件（包括目录）是否存在，如果是，则返回 true 字符串12345678910111213141516#!/bin/bash#定义字符串mtext=&quot;hello&quot; mtext2=&quot;world&quot;#字符串的拼接mtext3=$mtext&quot; &quot;$mtext2 #输出字符串echo $mtext3 #输出字符串长度echo $&#123;#mtext3&#125; #截取字符串echo $&#123;mtext3:1:4&#125; 数组12345678910111213141516#!/bin/bash#定义数组array=(1 2 3 4 5) array2=(aa bb cc dd ee) #找到某一个下标的数，然后赋值value=$&#123;array[3]&#125; echo $value #找到某一个下标的数，然后赋值value2=$&#123;array2[3]&#125; echo $value2 #获取数组长度length=$&#123;#array[*]&#125; echo $length echo12345678 #输出并且换行echo &quot;hello \nworld&quot; #重定向到文件echo &quot;hello world&quot; &gt; a.txt #输出当前系统时间echo `date` 判断语句12345678910111213141516171819202122232425#!/bin/basha=10b=20##### 1if [ $a == $b ]then echo &quot;true&quot;fi##### 2if [ $a == $b ]then echo &quot;true&quot;else echo &quot;false&quot;fi#### 3if [ $a == $b ]then echo &quot;a is equal to b&quot;elif [ $a -gt $b ]then echo &quot;a is greater than b&quot;elif [ $a -lt $b ]then echo &quot;a is less than b&quot;else echo &quot;None of the condition met&quot;fi test123456789test $[num1] -eq $[num2] #判断两个变量是否相等test num1=num2 #判断两个数字是否相等-e file 文件存在则返回真-r file 文件存在并且可读则返回真-w file 文件存在并且可写则返回真-x file 文件存在并且可执行则返回真-s file 文件存在并且内容不为空则返回真-d file 文件目录存在则返回真 for循环123456789101112#!/bin/bashfor i in &#123;1..5&#125;do echo $idone for i in 5 6 7 8 9 do echo $idone for FILE in $HOME/.bash* do echo $FILEdone while循环123456789101112#!/bin/bashCOUNTER=0while [ $COUNTER lt 5 ]do COUNTER=`expr $COUNTER + 1` echo $COUNTERdoneecho &apos;请输入。。。&apos;echo &apos;ctrl + d 即可停止该程序&apos;while read FILM do echo &quot;Yeah! great film the $FILM&quot;done #####]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cassandra-优化篇]]></title>
    <url>%2F2019%2F05%2F17%2Fcassandra-%E4%BC%98%E5%8C%96%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Cassandra不同于普通的应用程序，它是分布式数据库，它要大口吃内存，吃磁盘，吃CPU，所以机器要进行特殊的配置，以适应其需要。 使用最新的64位的jdk8的最新发布版本。 时钟同步，开启NTP服务. cassandra是分布式存储，就靠时间戳解决数据冲突，所以始终必须同步 TCP参数设置 在低带宽环境下，防火墙会检测闲置的连接并关闭，为了保护节点之间，或者多个DC节点之间的连接，建议如下配置系统参数 1234sudo sysctl -wnet.ipv4.tcp_keepalive_time=60net.ipv4.tcp_keepalive_probes=3net.ipv4.tcp_keepalive_intvl=10 设置这个就是可以快速的发现底层的TCP连接是否已经关闭，它间隔60秒开始探测3次，每次探测间隔10秒，也就是说最多在60+3*10=90秒内就可以检测到连接被中断。为了支撑上千个数据库连接，还建议修改以下参数 12345678sudo sysctl -wnet.core.rmem_max=16777216net.core.wmem_max=16777216net.core.rmem_default=16777216net.core.wmem_default=16777216net.core.optmem_max=40960net.ipv4.tcp_rmem=4096 87380 16777216net.ipv4.tcp_wmem=4096 65536 16777216 为了让参数永久生效，记得把它们写入系统配置文件/etc/sysctl.conf里 禁用CPU动态跳频功能。 最近的linux系统增加了一个新特性，就是可以动态调整CPU频率，就是在机器低负载的时候，可以降低CPU频率，以达到降低功耗的目的。这种动态调频功能会影响cassandra数据库的吞吐量。建议禁用，让CPU一直维持恒定的频率输出，尽管这很耗电，但是保证你的数据库的吞吐量。 1234567禁用方式：for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governordo[ -f $CPUFREQ ] || continueecho -n performance &gt; $CPUFREQdone 禁用zone_reclaim_mode 官方建议禁用，这个是关于多核CPU使用NUMA架构，分别访问内存，内存回收方面的一个参数这个参数的解释，可以参考：http://linuxinsight.com/proc_sys_vm_zone_reclaim_mode.html这里面有一句话，当你的机器用作文件服务器，或者你的大部分内存需要用于系统文件缓存的时候，你需要禁用这个功能。我们的Cassandra就相当于文件服务器，它对IO是依赖的，它需要系统内存用于大量缓存DB文件。所以要禁用这个功能。echo 0 &gt; /proc/sys/vm/zone_reclaim_modeCassandra官方描述了如果不禁用这个参数带来的后果： 12341、随机CPU尖峰带来时延增加，吞吐量增加。2、程序假死，什么也不做。3、一些突然发生又消失的莫名异常。4、重启机器，可能在一段时间内不再出现异常。 资源限制放开。 cassandra会使用很多内存，很多连接，很多文件，所以一律放开。 1234&lt;cassandra_user&gt; – memlock unlimited&lt;cassandra_user&gt; – nofile 100000&lt;cassandra_user&gt; – nproc 32768&lt;cassandra_user&gt; – as unlimited 这个加入到/etc/security/limits.conf 里（不同操作系统，可能不同，后面不再注明）vm.max_map_count = 1048575 将这个加入到 /etc/sysctl.conf 里 禁用swap 关闭 sudo swapoff –all修改 /etc/fstab. 去掉swap挂载。swap文件内存交换区，当你的内存不够的时候，使用文件内存，这会让你的数据库卡成狗的，一律禁用。 文件预 默认保持64k即可 1echo 64 &gt; /sys/class/block/&#123;sda&#125;/queue/read_ahead_kb 如果是ssd，设置为8k 1echo 8 &gt; /sys/class/block/&#123;sda&#125;/queue/read_ahead_kb 如果是ssd，还要设置下面的参数进行优化。 12345echo deadline &gt; /sys/block/sda/queue/scheduler#OR…#echo noop &gt; /sys/block/sda/queue/schedulertouch /var/lock/subsys/localecho 0 &gt; /sys/class/block/sda/queue/rotational 确保以上参数重启机器后仍然有效。 参考：赵岩的博客]]></content>
      <categories>
        <category>cassandra</category>
      </categories>
      <tags>
        <tag>cassandra-优化篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-问题篇]]></title>
    <url>%2F2019%2F05%2F17%2Flinux-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[lsof1lsof -i:80 netstat1netstat -anp|grep port ip1ip addr flindjournalctl curlcurl]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cassandra-问题篇]]></title>
    <url>%2F2019%2F05%2F17%2Fcassandra-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[为什么不可以设置listen_address为0.0.0.0（意思是监听所有地址）？ Cassandra是一个基于gossip协议的分布式系统，监听地址是用来告诉其它节点来访问的，告诉别的节点说“连接我任何地址都可以”，是一个糟糕的想法，如果集群中不同的节点使用了不同方式的地址，悲剧的事情就要发生了。如果你不想为你集群中的每个节点单独配置ip（非常可以理解）,你可以不配，空着它，Cassandra将会使用InetAddress.getLocalHost()来选择地址，然后只要你或者你的运维团队保证这个是正确的(/etc/hosts/,dns 等等要配置对)。一个例外是JMX,他默认监听的地址是0.0.0.0（这个是java的bug 6425769）请看CASSANDRA-256 和 CASSANDRA-43获取这方面更多的细节。 cassandra用了哪些端口？ 默认7000作为集群通信端口（如果开启了SSL就是7001端口）。9042端口用于native协议的客户端连接。7199端口用于JMX，9160端口用于废弃的Thrift接口。内部节点通信以及native协议的端口在cassandra配置文件里可以配置。JMX端口可以在cassandra-env.sh配置（通过JVM的参数)。所有端口都是TCP的。 当往集群中增加新节点的时候，对于存在的数据发生了什么？ 当一个新节点加入到集群，它将会自动连接集群中的其它节点，并且去复制正确的数据到本地，同样的增加、替换、移动、删除节点都是这样的。 我删除了数据，但是磁盘使用率没有变化，这是为什么？ 写入到cassandra里的数据会被持久化到SSTable文件里，SSTable文件是不可改变的，也就是说当你执行删除的时候，数据不会从文件中被去除掉的。相反，一个标记（也叫tombstone)会被写入用于标记对应记录的新状态。不用担心，当数据和tombstone发生第一次compaction的时候，数据会被删除掉，相应的磁盘空间也被回收，你可以了解关于Compaction的更多细节。 为什么用nodetool ring只能看到一条记录？ 即便所有节点输出的日志里可以看出，他们都发现彼此加入到了这个ring。 这个发生于你的所有节点都配了通用的token，不要这么做。这经常发生于哪些使用VM部署cassandra的用户，（特别是使用Debian package，它会在安装完自动启动cassandra，所以会生成token并保存它。），安装好后就把VM整个克隆出另外的节点。增很容易修复，只要把数据目录以及commitlog目录删除，然后保证每个节点是随机生成的token，再启动就可以了。 我可以修改一个正在运行中的集群中的keyspace的副本因子吗？ 可以，但是修改后需要执行repair或者cleanup来改变已存数据的副本个数。首先使用cqlsh修改目标keyspace的副本因子。如果你是减少副本因子，你可以执行nodetool cleanup去删除多余的副本数据，对每个节点都要执行。如果你是增加副本因子，你需要执行nodetool repair来保证数据的副本个数满足当前的配置。 Repair只要对每个副本集执行一次即可。这是个敏感的操作，这会影响集群的性能。强烈建议执行rolling repair，因为试图一次修复整个集群的话，那可能是个坑。 可以使用cassandra存储大的二进制字段吗？ Cassandra并没有对存储大文件或者二进制，以及这样一个二进制数据被经常读，也就是整个发送到客户端的情况进行优化。因为存储小的二进制数据（小于1MB)应该不是问题。但是还是建议把大的二进制数据分隔成小块。需要特别注意的是，任何大于16MB的值，将被Cassandra拒绝掉，这是由max_mutation_size_in_kb配置项决定的（这个配置项默认是commitlog_segment_size_in_mb的一半，commitlog_segment_size_in_mb默认是32M)。 Nodetool连接远程服务器的时候，提示“Connection refused to host: 127.0.1.1” ，这是为什么？ nodetool依赖JMX，JMX依赖RMI。RMI在两端通信的时候会根据需要创建自己的listenners和connectors。通常，这些都是底层透明的，但是不正确的hostname解析，无论是在连接方还是被连接方，都会导致错乱和这样的拒绝异常。如果你在使用DNS。确保两端机器的/etc/hosts文件是正确的。如果还是失败的，你可以尝试通过jvm选项-Djava.rmi.server.hostname=指定你要连接的远程机器名称给JMX接口，配置项大体在cassandra-env.sh文件的靠下的位置。 端口占用 错误： Cassandra关闭后，重启，提示，7199端口被占用，分析原因是关闭时使用的ctrl+c，实际上并没有关闭cassandra服务进程，所以提示端口已被使用； 12yxcdeMacBook-Pro:3.11.4 yxc$ cqlshConnection error: (&apos;Unable to connect to any servers&apos;, &#123;&apos;127.0.0.1&apos;: error(61, &quot;Tried connecting to [(&apos;127.0.0.1&apos;, 9042)]. Last error: Connection refused&quot;)&#125;) 解决：找出使用7199端口的进程lsof -i:7199杀死残留进程kill direct_pid 版本不一致 错误： 123Connection error: (‘Unable to connect to any servers’, &#123;‘127.0.0.1’: ProtocolError(“cql_version ‘3.3.0’ is not supported by remote (w/ native protocol). Supported versions: [u’3.3.1’]”,)&#125;) 解决： 12修改cassandra_home/bin/cqlsh.py: DEFAULT_CQLVER = ‘3.3.0’为DEFAULT_CQLVER = ‘3.3.1’； All host(s) tried for query failed 错误： 123message: ‘All host(s) tried for query failed. First host tried, 127.0.0.1:9042: Error: connect ECONNREFUSED 127.0.0.1:9042. See innerErrors.’ &#125; 解决： 12修改cassandra_home/conf/cassandra.yaml: start_native_transport=false改为start_native_transport=true; Cannot build a cluster without contact points 解决：1不能构建cassandra 的集群，查看是否ip 没有写对 caching 增加导致负载升高 解决：1234567Not enough replica available for query at consistency ONE (1 required but only 0 alive)https://issues.apache.org/jira/browse/CASSANDRA-7905https://issues.apache.org/jira/browse/CASSANDRA-11815https://stackoverflow.com/questions/27974911/not-enough-replica-available-for-query-at-consistency-one-1-required-but-only-0复制数据的官网解释：https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.htmlhttps://docs.datastax.com/en/archived/cassandra/2.1/cassandra/operations/ops_add_dc_to_cluster_t.html Cassandra received an invalid gossip generation for peer 解决：链接 参考赵岩]]></content>
      <categories>
        <category>cassandra</category>
      </categories>
      <tags>
        <tag>cassandra-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-学习篇]]></title>
    <url>%2F2019%2F05%2F17%2FDruid-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[druid 系列csdn]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>Druid-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homebrew-使用篇]]></title>
    <url>%2F2019%2F05%2F17%2FHomebrew-%E4%BD%BF%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[安装路径：/usr/local/Cellar/ 查找软件包 1brew search wget 安装软件包 1brew install wget 列出已安装的软件包 1brew list 删除软件包 1brew remove wget 查看软件包信息 1brew info wget 列出软件包的依赖关系 1brew deps wget 更新brew 1brew update 列出过时的软件包（已安装但不是最新版本） 1brew outdated 更新过时的软件包（全部或指定） 1brew upgrade 或 brew upgrade wget]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Homebrew-使用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cassandra-常用篇]]></title>
    <url>%2F2019%2F05%2F15%2Fcassandra-%E5%B8%B8%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[cassandra mac os 安装 shell 命令登陆1cqlsh -u cassandra -p cassandra --connect-timeout 300 查看集群信息1nodetool describecluster 查看节点状态1nodetool status 重启节点1sudo systemctl restart scylla-server 查看系统相关日志信息1sudo journalctl --follow _UID=`id -u scylla` Cql操作查看版本12345SHOW VERSIONOR SELECT release_version from system.local; 查看Schema 信息1SELECT * FROM system_schema.keyspaces; keyspaces 操作1234#查看desc keyspaces;#使用use keyspaceName 查看表12345#table listdescribe tables;#查看表结构describe table table_name; 修改keyspaces 的副本1ALTER KEYSPACE library WITH REPLICATION = &#123; &apos;class&apos; : &apos;SimpleStrategy&apos;, &apos;replication_factor&apos; : 1 &#125;;]]></content>
      <categories>
        <category>cassandra</category>
      </categories>
      <tags>
        <tag>cassandra-常用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cassandra-学习篇]]></title>
    <url>%2F2019%2F05%2F15%2Fcassandra-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[简介： Apache Cassandra 是一个开源的、分布式、无中心、弹性可扩展、高可用、容错、一致性可调、面向行的数据库，它基于 Amazon Dynamo 的分布式设计和 Google Bigtable 的数据模型，Cassandra 其协议是 P2P 的，并使用 gossip 来维护存活或死亡节点的列表（gossip 协议介绍：https://www.iteblog.com/archives/2505.html）由 Facebook 创建，在一些最流行的网站中得到应用. 优点：分布式和去中心化（Distributed and Decentralized）： 可以运行在多台机器上，并呈现给用户一个一致的整体； Cassandra 是无中心的，也就是说每个节点都是一样的，协议是 P2P 的，并使用 gossip 来维护存活或死亡节点的列表。 弹性可扩展： 集群可以不间断的情况下，方便扩展或缩减服务的规模，不需要重新启动进程，不必修改应用的查询，也无需自己手工重新均衡数据分布，只要加入新的计算机，Cassandra 就会自动地发现它并让它开始工作。 高可用和容错： 可以在不中断系统的情况下替换故障节点，还可以把数据分布到多个数据中心里，从而提供更好的本地访问性能，并且在某一数据中心发生火灾、洪水等不可抗灾难的时候防止系统彻底瘫痪 可调节一致性： 通过副本因子（replication factor），你可以决定准备牺牲多少性能来换取一致性。 副本因子是你要求更新在集群中传播到的节点数（注意，更新包括所有增加、删除和更新操作）。 一致性级别（consistency level）参数，这个参数决定了多少个副本写入成功才可以认定写操作是成功的，或者读取过程中读到多少个副本正确就可以认定是读成功的。这里 Cassandra 把决定一致性程度的权利留给了客户自己。 所以，如果需要的话，你可以设定一致性级别和副本因子相等，从而达到一个较高的一致性水平，不过这样就必须付出同步阻塞操作的代价，只有所有节点都被更新完成才能成功返回一次更新。而实际上，Cassandra 一般都不会这么来用，原因显而易见（这样就丧失了可用性目标，影响性能，而且这不是你选择 Cassandra 的初衷）。而如果一个客户端设置一致性级别低于副本因子的话，即使有节点宕机了，仍然可以写成功。 总体来说，Cassandra 更倾向于 CP，虽然它也可以通过调节一致性水平达到 AP；但是不推荐你这么设置。其CAP 定律的详细介绍可参见《分布式系统一致性问题、CAP定律以及 BASE 理论》以及《一篇文章搞清楚什么是分布式系统 CAP 定理》。 面向行 它的数据结构不是关系型的，而是一个多维稀疏哈希表。稀疏（Sparse）意味着任何一行都可能会有一列或者几列。更确切地说，应该把 Cassandra 看做是一个有索引的、面向行的存储系统。 灵活的模式（Flexible Schema） 从 3.0 版本开始，不推荐使用基于 Thrift API 的动态列创建的 API，并且 Cassandra 底层存储已经重新实现了，以更紧密地与 CQL 保持一致。 Cassandra 并没有完全限制动态扩展架构的能力，但它的工作方式却截然不同。 CQL 集合（比如 list、set、尤其是 map）提供了在无结构化的格式里面添加内容的能力，从而能扩展现有的模式。CQL 还提供了改变列的类型的能力，以支持 JSON 格式的文本的存储。 高性能(High Performance) 设计之初就特别考虑了要充分利用多处理器和多核计算机的性能，并考虑在分布于多个数据中心的大量这类服务器上运行。它可以一致而且无缝地扩展到数百台机器，存储数 TB 的数据。Cassandra 已经显示出了高负载下的良好表现，在一个非常普通的工作站上，Cassandra 也可以提供非常高的写吞吐量。而如果你增加更多的服务器，你还可以继续保持 Cassandra 所有的特性而无需牺牲性能 应用场景大规模部署 单节点不易发挥它的性能，多个节点部署cassandra才是最佳选择 写密集、统计和分析型工作 Cassandra 是为优异的写吞吐量而特别优化的。 早期用于存储用户状态更新、社交网络、建议/评价以及应用统计都是很好的应用场景。现又适用于窗口化的时间序列数据库，用于文档搜索的反向索引，以及分布式任务优先级队列。 地区分布 支持多地分布的数据存储，Cassandra 可以很容易配置成将数据分布到多个数据中心的存储方式。如果你有一个全球部署的应用，那么让数据贴近用户会获得不错的性能收益，Cassandra 正适合这种应用场合。 变化的应用 正在“初创阶段”，业务会不断改进，Cassandra 这种灵活的模式的数据模型可能更适合你。这让你的数据库能更快地跟上业务改进的步伐。 参考地址：cassandra 简介 Single column Primary Key Primary Key 可以由一列或多列组成,用于从表中检索数据，如果 Primary Key 由一列组成，那么称为 Single column Primary Key&gt; 如下： 1CREATE TABLE iteblog_user (first_name text , last_name text, PRIMARY KEY (first_name)) ; 我们在检索数据的时候需要指定 Primary Key,不指定查询数据会抛以下异常： 123InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING&quot; Composite Primary Key 如果 Primary Key 由多列组成，那么这种情况称为 Compound Primary Key 或 Composite Primary Key 如下： 1CREATE TABLE iteblog_user_composite (first_name text , last_name text, PRIMARY KEY (first_name, last_name)) ; 其中 first_name 称为 Partition key，last_name 称为 Clustering key（也可以称为 Clustering column）。在这种情况下，下面查询的前三条都是合法的，最后一条是非法的。 1234567891011cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite; cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite where first_name = &apos;iteblog&apos;; cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite where first_name = &apos;iteblog&apos; and last_name = &apos;hadoop&apos;; //非法查询cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite where last_name = &apos;hadoop&apos;;``` &gt; Partition key 和 Clustering key(查询的时候不可以仅指定，需要和partition key 组合) 也可以由多个字段组成，如果 Partition key 由多个字段组成，称之为 Composite partition key： create table iteblog_multiple ( k_part_one text, k_part_two int, k_clust_one text, k_clust_two int data text, PRIMARY KEY((k_part_one, k_part_two), k_clust_one, k_clust_two) );` 小知识：使用 Composite partition key 的一个原因其实一个 Partition 对应的 Cell 个数在 Cassandra 里面是有限制的。理论上来说，一个 Partition 的 Cell 个数大约在20亿个（231）。所以采用了 Composite partition key，我们可以将数据分散到不同的 Partition，这样有利于将同一个 Partition 的 Cell 个数减少。 Partition key &amp; Clustering key &amp; Primary Key 作用 Partition Key：将数据分散到集群的 node 上 Primary Key：在 Single column Primary Key 情况下作用和 Partition Key 一样；在 Composite Primary Key 情况下，组合 Partition key 字段决定数据的分发的节点； Clustering Key：决定同一个分区内相同 Partition Key 数据的排序，默认为升序，我们可以在建表语句里面手动设置排序的方式（DESC 或 ASC） 参考链接:Apache Cassandra Composite KeyPartition keyClustering key 介绍]]></content>
      <categories>
        <category>cassandra</category>
      </categories>
      <tags>
        <tag>cassandra-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码库-邮件发送]]></title>
    <url>%2F2019%2F05%2F06%2F%E4%BB%A3%E7%A0%81%E5%BA%93-%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%2F</url>
    <content type="text"><![CDATA[scala 版本示例：123456789101112131415161718192021222324252627282930313233343536373839import org.slf4j.LoggerFactoryimport javax.mail._import javax.mail.internet.InternetAddressimport javax.mail.internet.MimeMessageimport java.util.Propertiesobject Mail &#123; val logger = LoggerFactory.getLogger(Mail.getClass) val bodyHtml = &quot;&lt;!DOCTYPE html PUBLIC -//W3C//DTD HTML 4.01 Transitional//ENhttp://www.w3.org/TR/html4/loose.dtd&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=Content-Type content=text/html; charset=utf-8 pageEncoding=UTF-8&gt;&lt;/head&gt;&lt;body&gt;%s&lt;/body&gt;&lt;/html&gt;&quot; val prop = new Properties() prop.put(&quot;mail.smtp.host&quot;,&quot;smtp.exmail.qq.com&quot;) prop.put(&quot;mail.smtp.auth&quot;,&quot;true&quot;) prop.put(&quot;mail.smtp.connectiontimeout&quot;,&quot;10000&quot;) prop.put(&quot;mail.smtp.timeout&quot;,&quot;20000&quot;) def send(address: String, title: String, content: String) &#123; try &#123; val addresses = address.split(&quot;,&quot;).map(new InternetAddress(_).asInstanceOf[Address]) val authenticator = new SMTPAuthenticator(&quot;username@qq.com&quot;, &quot;password&quot;) val sendMailSession = Session.getDefaultInstance(prop, authenticator) val newMessage = new MimeMessage(sendMailSession) newMessage.setFrom(new InternetAddress(&quot;username@qq.com&quot;)) newMessage.setRecipients(Message.RecipientType.TO, addresses) newMessage.setSubject(title) val html = String.format(bodyHtml, content) newMessage.setContent(html, &quot;text/html;charset=utf-8&quot;) Transport.send(newMessage) logger.info(&quot;send an email to address[&#123;&#125;] title[&#123;&#125;] content[&#123;&#125;]&quot;, addresses, title, content); &#125; catch &#123; case e: MessagingException =&gt; logger.info(&quot;error occur when mail&quot;, e) &#125; &#125; class SMTPAuthenticator(username: String, password: String) extends Authenticator &#123; override def getPasswordAuthentication: PasswordAuthentication = new PasswordAuthentication(username, password) &#125;&#125;]]></content>
      <categories>
        <category>代码库</category>
      </categories>
      <tags>
        <tag>代码库-邮件发送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-问题篇]]></title>
    <url>%2F2019%2F04%2F29%2Fspark-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[org.apache.spark.SparkException: Could not find CoarseGrainedScheduler. 这个可能是一个资源问题，应该给任务分配更多的 cores 和Executors，并且分配更多的内存。并且需要给RDD分配更多的分区 在配置资源中加入这句话也许能解决你的问题： –conf spark.dynamicAllocation.enabled=false 经过一般调试，发现原来是因为spark任务生成task任务过少，而任务提交时所指定的Excutor 数过多导致，故调小 –num-executors 参数问题得以解决。 #####]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS-使用篇]]></title>
    <url>%2F2019%2F04%2F29%2FMacOS-%E4%BD%BF%E7%94%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[快捷键：退出关闭应用：Command+Q 目录地址： 软件的安装目录 1~/Library/Application Support brew 安装路径 /usr/local/Cellar/ 软件 Mac Beyond Compare4 破解方法 []]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>MacOS-使用篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术文章]]></title>
    <url>%2F2019%2F04%2F29%2F%E7%BB%8F%E5%85%B8%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[技术文章 MySQL索引原理及慢查询优化 美团点评基于 Flink 的实时数仓建设实践 从零开始入门推荐算法工程师 计算广告与流处理技术综述]]></content>
      <categories>
        <category>技术文章</category>
      </categories>
      <tags>
        <tag>技术文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-调优篇]]></title>
    <url>%2F2019%2F04%2F28%2Fspark-%E8%B0%83%E4%BC%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[性能调优分配资源：123456789101112131415161718192021222324252627spark-submit \--class cn.spark.sparktest.core.WordCountCluster \--num-executors 80 \ 配置executor的数量--driver-memory 6g \ 配置driver的内存（影响不大）--executor-memory 6g \ 配置每个executor的内存大小--executor-cores 3 \ 配置每个executor的cpu core数量(RDD cache/shuffle/task执行)--master yarn-cluster \--queue root.default \--conf spark.yarn.executor.memoryOverhead=2048 \ executor堆外内存--conf spark.core.connection.ack.wait.timeout=300 \ 连接的超时时长/usr/local/spark/spark.jar \$&#123;1&#125;spark-submit \ --master yarn \ --deploy-mode cluster \ --executor-cores 3 \ --executor-memory 10G \ --driver-memory 4G \ --conf spark.dynamicAllocation.enabled=true \ --conf spark.shuffle.service.enabled=true \ --conf spark.dynamicAllocation.initialExecutors=5 \ --conf spark.dynamicAllocation.maxExecutors=40 \ --conf spark.dynamicAllocation.minExecutors=0 \ --conf spark.dynamicAllocation.executorIdleTimeout=30s \ --conf spark.dynamicAllocation.schedulerBacklogTimeout=10s \ SparkStreaming 优雅退出 12345678910111213 public static void main(String[] args) throws Exception&#123; Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR);//String checkpointPath = PropertiesUtil.getProperty(&quot;streaming.checkpoint.path&quot;); JavaStreamingContext javaStreamingContext = JavaStreamingContext.getOrCreate(&quot;hdfs://Master:9000/streaming_checkpoint&quot;, createContext()); javaStreamingContext.start(); 每隔20秒钟监控是否有停止指令,如果有则优雅退出streaming final Properties serverProps = PropertiesUtil.properties; Thread thread = new Thread(new MonitorStopThread(javaStreamingContext,serverProps)); thread.start(); javaStreamingContext.awaitTermination(); &#125; &#125; 调节并行度： 并行度：其实就是指的是，Spark作业中，各个stage的task数量，也就代表了Spark作业的在各个阶段（stage）的并行度。 官方是推荐，task数量，设置成spark application总cpu core数量的2~3倍，比如150个cpu core，基本要设置task数量为300~500； SparkConf conf = new SparkConf().set(“spark.default.parallelism”, “500”) InputDStream并行化数据接收 创建多个InputDStream来接收同一数据源,把多个topic数据细化为单一的kafkaStream来接收 创建kafkaStream 1234567891011121314151617181920Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;(); kafkaParams.put(&quot;metadata.broker.list&quot;, &quot;192.168.1.164:9092,192.168.1.165:9092,192.168.1.166:9092&quot;); kafkaParams.put(&quot;zookeeper.connect&quot;,&quot;master:2181,data1:2181,data2:2181&quot;); 构建topic set String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS); String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;); Set&lt;String&gt; topics = new HashSet&lt;String&gt;(); for(String kafkaTopic : kafkaTopicsSplited) &#123; topics.add(kafkaTopic); JavaPairInputDStream&lt;String, String&gt; kafkaStream = KafkaUtils.createDirectStream( jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, topics); InputDStream并行化数据接收 12345678 int numStreams = 5; List&lt;JavaPairDStream&lt;String, String&gt;&gt; kafkaStreams = new ArrayList&lt;JavaPairDStream&lt;String,String&gt;&gt;(numStreams); for (int i = 0; i &lt; numStreams; i++) &#123; kafkaStreams.add(KafkaUtils.createStream(...)); &#125; JavaPairDStream&lt;String, String&gt; unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));unifiedStream.print(); 增加block数量，增加每个batch rdd的partition数量，增加处理并行度12345第一步：receiver从数据源源源不断地获取到数据，首先是会按照block interval，将指定时间间隔的数据，收集为一个block；默认时间是200ms，官方推荐不要小于50ms；第二步：根据指定batch interval时间间隔合并为一个batch，创建为一个rdd，第三步：启动一个job，去处理这个batch rdd中的数据。第四步：batch rdd 的partition数量是多少呢？一个batch有多少个block，就有多少个partition；就意味着并行度是多少；就意味着每个batch rdd有多少个task会并行计算和处理。调优：如果希望可以比默认的task数量和并行度再多一些，可以手动调节blockinterval，减少block interval。每个batch可以包含更多的block。因此也就有更多的partition，因此就会有更多的task并行处理每个batch rdd。 重分区，增加每个batch rdd的partition数量inputStream.repartition()：重分区，增加每个batch rdd的partition数量对dstream中的rdd进行重分区为指定数量的分区，就可以提高指定dstream的rdd的计算并行度调节并行度 重构RDD架构以及RDD持久化： RDD架构重构与优化 公共RDD一定要实现持久化,对于要多次计算和使用的公共RDD，一定要进行持久化。 持久化，是可以进行序列化的sessionid2actionRDD=sessionid2actionRDD.persist(StorageLevel.MEMORY_ONLY());1234MEMORY_ONLY 直接以Java对象的形式存储于JVM的内存中MYMORY_AND_DISK 存储于JVM的内存+磁盘MEMORY_ONLY_SER 序列化存储于内存中MEMORY_AND_DISK_SER 序列化存储于内存+磁盘 为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化 实现RDD高可用性：启动WAL预写日志机制 spark streaming，从原理上来说，是通过receiver来进行数据接收的；接收到的数据，会被划分成一个一个的block；block会被组合成一个batch；针对一个batch，会创建一个rdd；receiver接收到数据后，就会立即将数据写入一份到容错文件系统（比如hdfs）上的checkpoint目录中的，另一份写入到磁盘文件中去；作为数据的冗余副本。无论你的程序怎么挂掉，或者是数据丢失，那么数据都不肯能会永久性的丢失；因为肯定有副本。 123456789SparkConf conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;StreamingSpark&quot;); .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;); .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;); .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;); .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;); JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5)); jssc.checkpoint(&quot;hdfs://192.168.1.164:9000/checkpoint&quot;); 广播大变量（1m~100m）： 默认的情况下，task执行的算子中，使用了外部的变量，每个task都会获取一份变量的副本，有什么缺点呢？网络传输的开销、耗费内存、RDD持久化到内存（内存不够，持续到磁盘）、task创建对象导致gc； 广播变量，初始的时候，就在Drvier上有一份副本。 12345task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中；此后这个executor上的task，都会直接使用本地的BlockManager中的副本。executor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，举例越近越好。sc.boradcast(); 使用Kryo序列化: 默认情况下，Spark内部是使用Java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。 Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。 Kryo序列化机制，一旦启用以后，会生效的几个地方：123456781、算子函数中使用到的外部变量2、持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER3、shuffle .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);.set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;); .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;); .registerKryoClasses(new Class[]&#123;CategorySortKey.class&#125;) 序列化12、注册你使用到的，需要通过Kryo序列化的，一些自定义类，SparkConf.registerKryoClasses() 使用fastutil优化数据格式: fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue； fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度； fastutil也提供了64位的array、set和list，以及高性能快速的，以及实用的IO类，来处理二进制和文本类型的文件；fastutil最新版本要求Java 7以及以上版本； fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。 fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。 maven 依赖12345&lt;dependency&gt; &lt;groupId&gt;fastutil&lt;/groupId&gt; &lt;artifactId&gt;fastutil&lt;/artifactId&gt; &lt;version&gt;5.0.9&lt;/version&gt;&lt;/dependency&gt; 调节数据本地化等待时长： PROCESS_LOCAL：进程本地化；NODE_LOCAL：节点本地化；NO_PREF：对于task来说，没有好坏之分；RACK_LOCAL：机架本地化；ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差； 观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。日志里面会显示，starting task。。。，PROCESS LOCAL（不用调节）、NODE LOCAL、ANY（调节一下数据本地化的等待时长），反复调节，每次调节完以后，再来运行，观察日志 怎么调节？1234567spark.locality.wait，默认是3s；6s，10s默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3sspark.locality.wait.processspark.locality.wait.nodespark.locality.wait.racknew SparkConf() .set(&quot;spark.locality.wait&quot;, &quot;10&quot;) 定时清除不需要的数据 通过配置spark.cleaner.ttl为一个合理的值，但是这个值不能过小，因为如果后面计算需要用的数据被清除会带来不必要的麻烦。 另外通过配置spark.streaming.unpersist为true(默认就是true)来更智能地去持久化（unpersist）RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为。 去除压缩 (内存充足的情况下)在内存充足的情况下，可以设置spark.rdd.compress 设置为false. Yarn 优化Executors和cpu核心数设置和Spark On Yarn 动态资源分配 首先需要对YARN的NodeManager进行配置，使其支持Spark的Shuffle Service。 1234567891011121314#修改&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;&lt;/property&gt;#增加&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;&lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;spark.shuffle.service.port&lt;/name&gt;&lt;value&gt;7337&lt;/value&gt;&lt;/property&gt; 将spark中对应jar包拷贝到hadoop的目录下： 123首先找到spark版本的spark-&lt;version&gt;-yarn-shuffle.jarshuffle包，并将该包放到集群所有NodeManager的classpath下，比如放到HADOOP_HOME/share/hadoop/yarn/lib JVM 调优原理概述以及降低cache操作的内存占比: full gc / minor gc，无论是快，还是慢，都会导致jvm的工作线程停止工作，stop the world。简而言之，就是说，gc的时候，spark停止工作了。等着垃圾回收结束。 spark中，堆内存又被划分成了两块儿，存储内存和执行内存；一句话，让task执行算子函数时，有更多的内存可以使用。 GC优化策略(暂时不确定)建议用并行Mark-Sweep垃圾回收机制，虽然它消耗更多的资源，但是我们还是建议开启。在spark-submit中使用–driver-java-options “-XX:+UseConcMarkSweepGC”–conf “spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC” 调节executor堆外内存与连接等待时长(在spark-sbmit中修改)： 有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行，时不时的报错，shuffle file cannot find，executor、task lost，out of memory（内存溢出）；–conf spark.yarn.executor.memoryOverhead=2048 有时候，无法建立网络连接；会卡住；ok，spark默认的网络连接的超时时长，是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了:一串file id。uuid（dsfsfd-2342vs–sdf–sdfsd）。not found。file lost。 –conf spark.core.connection.ack.wait.timeout=300 Shuffle调优原理概述： 在spark中，主要是以下几个算子：groupByKey、reduceByKey、countByKey、join，等等。 shuffle，一定是分为两个stage来完成的。因为这其实是个逆向的过程，不是stage决定shuffle，是shuffle决定stage。 shuffle前半部分的task在写入数据到磁盘文件之前，都会先写入一个一个的内存缓冲，内存缓冲满溢之后，再spill溢写到磁盘文件中。 合并map端输出文件： 开启shuffle map端输出文件合并的机制；默认情况下，是不开启的，就是会发生如上所述的大量map端输出文件的操作，严重影响性能。 new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”) 合并map端输出文件： map端内存缓冲：spark.shuffle.file.buffer，默认32k reduce端内存占比：spark.shuffle.memoryFraction，0.2 调节的时候的原则。spark.shuffle.file.buffer，每次扩大一倍，然后看看效果，64，128；spark.shuffle.memoryFraction，每次提高0.1，看看效果。不能调节的太大，太大了以后过犹不及，因为内存资源是有限的，你这里调节的太大了，其他环节的内存使用就会有问题了。12new SparkConf().set(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)new SparkConf().set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.3&quot;) HashShuffleManager与SortShuffleManager spark.shuffle.manager：hash、sort、tungsten-sort（自己实现内存管理），spark 1.2.x版本以后，默认的shuffle manager，是SortShuffleManager。 spark.shuffle.sort.bypassMergeThreshold：200（默认值为200） SortShuffleManager会避免像HashShuffleManager那样，默认就去创建多份磁盘文件。一个task，只会写入一个磁盘文件，不同reduce task的数据，用offset来划分界定。12new SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)new SparkConf().set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;) 算子调优MapPartitions提升Map类操作性能: 如果是普通的map，比如一个partition中有1万条数据；function要执行和计算1万次。但是，使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。 但是，可能就OOM，内存溢出。 filter过后使用coalesce减少分区数量： 就会导致有些task运行的速度很快；有些task运行的速度很慢。这就是数据倾斜。 coalesce算子：主要就是用于在filter操作之后，针对每个partition的数据量各不相同的情况，来压缩partition的数量。减少partition的数量，而且让每个partition的数据量都尽量均匀紧凑。 foreachPartition优化写数据库性能：&gt; 1. 用了foreachPartition算子之后，好处在哪里？ 1、对于我们写的function函数，就调用一次，一次传入一个partition所有数据； 2、主要创建或者获取一个数据库连接就可以； 3、只要向数据库发送一次SQL语句和多组参数即可； &gt; 2. 很有可能会发生OOM，内存溢出的问题。 一个partition大概是1千条左右用foreach，跟用foreachPartition，性能的提升达到了2~3分钟。 repartition解决Spark SQL低并行度的性能问题：repartition算子，你用Spark SQL这一步的并行度和task数量，肯定是没有办法去改变了。但是呢，可以将你用Spark SQL查询出来的RDD，使用repartition算子，去重新进行分区，此时可以分区成多个partition，比如从20个partition，分区成100个。 reduceByKey本地聚合介绍：reduceByKey，相较于普通的shuffle操作（比如groupByKey），它的一个特点，就是说，会进行map端的本地聚合 代码 调优进行HA机制处理-针对Driver高可用性 在创建和启动StreamingContext的时候，将元数据写入容错的文件系统（比如hdfs）。保证在driver挂掉之后，spark集群可以自己将driver重新启动起来；而且driver在启动的时候，不会重新创建一个streaming context，而是从容错文件系统（比如hdfs）中读取之前的元数据信息，包括job的执行进度，继续接着之前的进度，继续执行。使用这种机制，就必须使用cluster模式提交，确保driver运行在某个worker上面； 123456789101112 JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() &#123; @Override public JavaStreamingContext create() &#123; JavaStreamingContext jssc = new JavaStreamingContext(...); JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...); jssc.checkpoint(checkpointDirectory); return jssc; &#125; &#125;;JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);context.start();context.awaitTermination(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115JavaStreamingContext.getOrCreate 基于Function0&lt; JavaStreamingContext &gt; 进行Driver高可用 Function0&lt;JavaStreamingContext&gt; createContextFunc = new Function0&lt;JavaStreamingContext&gt;()&#123; @Override public JavaStreamingContext call() throws Exception &#123; conf = new SparkConf() .setMaster(&quot;local[4]&quot;) .setAppName(&quot;java/RealTimeStreaming&quot;) .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .set(&quot;spark.default.parallelism&quot;, &quot;10&quot;) .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;); Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;(); kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;); kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class); kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class); kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;); kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); kafkaParams.put(&quot;enable.auto.commit&quot;,true); JavaStreamingContext jssc = new JavaStreamingContext( conf, Durations.seconds(30)); jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;); // 构建topic set String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS); String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;); Set&lt;String&gt; topics = new HashSet&lt;String&gt;(); for(String kafkaTopic : kafkaTopicsSplited) &#123; topics.add(kafkaTopic); &#125; JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc, LocationStrategies.PreferConsistent(), ConsumerStrategies.Subscribe(topics, kafkaParams)); hostMap = adRealTimeLogDStream.mapToPair(record -&gt; new Tuple2&lt;String, String&gt;(record.key(), record.value())); logPeakDstream = hostMap.mapToPair(new PairFunction&lt;Tuple2&lt;String, String&gt;, String, Long&gt;() &#123; @Override public Tuple2&lt;String,Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot;\\|&quot;); String eventTime= logSplited[1]; String todayDate = DATE_FORMAT.format(new Date()).trim(); String cutTime= eventTime.substring(13,eventTime.length()-7); String ip = logSplited[0].trim(); String host = logSplited[14].trim(); return new Tuple2&lt;String, Long&gt;(host+&quot;-&quot;+ip, 1L); &#125; &#125;); hostReduce = logPeakDstream.reduceByKeyAndWindow(new Function2&lt;Long, Long, Long&gt;() &#123; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;, Durations.minutes(10),Durations.seconds(30)); JavaPairDStream&lt;String, Long&gt; topNPairRdd = hostReduce.transformToPair(new Function&lt;JavaPairRDD&lt;String, Long&gt;, JavaPairRDD&lt;String, Long&gt;&gt;() &#123; @Override public JavaPairRDD&lt;String, Long&gt; call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception &#123; JavaPairRDD&lt;Long, String&gt; sortRDD = (JavaPairRDD&lt;Long, String&gt;) rdd.mapToPair(record -&gt; new Tuple2&lt;Long, String&gt;(record._2, record._1)); JavaPairRDD&lt;String, Long&gt; sortedRdd = (JavaPairRDD&lt;String, Long&gt;) sortRDD.sortByKey(false).mapToPair(record -&gt; new Tuple2&lt;String, Long&gt;(record._2, record._1)); List&lt;Tuple2&lt;String, Long&gt;&gt; topNs = sortedRdd.take(5);//取前5个输出 System.out.println(&quot; &quot;); System.out.println(&quot;*****************峰值访问窗统计*******************&quot;); for (Tuple2&lt;String, Long&gt; topN : topNs) &#123; System.out.println(topN); &#125; System.out.println(&quot;**********************END***********************&quot;); System.out.println(&quot; &quot;); return sortedRdd; &#125; &#125;); topNPairRdd.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Long&gt;&gt;() &#123; @Override public void call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception &#123; &#125; &#125;); logDetailDstream = hostMap.map(new Function&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123; @Override public String call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot;\\|&quot;); String eventTime= logSplited[1]; String todayDate = DATE_FORMAT.format(new Date()).trim(); String cutTime= eventTime.substring(13,eventTime.length()-7); String[] urlDetails = logSplited[7].split(&quot;/&quot;); String ip = logSplited[0].trim(); String url =&quot;&quot;; if(urlDetails.length==4)&#123; url = urlDetails[3]; &#125;else if(urlDetails.length==5)&#123; url = urlDetails[3] + &quot;/&quot; + urlDetails[4]; &#125;else if(urlDetails.length&gt;=6)&#123; url = urlDetails[3] + &quot;/&quot; + urlDetails[4]+ &quot;/&quot; + urlDetails[5]; &#125; String host = logSplited[14].trim(); String dataTime =todayDate +&quot; &quot;+ cutTime; String bytesSent = logSplited[5].trim(); return dataTime+&quot; &quot;+host+&quot; &quot;+ip+&quot; &quot;+url+&quot; &quot;+bytesSent; &#125; &#125;); //logDetailDstream.print(); return jssc; &#125; &#125;; return createContextFunc; 12345提交方式 spark-submit --deploy-mode cluster --supervise SparkStreaming 与kafka整合调优 LocationStrategies 位置策略：123The new Kafka consumer API will pre-fetch messages into buffers. Therefore it is important for performance reasons that the Spark integration keep cached consumers on executors (rather than recreating them for each batch), and prefer to schedule partitions on the host locations that have the appropriate consumers. 新的Kafka消费者API可以预获取消息缓存到缓冲区，因此Spark整合Kafka让消费者在executor上进行缓存对性能是非常有助的，可以调度消费者所在主机位置的分区。 12345In most cases, you should use LocationStrategies.PreferConsistent as shown above. This will distribute partitions evenly across available executors. If your executors are on the same hosts as your Kafka brokers, use PreferBrokers,which will prefer to schedule partitions on the Kafka leader for that partition. Finally, if you have a significant skew in load among partitions, use PreferFixed. This allows you to specify an explicit mapping of partitions to hosts (any unspecified partitions will use a consistent location). 通常，你可以使用 LocationStrategies.PreferConsistent，这个策略会将分区分布到所有可获得的executor上。如果你的executor和kafkabroker在同一主机上的话，可以使用PreferBrokers，这样kafka leader会为此分区进行调度。最后，如果你加载数据有倾斜的话可以使用PreferFixed，这将允许你制定一个分区和主机的映射（没有指定的分区将使用PreferConsistent 策略） 123The cache for consumers has a default maximum size of 64. If you expect to be handling more than (64 * number of executors) Kafka partitions, you can change this settingvia spark.streaming.kafka.consumer.cache.maxCapacity 消费者默认缓存大小是64，如果你期望处理较大的Kafka分区的话，你可以使用 12spark.streaming.kafka.consumer.cache.maxCapacity设置大小。The cache is keyed by topicpartition and group.id, so use a separate group.id for each call to createDirectStream. 缓存是使用key为topic partition 和组id的，因此对于每一次调用 createDirectStream 可以使用不同的 group . id 123456789101112131415161718public static SparkConf conf = new SparkConf() .setMaster(&quot;local[4]&quot;) .setAppName(&quot;java/RealTimeStreaming&quot;) .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .set(&quot;spark.default.parallelism&quot;, &quot;10&quot;) .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);kafkaParams.put(&quot;enable.auto.commit&quot;,true);JavaStreamingContext jssc = new JavaStreamingContext( conf, Durations.seconds(30));jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;); 构建topic set 12345678910String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);Set&lt;String&gt; topics = new HashSet&lt;String&gt;();for(String kafkaTopic : kafkaTopicsSplited) &#123; topics.add(kafkaTopic); &#125; JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc, LocationStrategies.PreferConsistent(), ConsumerStrategies.Subscribe(topics, kafkaParams));]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark-调优篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grafana-资料篇]]></title>
    <url>%2F2019%2F04%2F28%2Fgrafana-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"><![CDATA[参考地址： 官网 官网视频 安装插件 插件安装步骤 插件搜索]]></content>
      <categories>
        <category>grafana</category>
      </categories>
      <tags>
        <tag>grafana-资料篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink-资料篇]]></title>
    <url>%2F2019%2F04%2F26%2Fflink-%E8%B5%84%E6%96%99%E7%AF%87%2F</url>
    <content type="text"><![CDATA[flink 学习（系列） flink官网 training flink-阿里巴巴课程 flink-vinoYang博客 flink-云邪博客 云星数据-大数据团队 flink-归来朝歌博客 阿里云实时计算 flink 代码 flink flink-learning flink-training-exercises flink 书籍 flink基础教程 [#] 博客(单篇) 什么是流处理 理解Flink的设计原则 Google Stream 101越了批处理的流处理世界 Google Stream 102超越了批处理的流处理世界 Flink原理和实现 追源索骥：透过源码看懂Flink核心框架的执行流程 Flink的架构和拓扑概览 Flink核心框架的执行流程 理解 Flink 中的计算资源 Flink如何生成ExecutionGraph及物理执行图 Flink 生成StreamGraph Flink Window Flink Window的实现原理 Flink Window的实现原理：Session Window Flink 滑动窗口优化 Flink State Flink中的状态管理 Flink中的反压Back-Pressure Flink Operator Chain原理 Flink内存管理 Flink异步快照机制-Failover 数据流的类型和操作 Flink Async IO Flink源码解析 Stream Operator Flink SQL Flink SQL的大部分代码实现是阿里巴巴的Blink团队贡献给Apache的。 Flink SQL 核心功能解密 Flink SQL维表Join和异步优化 Flink SQL 异步IO设计 Flink SQL数据去重的技巧和思考 Flink SQL TOP N的挑战与实现 Flink SQL 流计算“撤回(Retraction)”案例分析 Flink SQL 解决热点问题的大杀器MiniBatch Flink Table API&amp;SQL的概念和通用API Flink CEP复杂事件处理系列1 Flink-CEP论文与源码解读之状态与状态转换 Flink之CEP-API简介 Flink之CEP案例分析-网络攻击检测 Flink-CEP之NFA Flink-CEP之NFA编译器 Flink-CEP之模式流与运算符 系列2 CEP In Flink (1) - CEP规则解析 CEP In Flink (2) - CEP规则匹配 CEP In Flink (3) - 匹配事件提取 CEP In Flink (4) - 使用瓶颈 Flink事务 Flink Streaming Ledger 支持流式处理ACID事务 Flink源码解析 Apache Flink源码解析 DataStream API Flink Exactly Once语义flink的两阶段提交协议-实现端到端的Exactly Once语义 Flink案例 Flink在唯品会的实践 Flink在美团的实践应用 Flink在G7的实践 Flink在饿了么的应用 基于Flink的实时特征平台在Flink的应用 日均处理万亿数据！Flink在快手的应用实践与技术演进之路 其它 流计算框架 Flink 与 Storm 的性能对比]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink-资料篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink-问题篇]]></title>
    <url>%2F2019%2F04%2F26%2Fflink-%E9%97%AE%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[资源管理：flinkvcore:提示：并行度一定要小于solt数，也就是提交命令里 ynys的值，最好保持一致，这样可以将申请的资源充分利用。计算方式：如果并行度小于solt数（ynys），则vocer默认是：并行度 +1 ；如果并行度和solt数(ynys)一致，则vcore数为yn ys + 1memory:计算方式：并行度和solt数保持一致时： yn yjm + ytm并行度小于solt 数是：并行度 yjm + ytm 隐式转换器的异常：12345错误信息：value foreach is not a member of java.util.ArrayList解决：import scala.collection.JavaConversions._错误信息：could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation解决：import org.apache.flink.streaming.api.scala._（DataStream API） import org.apache.flink.api.scala._（DataSet API） 如果在使用泛型参数的函数或类中使用Flink操作，则TypeInformation必须可用于该参数。这可以通过使用上下文绑定来实现123def myFunction[T: TypeInformation](input: DataSet[T]): DataSet[Seq[T]] = &#123; input.reduceGroup( i =&gt; i.toSeq )&#125; ClassCastException：X无法强制转换为X1231.尝试classloader.resolve-order: parent-first在配置中进行设置2.从不同的执行尝试缓存类3.通过child-first类加载进行类复制 AbstractMethodError或NoSuchFieldError1存在依赖项版本冲突,确保所有依赖项版本都一致。 事件正在进行，DataStream应用程序不产生任何输出1231.如果您的DataStream应用程序使用事件时间，请检查您的水印是否已更新。 如果没有产生水印，事件时间窗口可能永远不会触发，应用程序将不会产生任何结果。2.您可以在Flink的Web UI（水印部分）中查看水印是否正在取得进展。 exception reporting “Insufficient number of network buffers”1234561.原因可能：以很高的并发来执行flink ,增加了网络缓冲区的数量。 默认情况下，Flink占用网络缓冲区的JVM堆大小的10％，最小为64MB，最大为1GB。2.可以通过修改一下以下参数来调整：taskmanager.network.memory.fractiontaskmanager.network.memory.mintaskmanager.network.memory.max flink 配置信息 由于javadoc错误无法构建maven项目123456789101112131415三个选择：1.修复错误2.禁用严格检查3.在建造时跳过Javadoc&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt; &lt;/configuration&gt; &lt;/plugin&gt;&lt;/plugins&gt;构建时跳过javadocmvn -Dmaven.javadoc.skip=true verify java.lang.ClassNotFoundException: scala.Product$class12原因：构建文件不对。您的scala版本是2.12.x但您使用的是scala版本2.11中编译的库。解决：查看构建文件的lib 中是否包含2.12的版本。 Flink BucketingSink crashes with NoClassDefFoundError: Lorg/apache/hadoop/fs/FileSystem1234567891011缺少hadoop 依赖：（ 版本根据自己的来） &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; Name node is in safe123modeorg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create file/user/test. Name node is in safe mode.解决方法：bin/hadoop dfsadmin -safemode leave 参考：https://blog.csdn.net/hongweigg/article/details/7185328 No implicit arguments of type: XXXXXX错误：could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation[Int]解决办法：import org.apache.flink.api.scala._ 或者 import org.apache.flink.streaming.api.scala._ Stack trace: ExitCodeException exitCode=1: 报错信息12345Application application_1556525638125_0015 failed 1 times (global limit =2; local limit is =1) due to AM Container for appattempt_1556525638125_0015_000001 exited with exitCode: 1Failing this attempt.Diagnostics: Exception from container-launch.Container id: container_e04_1556525638125_0015_01_000001Exit code: 1Stack trace: ExitCodeException exitCode=1: 解决方式： 11.查看提交flink 作业的版本是否和提交环境版本一致。 Checkpoint失败：Checkpoint expired before completing1234env.enableCheckpointing(1000L)val checkpointConf = env.getCheckpointConfigcheckpointConf.setMinPauseBetweenCheckpoints(30000L)checkpointConf.setCheckpointTimeout(8000L) 解决方式： 主要因为checkpointConf.setCheckpointTimeout(8000L)设置的太小了，默认是10min。 这里只设置了8sec。当一个Flink App背压的时候（例如由外部组件异常引起），Barrier会流动的非常缓慢，导致Checkpoint时长飙升。 FlinkException：The assigned slot container_XXX was removed 异常 问题原因：一般就是某一个Flink App内存占用大，导致TaskManager（在Yarn上就是Container）被Kill掉。如果代码写的没问题，就确实是资源不够了，其实1G Slot跑多个Task（Slot Group Share）其实挺容易出现的。因此有两种选择。可以根据具体情况，权衡选择一个。解决方式： 将该Flink App调度在Per Slot内存更大的集群上。 通过slotSharingGroup(“xxx”)，减少Slot中共享Task的个数 map 和list 声明方式1234val seMap: java.util.Map[String, java.util.Map[String, Object]] = new java.util.HashMap[String, java.util.Map[String, Object]]()val topList: java.util.List[String] = new java.util.ArrayList[String]() Flink参数设置slot数量增加,导致作业无法启动报错信息：12345Insufficient number of network buffers: required 96, but only 25 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys &apos;taskmanager.network.memory.fraction&apos;, &apos;taskmanager.network.memory.min&apos;, and &apos;taskmanager.network.memory.max&apos; 解决方法：1234调整Flink里面flink-conf.yaml里面的新增参数增加可支持的slot数量taskmanager.network.memory.fraction: 0.1taskmanager.network.memory.min: 268435456taskmanager.network.memory.max: 4294967296 flink 任务物理内存溢出问题定位12org.apache.flink.yarn.YarnTaskExecutorRunner - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested. 问题定位：物理内存溢出问题定位 某个TaskManger丢失的问题排查12ERROR [flink-akka.remote.default-remote-dispatcher-5] org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandler - Fetching TaskManager log failed.java.util.concurrent.CompletionException: java.io.IOException: TaskManager log files are unavailable. Log file could not be found at /mnt/disk1/log/hadoop-yarn/containers/application_1556227576661_152377/container_1556227576661_152377_01_000012/taskmanager.log 地址：问题解决地址 flink 设置uid 以及savepoint1234567891011121314151617181920DataStream&lt;String&gt; stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(&quot;source-id&quot;) // ID for the source operator .shuffle() // Stateful mapper with ID .map(new StatefulMapper()) .uid(&quot;mapper-id&quot;) 不人为指定ID，它们会被自动生成。只要ID不变化，则程序可以自动的从savepoint恢复。ID的生成依赖于程序的结构，并且对程序变化敏感。因此强烈建议人为分配ID可以将savepoint想象成持有每个有状态的操作的Operator ID到State的映射关系：Operator ID | State------------+------------------------source-id | State of StatefulSourcemapper-id | State of StatefulMapper在上面的例子中，print sink是无状态的，因此不是savepoint的一部分。默认情况下，会尝试映射savepoint的每条记录到新的程序中 参考： flink官网：https://flink.apache.org/gettinghelp.html]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink-问题篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.flink-学习篇]]></title>
    <url>%2F2019%2F04%2F26%2Fflink-%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[什么是flink Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算。 flink 特点: 其异步和增量检查点算法确保对处理延迟的影响最小，同时保证一次性状态一致性。 随处部署应用程序: 常见的集群资源管理器（如Hadoop YARN，Apache Mesos和Kubernetes）集成，但也可以设置为作为独立集群运行 处理无界和有界数据 充分利用内存性能: 任务状态始终保留在内存中，如果状态大小超过可用内存，则保存在访问高效的磁盘上数据结构中 性能：每天处理数万亿个事件；维护多个TB的状态；在数千个内核的运行 flink丰富状态功能： 多状态基元：Flink为不同的数据结构提供状态基元，例如原子值，列表或映射。 可插拔状态后端：应用程序状态由可插拔状态后端管理和检查点。(内存或RocksDB 存储或自定义) 完全一次的状态一致性。 非常大的状态：由于其异步和增量检查点算法，Flink能够维持几兆兆字节的应用程序状态。 可扩展的应用程序：Flink通过将状态重新分配给更多或更少的工作人员来支持有状态应用程序的扩展。 flink 的容错机制： 一致的检查点: 如果发生故障，将重新启动应用程序并从最新检查点加载其状态,此功能可以保证一次性状态一致性。 高效检查点: Flink可以执行异步和增量检查点，以便将检查点对应用程序的延迟SLA的影响保持在非常小的水平。 端到端完全一次: Flink为特定存储系统提供事务接收器，保证数据只写出一次，即使出现故障 与集群管理器集成: Flink与集群管理器紧密集成，例如Hadoop YARN，Mesos或Kubernetes。当进程失败时，将自动启动一个新进程来接管其工作。 高可用性设置: Flink具有高可用性模式，可消除所有单点故障。HA模式基于Apache ZooKeeper，这是一种经过验证的可靠分布式协调服务。 Savepoints操作：用于启动状态兼容的应用程序并初始化其状态。 适用场景： 程序版本升级。 程序迁移集群。 flink 版本更新。 暂停和恢复、存档。 A/B测试：启动同一保存点的所有版本来比较两个（或更多）不同版本的应用程序的性能或质量。 flink 丰富time功能: Event-time Mode(事件时间):根据事件的时间戳计算结果 Watermark Support（水印支持）:使用水印来推断事件时间应用中的时间，也可以使用水印来推断事件时间应用中的时间。 Late Data Handling(延迟数据处理):当使用水印在事件 - 时间模式下处理流时，可能会在所有相关事件到达之前完成计算. Processing-time Mode(处理时间):由处理机器的挂钟时间触发的计算,处理时间模式适用于具有严格的低延迟要求的某些应用，这些要求可以容忍近似结果. flink 分层API: ProcessFunctions:提供对时间和状态的细粒度控制,可以任意修改其状态并注册将在未来触发回调函数的定时器,因此，它可以根据许多有状态事件驱动的应用程序的需要实现复杂的事件业务逻辑:open -&gt; processElement -&gt; onTimer（相关介绍） DataStream API: 提供了许多常见的流处理操作，如窗口，记录在-A-时间变换，并丰富事件原语 SQL和Table API: Table API和SQL利用Apache Calcite进行解析，验证和查询优化,它们可以与DataStream和DataSet API无缝集成，并支持用户定义的标量，聚合和表值函数。 flink Libraries: 复杂事件处理（CEP）:CEP库的应用包括网络入侵检测，业务流程监控和欺诈检测。 DataSet API: 用于批处理应用程序的核心API。 Gelly：Gelly是一个可扩展的图形处理和分析库。 flink 监控方式： Web UI:可以检查，监视和调试正在运行的应用程序。 日志记录：Flink实现了流行的slf4j日志记录界面，并与日志框架log4j或logback集成。 指标：Flink具有复杂的指标系统，可收集和报告系统和用户定义的指标。 REST API：Flink公开REST API以提交新应用程序，获取正在运行的应用程序的保存点或取消应用程序。]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink-学习篇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markDown-使用]]></title>
    <url>%2F2019%2F04%2F25%2FmarkDown-%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[标签123456# h1 最大## h2### h3#### h4##### h5###### h6 最小 段落及区块引用1&gt; 插入链接和图片1234链接 []()[点击跳转至百度](http://www.baidu.com)图片 ![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)![图片](https://user-gold-cdn.xitu.io/2018/4/18/162d75d959444389?w=1240&amp;h=703&amp;f=jpeg&amp;s=56927) 列表1234567891011* | + | - 是无序列表1. 数字点加空格 是有序列表列表中加入了区块引用，区域引用标记符也需要缩进4个空格上一级和下一级之间敲三个空格即可示例：* 段落一 &gt; 区块标记一 &gt;&gt;区块标记二* 段落二 &gt; 区块标记二 分割线1*** 强调1234*这里是斜体***这里是加粗*****这里是斜线加粗***～～这里是删除线～～ 代码块12单行代码：单反引号包裹代码块：三个反引号包裹。 表格12345678910表头|条目一|条目二:---:|:---:|:---:项目|项目一|项目二注意：第二行分割表头和内容。- 有一个就行，为了对齐，多加了几个文字默认居左-两边加：表示文字居中-右边加：表示文字居右]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>markDown-使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启程]]></title>
    <url>%2F2019%2F04%2F24%2F%E5%90%AF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 支持以 PDF 格式导出文稿 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 新增 Todo 列表功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式[^LaTeX]$$E=mc^2$$ 3. 高亮一段代码[^code]1234567@requires_authorizationclass SomeClass: passif __name__ == '__main__': # A comment print 'hello world' 4. 高效绘制 流程图12345678st=&gt;start: Startop=&gt;operation: Your Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 5. 高效绘制 序列图123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 6. 高效绘制 甘特图12345678910111213title 项目开发流程section 项目确定 需求分析 :a1, 2016-06-22, 3d 可行性报告 :after a1, 5d 概念验证 : 5dsection 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5dsection 发布验收 发布: 2d 验收: 3d 7. 绘制表格 项目 价格 数量 计算机 \$1600 5 手机 \$12 12 管线 \$1 234 8. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 @ghosert2016 年 07月 07日 [^LaTeX]: 支持 LaTeX 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。 [^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他</tag>
      </tags>
  </entry>
</search>
