<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>博客文章总目录</title>
      <link href="/2020/01/07/%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0%E6%80%BB%E7%9B%AE%E5%BD%95/"/>
      <url>/2020/01/07/%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0%E6%80%BB%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h4 id="A"><a href="#A" class="headerlink" title="A"></a>A</h4><h6 id="azkaban系列："><a href="#azkaban系列：" class="headerlink" title="azkaban系列："></a>azkaban系列：</h6><pre><code>   1.azkaban-部署篇   2.azkaban-常用篇   3.azkaban-剖析篇   4.azkaban-问题篇</code></pre><h6 id="airflow"><a href="#airflow" class="headerlink" title="airflow"></a>airflow</h6><pre><code>   1.airflow-常用篇</code></pre><h6 id="ambari"><a href="#ambari" class="headerlink" title="ambari"></a>ambari</h6><pre><code>   1.ambari-学习篇   2.ambari-资料篇</code></pre><h4 id="B"><a href="#B" class="headerlink" title="B"></a>B</h4><h6 id="brew"><a href="#brew" class="headerlink" title="brew"></a>brew</h6><pre><code>   1.brew-使用篇</code></pre><h4 id="C"><a href="#C" class="headerlink" title="C"></a>C</h4><h6 id="cassandra"><a href="#cassandra" class="headerlink" title="cassandra"></a>cassandra</h6><pre><code>   1.cassandra-学习篇   2.cassandra-常用篇   3.cassandra-优化篇   4.cassandra-问题篇   5.cassandra-资料篇</code></pre><h6 id="clickhouse"><a href="#clickhouse" class="headerlink" title="clickhouse"></a>clickhouse</h6><pre><code>   1.clickhouse-学习篇   2.clickhouse-常用篇   3.clickhouse-问题篇   4.clickhouse-资料篇</code></pre><h4 id="D"><a href="#D" class="headerlink" title="D"></a>D</h4><h6 id="druid"><a href="#druid" class="headerlink" title="druid"></a>druid</h6><pre><code>   1.druid-学习篇</code></pre><h4 id="E"><a href="#E" class="headerlink" title="E"></a>E</h4><h6 id="elasticsearch"><a href="#elasticsearch" class="headerlink" title="elasticsearch"></a>elasticsearch</h6><pre><code>   1.elasticsearch-学习篇   2.elasticsearch-常用篇   3.elasticsearch-问题篇   4.elasticsearch-资料篇</code></pre><h4 id="F"><a href="#F" class="headerlink" title="F"></a>F</h4><h6 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h6><pre><code>   1.flink-常用篇   2.flink-优化篇   3.flink-问题篇   4.flink-资料篇</code></pre><blockquote><p>flink优化(1)-配置参数优化<br>flink优化(2)-优化<br>flink优化(3)-<br>flink优化(4)-相关文章</p></blockquote><blockquote><p>flink应用-<br>flink案例-<br>flink</p></blockquote><h4 id="G"><a href="#G" class="headerlink" title="G"></a>G</h4><h6 id="grafana"><a href="#grafana" class="headerlink" title="grafana"></a>grafana</h6><h6 id="git"><a href="#git" class="headerlink" title="git"></a>git</h6><h4 id="H"><a href="#H" class="headerlink" title="H"></a>H</h4><h6 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h6><h6 id="hutool"><a href="#hutool" class="headerlink" title="hutool"></a>hutool</h6><pre><code>   1.hutool-常用篇</code></pre><h6 id="haproxy"><a href="#haproxy" class="headerlink" title="haproxy"></a>haproxy</h6><pre><code>   1.haproxy-学习篇   2.haproxy-使用篇   3.haproxy-问题篇</code></pre><h6 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h6><pre><code>   1.hive-常用篇   2.hive-问题篇</code></pre><h6 id="httpd"><a href="#httpd" class="headerlink" title="httpd"></a>httpd</h6><h4 id="I"><a href="#I" class="headerlink" title="I"></a>I</h4><h6 id="idea"><a href="#idea" class="headerlink" title="idea"></a>idea</h6><pre><code>   1.idea-常用篇   2.idae-问题篇</code></pre><h6 id="impala"><a href="#impala" class="headerlink" title="impala"></a>impala</h6><pre><code>   1.impala-学习篇   2.impala-常用篇   3.impala-问题篇</code></pre><h4 id="J"><a href="#J" class="headerlink" title="J"></a>J</h4><h6 id="java"><a href="#java" class="headerlink" title="java"></a>java</h6><pre><code>   1.java-cache篇   2.java-guava篇   3.java-JVM篇   4.java-ThreadLocalRandom篇   5.java-多线程篇   6.java-设计模式</code></pre><h4 id="K"><a href="#K" class="headerlink" title="K"></a>K</h4><h6 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h6><h6 id="kubernets"><a href="#kubernets" class="headerlink" title="kubernets"></a>kubernets</h6><pre><code>   1.k8s-学习篇</code></pre><h6 id="kudu"><a href="#kudu" class="headerlink" title="kudu"></a>kudu</h6><h6 id="kylin"><a href="#kylin" class="headerlink" title="kylin"></a>kylin</h6><h4 id="L"><a href="#L" class="headerlink" title="L"></a>L</h4><h6 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h6><h6 id="livy"><a href="#livy" class="headerlink" title="livy"></a>livy</h6><h4 id="M"><a href="#M" class="headerlink" title="M"></a>M</h4><h6 id="maven"><a href="#maven" class="headerlink" title="maven"></a>maven</h6><h4 id="N"><a href="#N" class="headerlink" title="N"></a>N</h4><h6 id="nginx"><a href="#nginx" class="headerlink" title="nginx"></a>nginx</h6><h6 id="netty"><a href="#netty" class="headerlink" title="netty"></a>netty</h6><h4 id="O"><a href="#O" class="headerlink" title="O"></a>O</h4><h6 id="orc"><a href="#orc" class="headerlink" title="orc"></a>orc</h6><h4 id="P"><a href="#P" class="headerlink" title="P"></a>P</h4><h6 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h6><h6 id="python"><a href="#python" class="headerlink" title="python"></a>python</h6><h4 id="R"><a href="#R" class="headerlink" title="R"></a>R</h4><h6 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h6><h6 id="rpc"><a href="#rpc" class="headerlink" title="rpc"></a>rpc</h6><h4 id="S"><a href="#S" class="headerlink" title="S"></a>S</h4><h6 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h6><h6 id="spring-全家桶"><a href="#spring-全家桶" class="headerlink" title="spring 全家桶"></a>spring 全家桶</h6><h6 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h6><h6 id="sqoop"><a href="#sqoop" class="headerlink" title="sqoop"></a>sqoop</h6><h4 id="T"><a href="#T" class="headerlink" title="T"></a>T</h4><h6 id="tomcat"><a href="#tomcat" class="headerlink" title="tomcat"></a>tomcat</h6><h4 id="V"><a href="#V" class="headerlink" title="V"></a>V</h4><h6 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h6><h4 id="Y"><a href="#Y" class="headerlink" title="Y"></a>Y</h4><h6 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h6><h4 id="Z"><a href="#Z" class="headerlink" title="Z"></a>Z</h4><h6 id="zabbix"><a href="#zabbix" class="headerlink" title="zabbix"></a>zabbix</h6><pre><code>   1.zabbix-常用篇   2.zabbix-报警脚本篇</code></pre><h6 id="zeppelin"><a href="#zeppelin" class="headerlink" title="zeppelin"></a>zeppelin</h6><pre><code>   1.zeppelin-部署篇   2.zeppelin-常用篇   3.zeppelin-剖析篇   4.zeppelin-问题篇   5.zeppelin-资料篇</code></pre><h6 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h6><h6 id="广告"><a href="#广告" class="headerlink" title="广告"></a>广告</h6><pre><code>   1.广告-学习篇 </code></pre><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><h6 id="心灵港湾"><a href="#心灵港湾" class="headerlink" title="心灵港湾"></a>心灵港湾</h6><h6 id="书籍阅读"><a href="#书籍阅读" class="headerlink" title="书籍阅读"></a>书籍阅读</h6><h6 id="生活感悟"><a href="#生活感悟" class="headerlink" title="生活感悟"></a>生活感悟</h6>]]></content>
      
      
      <categories>
          
          <category> 目录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix-报警脚本篇</title>
      <link href="/2019/12/27/zabbix-%E6%8A%A5%E8%AD%A6%E8%84%9A%E6%9C%AC%E7%AF%87/"/>
      <url>/2019/12/27/zabbix-%E6%8A%A5%E8%AD%A6%E8%84%9A%E6%9C%AC%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>邮件报警</p></blockquote><pre class=" language-sh"><code class="language-sh">#!/bin/bash#export.UTF-8FILE=/tmp/mailtmp.txtecho "$3" >$FILEdos2unix -k $FILE/bin/mail -s "$2" $1 < $FILE</code></pre><blockquote><p>微信报警</p></blockquote><pre><code>#!/bin/bash## Filename:    sendWeiXin.sh# Revision:    1.0# Description: zabbix微信报警脚本CropID=&#39;wxxxxxxxxxxxxxxxxx&#39;Secret=&#39;XXXXXXXXXXXXXXXXXXXXXX-XXXXX-XXXXXXXXXXXXXXXXX-XXXXXXXXXXXXXXXXX&#39;GURL=&quot;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=$CropID&amp;corpsecret=$Secret&quot; Gtoken=$(/usr/bin/curl -s -G $GURL | awk -F\&quot; &#39;{print $4}&#39;)PURL=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=$Gtoken&quot;function body() {        local int AppID=6                       # 企业号中的应用id        local UserID=$1                         # 部门成员id，zabbix中定义的微信接收者        local PartyID=                         # 部门id，定义了范围，组内成员都可接收到消息        local Msg=$(echo &quot;$@&quot; | cut -d&quot; &quot; -f3-) # 过滤出zabbix中传递的第三个参数        printf &#39;{\n&#39;        printf &#39;\t&quot;touser&quot;: &quot;&#39;&quot;$UserID&quot;&#39;&quot;,\n&#39;        printf &#39;\t&quot;toparty&quot;: &quot;&#39;&quot;$PartyID&quot;&#39;&quot;,\n&#39;        printf &#39;\t&quot;msgtype&quot;: &quot;text&quot;,\n&#39;        printf &#39;\t&quot;agentid&quot;: &#39;&quot;$AppID&quot;&#39;,\n&#39;        printf &#39;\t&quot;text&quot;: {\n&#39;        printf &#39;\t\t&quot;content&quot;: &quot;&#39;&quot;$Msg&quot;&#39;&quot;\n&#39;        printf &#39;\t},\n&#39;        printf &#39;\t&quot;safe&quot;:&quot;0&quot;\n&#39;        printf &#39;}\n&#39;}/usr/bin/curl --data-ascii &quot;$(body $1 $2 $3)&quot; $PURL#body $1 $2 $3</code></pre><blockquote><p>短信报警</p></blockquote><pre><code>#!/bin/bash## Filename:    sendSMS.sh# Description: zabbix短信告警脚本# 脚本的日志文件LOGFILE=&quot;/tmp/SMS.log&quot;&gt;&quot;$LOGFILE&quot;exec 1&gt;&quot;$LOGFILE&quot;exec 2&gt;&amp;1# Uid 网站用户名# Key 接口秘钥Uid=&quot;itttl-user&quot;Key=&quot;itttl-passwd&quot;MOBILE_NUMBER=$1    # 手机号码MESSAGE_FUCK=$3        # 短信内容MESSAGE_UTF8=`iconv -t GB2312 -f UTF-8 &lt;&lt; EOF$MESSAGE_FUCKEOF`XXD=&quot;/usr/bin/xxd&quot;CURL=&quot;/usr/bin/curl&quot;TIMEOUT=5# 短信内容要经过URL编码处理，除了下面这种方法，也可以用curl的--data-urlencode选项实现。MESSAGE_ENCODE=$(echo &quot;$MESSAGE_UTF8&quot; | ${XXD} -ps | sed &#39;s/\(..\)/%\1/g&#39; | tr -d &#39;\n&#39;)# SMS APIURL=&quot;http://123.45.67.89:9876/QxtSms/QxtFirewall?OperID=${Uid}&amp;OperPass=${Key}&amp;SendTime=&amp;ValidTime=&amp;AppendID=0000&amp;DesMobile=${MOBILE_NUMBER}&amp;Content=${MESSAGE_ENCODE}&amp;ContentType=8&quot;# Send itset -x${CURL} -s --connect-timeout ${TIMEOUT} &quot;${URL}&quot;</code></pre><blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix 报警脚本篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扫雷系列-Java进程CPU过高故障排查</title>
      <link href="/2019/12/27/%E6%89%AB%E9%9B%B7%E7%B3%BB%E5%88%97-Java%E8%BF%9B%E7%A8%8BCPU%E8%BF%87%E9%AB%98%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/"/>
      <url>/2019/12/27/%E6%89%AB%E9%9B%B7%E7%B3%BB%E5%88%97-Java%E8%BF%9B%E7%A8%8BCPU%E8%BF%87%E9%AB%98%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>转载地址：<a href="http://blog.itttl.com/blog/java/java_tuning.html" target="_blank" rel="noopener">http://blog.itttl.com/blog/java/java_tuning.html</a></p></blockquote><blockquote><p>问题描述：</p></blockquote><p>排查java服务CPU负载异常过高。</p><blockquote><p>解决过程：</p></blockquote><p>1.根据top命令，发现PID为9914的Java进程占用CPU高达150%，出现故障。</p><p>2.找到该进程后，如何定位具体线程或代码呢，首先显示线程列表,并按照CPU占用高的线程排序：</p><pre><code>[root@test01 logs]# ps -mp 9914 -o THREAD,tid,time | sort -rn显示结果如下：USER     %CPU PRI SCNT WCHAN  USER SYSTEM   TID     TIMEroot     28.6  19    - futex_    -      - 10032 00:16:56root     28.6  19    - -         -      -  9985 00:16:59root     28.5  19    - futex_    -      -  9981 00:16:56root     28.5  19    - -         -      - 10108 00:16:55找到了耗时最高的线程10032，占用CPU时间有16分钟了！将需要的线程ID转换为16进制格式：[root@test01 logs]# printf &quot;%x\n&quot; 100322730最后打印线程的堆栈信息：[root@test01 logs]# jstack 2633 |grep 2730 -A 30将输出的信息发给开发部进行确认，这样就能找出有问题的代码。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 扫雷系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 扫雷系列 Java进程CPU过高故障排查 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zeppelin-部署篇</title>
      <link href="/2019/12/24/zeppelin-%E9%83%A8%E7%BD%B2%E7%AF%87/"/>
      <url>/2019/12/24/zeppelin-%E9%83%A8%E7%BD%B2%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> zeppelin-部署篇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zeppelin-部署篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>azkaban-部署篇</title>
      <link href="/2019/12/24/azkaban-%E9%83%A8%E7%BD%B2%E7%AF%87/"/>
      <url>/2019/12/24/azkaban-%E9%83%A8%E7%BD%B2%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h4><blockquote><p>macOS | java 8 |gradle | azkaban</p></blockquote><h4 id="构建source-的命令"><a href="#构建source-的命令" class="headerlink" title="构建source 的命令"></a>构建source 的命令</h4><pre><code># Build Azkaban./gradlew build# Clean the build./gradlew clean# Build and install distributions./gradlew installDist# Run tests./gradlew test# Build without running tests./gradlew build -x test</code></pre><h4 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h4><pre><code>#下载azkaban git clone https://github.com/azkaban/azkaban.git#选择版本git checkout 3.81.4#source 进行build./gradlew clean build#或者构建成tar 的形式，上传服务器进行部署，只需部署exec 和web ./gradlew clean distTar</code></pre>]]></content>
      
      
      <categories>
          
          <category> azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> azkaban-部署篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java-ThreadLocalRandom篇</title>
      <link href="/2019/12/19/java-ThreadLocalRandom%E7%AF%87/"/>
      <url>/2019/12/19/java-ThreadLocalRandom%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="1-为什么用？"><a href="#1-为什么用？" class="headerlink" title="1.为什么用？"></a>1.为什么用？</h4><p>因为在Java 7 才引入了 java.util.concurrent.ThreadLocalRandom 类，主要是用于在多线程环境中生成随机数。是 ThreadLocal 类和 Random 类的组合，与当前线程隔离，通过简单地避免对 Random 对象的任何并发访问，在多线程环境中实现了更好的性能。</p><h4 id="2-怎么用？"><a href="#2-怎么用？" class="headerlink" title="2.怎么用？"></a>2.怎么用？</h4><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">//当前边界时 int 的边界</span><span class="token keyword">int</span> boundedRandomValue <span class="token operator">=</span> ThreadLocalRandom<span class="token punctuation">.</span><span class="token function">current</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//当前边界时【0，100） 之间，包含0 不包含100</span><span class="token keyword">int</span> boundedRandomValue <span class="token operator">=</span> ThreadLocalRandom<span class="token punctuation">.</span><span class="token function">current</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h4 id="3-使用场景"><a href="#3-使用场景" class="headerlink" title="3.使用场景"></a>3.使用场景</h4><pre><code>//可用于大数据倾斜时，用于打散数据。//key + &quot;@&quot; + ThreadLocalRandom.current().nextInt()</code></pre><p><img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/ed298b30.png" alt="图来自美团技术团队的博客：https://tech.meituan.com/2016/05/12/spark-tuning-pro.html"></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java ThreadLocalRandom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>azkaban-问题篇</title>
      <link href="/2019/12/19/azkaban-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/12/19/azkaban-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="1-azkaban-exec-遇到的问题"><a href="#1-azkaban-exec-遇到的问题" class="headerlink" title="1.azkaban exec 遇到的问题"></a>1.azkaban exec 遇到的问题</h4><h5 id="1-1-Job-failed-Cannot-request-memory-Xms-0-kb-Xmx-0-kb-from-system-for-job"><a href="#1-1-Job-failed-Cannot-request-memory-Xms-0-kb-Xmx-0-kb-from-system-for-job" class="headerlink" title="1.1 Job failed, Cannot request memory (Xms 0 kb, Xmx 0 kb) from system for job"></a>1.1 Job failed, Cannot request memory (Xms 0 kb, Xmx 0 kb) from system for job</h5><pre><code>异常：    ERROR - Cannot request memory (Xms 0 kb, Xmx 0 kb) from system for job job_run_distinct_impression cause: null分析：当前遇到的问题，是在提交作业的时候进行了内存的检查，认为空闲资源不足，抛除的异常。解决：在 azkaban/exec-server/plugins/jobtypes/commonprivate.properties文件中添加memCheck.enable=false参考： https://github.com/azkaban/azkaban/issues/481      https://my.oschina.net/u/2988360/blog/1537561</code></pre><h5 id="1-2-exec出现内存溢出"><a href="#1-2-exec出现内存溢出" class="headerlink" title="1.2 exec出现内存溢出"></a>1.2 exec出现内存溢出</h5><pre><code>异常：在azkanan-exec.log 日志中报堆内存溢出，并且作业提交不到exec 上来。分析：部署应用时，设置的内存大小不够导致。解决：在azkaban-executer-start.sh 脚本中， 增大 AZKABAN_OPTS=&quot;-Xmx4G&quot;</code></pre><h4 id="2-azkaban-web-遇到的问题"><a href="#2-azkaban-web-遇到的问题" class="headerlink" title="2.azkaban web 遇到的问题"></a>2.azkaban web 遇到的问题</h4>]]></content>
      
      
      <categories>
          
          <category> azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink-优化篇</title>
      <link href="/2019/12/17/flink-%E4%BC%98%E5%8C%96%E7%AF%87/"/>
      <url>/2019/12/17/flink-%E4%BC%98%E5%8C%96%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="flink-基础配置层面"><a href="#flink-基础配置层面" class="headerlink" title="flink 基础配置层面"></a>flink 基础配置层面</h4><h5 id="1-flink-conf-yaml-配置优化"><a href="#1-flink-conf-yaml-配置优化" class="headerlink" title="1.flink-conf.yaml 配置优化"></a>1.flink-conf.yaml 配置优化</h5><blockquote><p>1.优化GC</p></blockquote><pre><code>Flink是依赖内存计算，计算过程中内存不够对Flink的执行效率影响很大。可以通过监控GC（Garbage Collection），评估内存使用及剩余情况来判断内存是否变成性能瓶颈，并根据情况优化。监控节点进程的YARN的Container GC日志，如果频繁出现Full GC，需要优化GC。解决方法：GC的配置：在客户端的“conf/flink-conf.yaml”配置文件中，在“env.java.opts”配置项中添加参数：“-Xloggc:&lt;LOG_DIR&gt;/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=20M”。 此处默认已经添加GC日志。调整老年代和新生代的比值。在客户端的“conf/flink-conf.yaml”配置文件中，在“env.java.opts”配置项中添加参数：“-XX:NewRatio”。如“ -XX:NewRatio=2”，则表示老年代与新生代的比值为2:1，新生代占整个堆空间的1/3，老年代占2/3。</code></pre><blockquote><p>2.配置进程参数</p></blockquote><pre><code>操作场景Flink on YARN模式下，有JobManager和TaskManager两种进程。在任务调度和运行的过程中，JobManager和TaskManager承担了很大的责任。因而JobManager和TaskManager的参数配置对Flink应用的执行有着很大的影响意义。用户可通过如下操作对Flink集群性能做优化。操作步骤1.配置JobManager内存。    JobManager负责任务的调度，以及TaskManager、RM之间的消息通信。当任务数变多，任务平行度增大时，JobManager内存都需要相应增大。您可以根据实际任务数量的多少，为JobManager设置一个合适的内存。1.1在使用yarn-session命令时，添加“-jm MEM”参数设置内存。1.2在使用yarn-cluster命令时，添加“-yjm MEM”参数设置内存。2.配置TaskManager个数。每个TaskManager每个核同时能跑一个task，所以增加了TaskManager的个数相当于增大了任务的并发度。在资源充足的情况下，可以相应增加TaskManager的个数，以提高运行效率。2.1在使用yarn-session命令时，添加“-n NUM”参数设置TaskManager个数。2.2在使用yarn-cluster命令时，添加“-yn NUM”参数设置TaskManager个数。3.配置TaskManager Slot数。每个TaskManager多个核同时能跑多个task，相当于增大了任务的并发度。但是由于所有核共用TaskManager的内存，所以要在内存和核数之间做好平衡。3.1在使用yarn-session命令时，添加“-s NUM”参数设置SLOT数。3.2在使用yarn-cluster命令时，添加“-ys NUM”参数设置SLOT数。4.配置TaskManager内存。TaskManager的内存主要用于任务执行、通信等。当一个任务很大的时候，可能需要较多资源，因而内存也可以做相应的增加。4.1将在使用yarn-sesion命令时，添加“-tm MEM”参数设置内存。4.2将在使用yarn-cluster命令时，添加“-ytm MEM”参数设置内存。</code></pre><blockquote><ol start="3"><li>设计分区方法</li></ol></blockquote><pre><code>操作场景合理的设计分区依据，可以优化task的切分。在程序编写过程中要尽量分区均匀，这样可以实现每个task数据不倾斜，防止由于某个task的执行时间过长导致整个任务执行缓慢。操作步骤以下是几种分区方法。随机分区：将元素随机地进行分区。dataStream.shuffle();Rebalancing (Round-robin partitioning)：基于round-robin对元素进行分区，使得每个分区负责均衡。对于存在数据倾斜的性能优化是很有用的。dataStream.rebalance();Rescaling：以round-robin的形式将元素分区到下游操作的子集中。如果你想要将数据从一个源的每个并行实例中散发到一些mappers的子集中，用来分散负载，但是又不想要完全的rebalance 介入（引入`rebalance()`），这会非常有用。dataStream.rescale();广播：广播每个元素到所有分区。dataStream.broadcast();自定义分区：使用一个用户自定义的Partitioner对每一个元素选择目标task，由于用户对自己的数据更加熟悉，可以按照某个特征进行分区，从而优化任务执行。简单示例如下所示：// fromElements构造简单的Tuple2流DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env.fromElements(Tuple2.of(&quot;hello&quot;,1), Tuple2.of(&quot;test&quot;,2), Tuple2.of(&quot;world&quot;,100));// 定义用于分区的key值，返回即属于哪个partition的，该值加1就是对应的子任务的id号Partitioner&lt;Tuple2&lt;String, Integer&gt;&gt; strPartitioner = new Partitioner&lt;Tuple2&lt;String, Integer&gt;&gt;() {    @Override    public int partition(Tuple2&lt;String, Integer&gt; key, int numPartitions) {        return (key.f0.length() + key.f1) % numPartitions;    }};// 使用Tuple2进行分区的key值dataStream.partitionCustom(strPartitioner, new KeySelector&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt;() {    @Override    public Tuple2&lt;String, Integer&gt; getKey(Tuple2&lt;String, Integer&gt; value) throws Exception {        return value;    }}).print();</code></pre><blockquote><ol start="4"><li>配置netty网络通信</li></ol></blockquote><pre><code>操作场景Flink通信主要依赖netty网络，所以在Flink应用执行过程中，netty的设置尤为重要，网络通信的好坏直接决定着数据交换的速度以及任务执行的效率。操作步骤以下配置均可在客户端的“conf/flink-conf.yaml”配置文件中进行修改适配，默认已经是相对较优解，请谨慎修改，防止性能下降。“taskmanager.network.netty.num-arenas”： 默认是“taskmanager.numberOfTaskSlots”，表示netty的域的数量。“taskmanager.network.netty.server.numThreads”和“taskmanager.network.netty.client.numThreads”：默认是“taskmanager.numberOfTaskSlots”，表示netty的客户端和服务端的线程数目设置。“taskmanager.network.netty.client.connectTimeoutSec”：默认是120s，表示taskmanager的客户端连接超时的时间。“taskmanager.network.netty.sendReceiveBufferSize”：默认是系统缓冲区大小(cat /proc/sys/net/ipv4/tcp_[rw]mem) ，一般为4MB，表示netty的发送和接收的缓冲区大小。“taskmanager.network.netty.transport”：默认为“nio”方式，表示netty的传输方式，有“nio”和“epoll”两种方式。</code></pre><blockquote><p>5.优化设置总结</p></blockquote><pre><code>1.数据倾斜当数据发生倾斜（某一部分数据量特别大），虽然没有GC（Gabage Collection，垃圾回收），但是task执行时间严重不一致。1.1需要重新设计key，以更小粒度的key使得task大小合理化。1.2修改并行度。1.3调用rebalance操作，使数据分区均匀。2.缓冲区超时设置由于task在执行过程中存在数据通过网络进行交换，数据在不同服务器之间传递的缓冲区超时时间可以通过setBufferTimeout进行设置。当设置“setBufferTimeout(-1)”，会等待缓冲区满之后才会刷新，使其达到最大吞吐量；当设置“setBufferTimeout(0)”时，可以最小化延迟，数据一旦接收到就会刷新；当设置“setBufferTimeout”大于0时，缓冲区会在该时间之后超时，然后进行缓冲区的刷新。示例可以参考如下：env.setBufferTimeout(timeoutMillis);env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis);</code></pre><h5 id="2-程序内部接收外部传参（配置常变的参数）"><a href="#2-程序内部接收外部传参（配置常变的参数）" class="headerlink" title="2.程序内部接收外部传参（配置常变的参数）"></a>2.程序内部接收外部传参（配置常变的参数）</h5><pre class=" language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">//解析参数</span><span class="token keyword">val</span> parameters <span class="token operator">=</span> ParameterTool<span class="token punctuation">.</span>fromArgs<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//将参数设置到job 的全局参数中</span>env<span class="token punctuation">.</span>getConfig<span class="token punctuation">.</span>setGlobalJobParameters<span class="token punctuation">(</span>parameters<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//register the parameters globally</span><span class="token comment" spellcheck="true">// 获取方式： 外部的参数名</span><span class="token keyword">val</span> parallelisms <span class="token operator">=</span> parameters<span class="token punctuation">.</span>getRequired<span class="token punctuation">(</span><span class="token string">"parallelisms"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toInt###在继承富函数后 open中初始化查询<span class="token keyword">val</span> params<span class="token operator">:</span> ParameterTool <span class="token operator">=</span> getRuntimeContext<span class="token punctuation">.</span>getExecutionConfig<span class="token punctuation">.</span>getGlobalJobParameters<span class="token punctuation">.</span>asInstanceOf<span class="token punctuation">[</span>ParameterTool<span class="token punctuation">]</span><span class="token keyword">val</span> restore <span class="token operator">=</span> params<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"restoreFlag"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim</code></pre><hr><h5 id="3-加载内部配置文件-（配置固定不变的参数）"><a href="#3-加载内部配置文件-（配置固定不变的参数）" class="headerlink" title="3.加载内部配置文件 （配置固定不变的参数）"></a>3.加载内部配置文件 （配置固定不变的参数）</h5><pre class=" language-scala"><code class="language-scala"> <span class="token keyword">def</span> loadConfig<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Properties <span class="token operator">=</span> <span class="token punctuation">{</span>     <span class="token keyword">val</span> p <span class="token operator">=</span> <span class="token keyword">new</span> Properties<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">val</span> in <span class="token operator">=</span> <span class="token keyword">this</span><span class="token punctuation">.</span>getClass<span class="token punctuation">.</span>getClassLoader<span class="token punctuation">.</span>getResourceAsStream<span class="token punctuation">(</span><span class="token string">"config.properties"</span><span class="token punctuation">)</span>     p<span class="token punctuation">.</span>load<span class="token punctuation">(</span>in<span class="token punctuation">)</span>     in<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>     p   <span class="token punctuation">}</span></code></pre><hr><h5 id="4-程序中的并行执行设置"><a href="#4-程序中的并行执行设置" class="headerlink" title="4.程序中的并行执行设置"></a>4.程序中的并行执行设置</h5><blockquote><p>并行度的概念</p></blockquote><pre><code> 并行度数和slot 数有关，而slot 数是有taskManager 的核数决定的，</code></pre><blockquote><p>并行设置的方式</p></blockquote><pre><code> 1.可以内部程序接收外部传参数的形式。 2.可以内部程序env.setParallelism(16)的形式设置全局的并行度。 3.可以设置算子级别的并行度 4.命令行中进行全局设置： ./bin/flink run -p 5 ../wordCount-java*.jar</code></pre><blockquote><p>并行度设置优化点：</p></blockquote><pre><code>1. 在设置kafka source的时候，可以设置与partition 数一致的并行度。    这样会启动与分区数对等的flinkKafkaConsumer 实例。每个FlinkKafkaConsumer实例消费的topic和partition则是根据探测到的所有指定topic分区对于并行数量取余数拿到的。    公式：(startIndex+parttion.getPartition())%numParallerSubtasks == currentTaskId2.当然kafka source也可以根据topic的数量进行设置，当数据量少时，可以设置并行度小于partition数。  注意：不要设置并发数大于 partitions 总数，因为这种情况下某些并发因为分配不到 partition 导致没有数据处理。3.#        </code></pre><blockquote><p>并行度设置注意事项：</p></blockquote><pre><code>Apache Flink的并行度设置并不是说越大越好、数据处理的效率就越高。而是需要设置合理的并行度。那么何谓合理呢？Apache Flink的 并行度取决于每个TaskManager上的slot数量而决定的。Flink的JobManager把任务分成子任务提交给slot进行执行。相同的slot共享相同的JVM资源，同时对Flink提供维护的心跳等信息。slot是指TaskManagere的并发执行能力，通常来说TaskManager有多少核CPU也就会有多少个slot。这样来看，我们设置的并行度其实是与TaskManager所有Slot数量有关的</code></pre><hr><h5 id="5-name-和uid-设置的作用"><a href="#5-name-和uid-设置的作用" class="headerlink" title="5.name 和uid 设置的作用"></a>5.name 和uid 设置的作用</h5><hr><h4 id="flink-的容错层面"><a href="#flink-的容错层面" class="headerlink" title="flink 的容错层面"></a>flink 的容错层面</h4><h5 id="1-内部失败重试策略"><a href="#1-内部失败重试策略" class="headerlink" title="1.内部失败重试策略"></a>1.内部失败重试策略</h5><blockquote><p>故障恢复可设置指标：</p></blockquote><blockquote><ul><li>RestartStrategies.fixedDelayRestart  </li><li>RestartStrategies.failureRateRestart</li><li>RestartStrategies.noRestart</li></ul></blockquote><blockquote><ol><li>如果发生故障，系统将尝试重新启动作业3次，并在连续的重新启动尝试之间等待10秒。</li></ol></blockquote><pre class=" language-scala"><code class="language-scala">env<span class="token punctuation">.</span>setRestartStrategy<span class="token punctuation">(</span>RestartStrategies<span class="token punctuation">.</span>fixedDelayRestart<span class="token punctuation">(</span>  <span class="token number">3</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">// number of restart attempts</span>  Time<span class="token punctuation">.</span>of<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> TimeUnit<span class="token punctuation">.</span>SECONDS<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">// delay</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><blockquote><ol start="2"><li>超过设定的指标，则该作业最终将失败（程序常用）</li></ol></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 设置内部 如果10分钟内连续失败3次，或者每次失败间隔时间超过15s 将认为不可恢复，则作业失败。其他情况作业默认启用内部恢复策略，直到作业恢复。</span>env<span class="token punctuation">.</span>setRestartStrategy<span class="token punctuation">(</span>RestartStrategies<span class="token punctuation">.</span>failureRateRestart<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>   org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Time<span class="token punctuation">.</span>of<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> TimeUnit<span class="token punctuation">.</span>MINUTES<span class="token punctuation">)</span><span class="token punctuation">,</span>   org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Time<span class="token punctuation">.</span>of<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> TimeUnit<span class="token punctuation">.</span>SECONDS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><blockquote><ol start="3"><li>不设置重试策略，作业有异常时直接失败。</li></ol></blockquote><pre class=" language-scala"><code class="language-scala"> env<span class="token punctuation">.</span>setRestartStrategy<span class="token punctuation">(</span>RestartStrategies<span class="token punctuation">.</span>noRestart<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><hr><h4 id="flink-状态数据层面"><a href="#flink-状态数据层面" class="headerlink" title="flink 状态数据层面"></a>flink 状态数据层面</h4><h5 id="1-checkpoint-优化设置"><a href="#1-checkpoint-优化设置" class="headerlink" title="1.checkpoint 优化设置"></a>1.checkpoint 优化设置</h5><pre class=" language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">//设置checkpoint 快照时间为5分钟</span>env<span class="token punctuation">.</span>enableCheckpointing<span class="token punctuation">(</span><span class="token number">300000l</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//设置的是事件的处理时间</span><span class="token punctuation">.</span>setStreamTimeCharacteristic<span class="token punctuation">(</span>TimeCharacteristic<span class="token punctuation">.</span>ProcessingTime<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">/***设置checkpoint 取消时清理和保留机制：* 1.DELETE_ON_CANCELLATION 工作取消时删除checkpoint 做的检查点。* 2.RETAIN_ON_CANCELLATION 工作取消时保留checkpoint 做的检查点*/</span>env<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span>enableExternalizedCheckpoints<span class="token punctuation">(</span>ExternalizedCheckpointCleanup<span class="token punctuation">.</span>RETAIN_ON_CANCELLATION<span class="token punctuation">)</span><span class="token comment" spellcheck="true">// 设置两个检查点之间的最小间隔，</span>env<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span>setMinPauseBetweenCheckpoints<span class="token punctuation">(</span><span class="token number">1000L</span> <span class="token operator">*</span> <span class="token number">60L</span> <span class="token operator">*</span> <span class="token number">5L</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置checkpoint的超时时间</span>env<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span>setCheckpointTimeout<span class="token punctuation">(</span><span class="token number">1000L</span> <span class="token operator">*</span> <span class="token number">60L</span> <span class="token operator">*</span> <span class="token number">30L</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置checkpoint 的最大并发数       </span>env<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span>setMaxConcurrentCheckpoints<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//当前path 最好是hdfs 的路径，s3 的路径有时会出现一致性的问题。</span><span class="token keyword">val</span> backend <span class="token operator">=</span> <span class="token keyword">new</span> RocksDBStateBackend<span class="token punctuation">(</span>path<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">/*** 设置rsDB的保存策略* 由于flink 不依赖磁盘上的RocksDB数据进行恢复，因此无需将数据同步到稳定的存储中。* 1.DEFAULT ：所有设置都是默认选项，但不强制写入磁盘* 2.SPINNING_DISK_OPTIMIZED：使用常规硬盘提高性能。* 3.SPINNING_DISK_OPTIMIZED_HIGH_MEM：此配置将会应用大量的内存用于块的缓存和压缩，如果遇到DB内存不足，建议切换为第二种SPINNING_DISK_OPTIMIZED。* 4.FLASH_SSD_OPTIMIZED：使用SSD 提高性能**/</span>backend<span class="token punctuation">.</span>setPredefinedOptions<span class="token punctuation">(</span>PredefinedOptions<span class="token punctuation">.</span>FLASH_SSD_OPTIMIZED<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置checkpoint的模式：CheckpointingMode.EXACTLY_ONCE or CheckpointingMode.AT_LEAST_ONCE</span><span class="token comment" spellcheck="true">//默认是使用的 EXACTLY_ONCE</span>env<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span>setCheckpointingMode<span class="token punctuation">(</span>CheckpointingMode<span class="token punctuation">.</span>EXACTLY_ONCE<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//将ck保存进行set</span>env<span class="token punctuation">.</span>setStateBackend<span class="token punctuation">(</span>backend<span class="token punctuation">.</span>asInstanceOf<span class="token punctuation">[</span>StateBackend<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><hr><h5 id="2-自动清理程序状态（flink-1-8-And-Processing-Time）"><a href="#2-自动清理程序状态（flink-1-8-And-Processing-Time）" class="headerlink" title="2.自动清理程序状态（flink 1.8+ And Processing Time）"></a>2.自动清理程序状态（flink 1.8+ And Processing Time）</h5><blockquote><p>基本介绍</p></blockquote><pre><code>1.在Flink的DataStream API中，应用程序状态是由状态描述符（state descriptor）来定义的。状态生存时间是通过将StateTtlConfiguration对象传递给状态描述符来配置的。 2.flink 提供多个选项配置状态的生存时间行为* 什么时候重置生存时间？ 默认情况下，当状态被修改时，生存时间就会被更新。我们也可以在读操作访问状态时更新相关项的生存时间，但这样要花费额外的写操作来更新时间戳。*已经过期的数据是否可以访问？ 状态生存时间机制使用的是惰性策略来清除过期状态。这可能导致应用程序会尝试读取过期但尚未删除的状态。用户可以配置对这样的读取请求是否返回过期状态。无论哪种情况，过期状态都会在之后立即被删除。虽然返回已经过期的状态有利于数据可用性，但不返回过期状态更符合相关数据保护法规的要求。*哪种时间语义被用于定义生存时间？ 在Apache Flink 1.8.0中，用户只能根据处理时间（Processing Time）定义状态生存时间。未来的Flink版本中计划支持事件时间（Event Time）。在实现上，状态生存时间特性会额外存储上一次相关状态访问的时间戳。虽然这种方法增加了一些存储开销，但它允许Flink在访问状态、创建检查点、恢复或存储清理过程时可以检查过期状态。3.堆内存状态后端的增量清理适用于堆内存状态后端（FsStateBackend和MemoryStateBackend）。其基本思路是在存储后端的所有状态条目上维护一个全局的惰性迭代器。某些事件（例如状态访问）会触发增量清理，而每次触发增量清理时，迭代器都会向前遍历删除已遍历的过期数据。》注意：首先是增量清理所花费的时间会增加记录处理的延迟。其次，如果没有状态被访问（state accessed）或者没有记录被处理（record processed），过期的状态也将不会被删除。4.RocksDB状态后端利用后台压缩来清理过期状态该策略基于Flink定制的RocksDB压缩过滤器（compaction filter）。RocksDB会定期运行异步的压缩流程以合并数据并减少相关存储的数据量，该定制的压缩过滤器使用生存时间检查状态条目的过期时间戳，并丢弃所有过期值。》注意：启用Flink的生存时间压缩过滤机制后，会放缓RocksDB的压缩速度。4.也可以使用定时器进行状态的清理</code></pre><pre class=" language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">//当前是以RocksDB状态后端利用后台压缩来清理过期状态 为例的代码</span>    <span class="token keyword">val</span> ttlConfig <span class="token operator">=</span> StateTtlConfig      <span class="token punctuation">.</span>newBuilder<span class="token punctuation">(</span>Time<span class="token punctuation">.</span>minutes<span class="token punctuation">(</span>parameters<span class="token punctuation">.</span>getRequired<span class="token punctuation">(</span><span class="token string">"ttl-minutes"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toLong<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//更新当前时间戳之前，压缩过滤器要处理的状态条目数 默认值1000L</span>      <span class="token punctuation">.</span>cleanupInRocksdbCompactFilter<span class="token punctuation">(</span>parameters<span class="token punctuation">.</span>getRequired<span class="token punctuation">(</span><span class="token string">"query-time-after-num-entries"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">.</span>setTimeCharacteristic<span class="token punctuation">(</span>TimeCharacteristic<span class="token punctuation">.</span>ProcessingTime<span class="token punctuation">)</span>      <span class="token punctuation">.</span>setUpdateType<span class="token punctuation">(</span>StateTtlConfig<span class="token punctuation">.</span>UpdateType<span class="token punctuation">.</span>OnCreateAndWrite<span class="token punctuation">)</span>      <span class="token punctuation">.</span>setStateVisibility<span class="token punctuation">(</span>StateTtlConfig<span class="token punctuation">.</span>StateVisibility<span class="token punctuation">.</span>NeverReturnExpired<span class="token punctuation">)</span>      <span class="token punctuation">.</span>build    <span class="token keyword">val</span> stateDescriptor <span class="token operator">=</span> <span class="token keyword">new</span> ValueStateDescriptor<span class="token punctuation">[</span>ArrayBuffer<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"state"</span><span class="token punctuation">,</span> createTypeInformation<span class="token punctuation">[</span>ArrayBuffer<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//开启状态生存时间</span>    stateDescriptor<span class="token punctuation">.</span>enableTimeToLive<span class="token punctuation">(</span>ttlConfig<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//开启状态client 可查询</span>    stateDescriptor<span class="token punctuation">.</span>setQueryable<span class="token punctuation">(</span><span class="token string">"state"</span><span class="token punctuation">)</span>    state <span class="token operator">=</span> getRuntimeContext<span class="token punctuation">.</span>getState<span class="token punctuation">(</span>stateDescriptor<span class="token punctuation">)</span></code></pre><h5 id="3-状态数据的使用"><a href="#3-状态数据的使用" class="headerlink" title="3.状态数据的使用"></a>3.状态数据的使用</h5><h4 id="flink-监控层面"><a href="#flink-监控层面" class="headerlink" title="flink 监控层面"></a>flink 监控层面</h4><h5 id><a href="#" class="headerlink" title></a></h5><h4 id="flink-延迟数据的处理"><a href="#flink-延迟数据的处理" class="headerlink" title="flink 延迟数据的处理"></a>flink 延迟数据的处理</h4><p>Watermark机制假设在某个时间点上，不会有比这个时间点更晚的上报数据，常被作为一个时间窗口的结束时间，</p><blockquote><p>生成WaterMaker</p></blockquote><pre><code>流数据中的事件时间戳与Watermark高度相关，事件时间戳的抽取和Watermark的生成也基本是同时进行的，抽取的过程会遇到下面两种情况：1.数据流中已经包含了事件时间戳和Watermark。2.使用抽取算子生成事件时间戳和Watermark，这也是实际应用中更为常见的场景。因为后续的计算都依赖时间，抽取算子最好在数据接入后马上使用。具体而言，抽取算子包含两个函数：第一个函数从数据流的事件中抽取时间戳，并将时间戳赋值到事件的元数据上，第二个函数生成Watermark。</code></pre><h4 id="flink-streamAPI层面"><a href="#flink-streamAPI层面" class="headerlink" title="flink streamAPI层面"></a>flink streamAPI层面</h4><h5 id="1-source-和-sink-常用"><a href="#1-source-和-sink-常用" class="headerlink" title="1.source 和 sink 常用"></a>1.source 和 sink 常用</h5><blockquote><p>source         </p><blockquote><ol><li>flink kafka connector</li></ol><ul><li>基本设置：</li></ul></blockquote></blockquote><pre><code>1.因为 kafka 中数据都是以二进制 byte 形式存储的。读到 Flink 系统中之后，需要将二进制数据转化为具体的 java、scala 对象。具体需要实现一个 schema 类，定义如何序列化和反序列数据，flink 也提供了常用的序列化反序列化的schema类，例如SimpleStringSchema（按字符串方式进行序列化、反序列化），JsonDeserializationSchema 使用 jackson 反序列化 json 格式消息，并返回 ObjectNode，可以使用 .get(“property”) 方法来访问相应字段。2.消费起始位置设置 2.1 setStartFromLatest: 设置最新位置开始读。 2.2 setStartFromEarliest: 设置最早位置开始读. 2.3 setStartFromGroupOffsets: 也是默认的策略，从 group offset 位置读取数据，group offset 指的是 kafka broker 端记录的某个 group 的最后一次的消费位置。但是 kafka broker 端没有该 group 信息，会根据 kafka 的参数&quot;auto.offset.reset&quot;的设置来决定从哪个位置开始消费.注意事项：作业故障从savepoint自动恢复时，以及手动做savepoint时，消费位置从保存状态中恢复，与上面的起始位置设置无关。因为Flink Kafka Consumer 不依赖于提交的 offset 来实现容错保证。提交的 offset 只是一种方法，用于公开 consumer 的进度以便进行监控。3.setCommitOffsetsOnCheckpoints(true)时 会将偏移量提交到checkpoint快照的状态数据中。4.other 配置： 4.1 flink.poll-timeout:配置轮询的超时时间。如果没有可用数据，则等待轮询所需的时间 默认是100毫秒 4.2 flink.partition-discovery.interval-millis 设置参数为非负值，表示开启动态发现的开关，以及设置的时间间隔，下面会以场景的形式进行讲解,也就是下面要说的topic 和 partition 的动态发现。5.topic 和 partition 动态发现    场景一：有一个 Flink 作业需要将五份数据聚合到一起，五份数据对应五个 kafka topic，随着业务增长，新增一类数据，同时新增了一个 kafka topic，如何在不重启作业的情况下作业自动感知新的 topic。    场景二：作业从一个固定的 kafka topic 读数据，开始该 topic 有 10 个 partition，但随着业务的增长数据量变大，需要对 kafka partition 个数进行扩容，由 10 个扩容到 20。该情况下如何在不重启作业情况下动态感知新扩容的 partition？    针对上面两种场景，需要在构建 FlinkKafkaConsumer 时的 properties 中设置 flink.partition-discovery.interval-millis(默认为false) 参数为非负值，表示开启动态发现的开关，而设置的值为发现时间间隔。    原理：FlinkKafkaConsumer 内部会启动一个单独的线程定期去 kafka 获取最新的 meta 信息。    针对场景一：需在构建 FlinkKafkaConsumer 时，topic 的描述可以传一个正则表达式描述的 pattern。每次获取最新 kafka meta 时获取正则匹配的最新 topic 列表。    代码：    val topicStr = &quot;topic1&quot;    val topicPattern: Pattern = java.util.regex.Pattern.compile(&quot;topic[0-9]&quot;)    val consumer = new FlinkKafkaConsumer011[String](topicPattern, new SimpleStringSchema(), props)        针对场景二：设置前面的动态发现参数，在定期获取 kafka 最新 meta 信息时会匹配新的 partition。为了保证数据的正确性，新发现的 partition 从最早的位置开始读取    代码：在properties  设置 flink.partition-discovery.interval-millis:发现时间间隔（毫秒）6.commit offset 方式 提交 offset 的方式分为两种： checkpoint 关闭：        依赖kafka 客户端的auto commit定期提交。        需要设置 enable.auto.commit,auto.commit.interval.ms 参数到配置文件。 checkpoint 开启：构建source 时 setCommitOffsetsOnCheckpoints(true)          offset 自己在checkpoint state 中进行管理和维护。提交kafka offset 和ck时间一致，仅仅为了外部监控消费情况。         通过setCommitOffsetsOnCheckpoints 设置,ck成功后是否提交offset 到kafka 。7.Timestamp Extraction/Watermark 生成  我们知道当 Flink 作业内使用 EventTime 属性时，需要指定从消息中提取时戳和生成水位的函数。  FlinkKakfaConsumer 构造的 source 后直接调用 assignTimestampsAndWatermarks 函数设置水位生成器的好处是此时是每个 partition 一个 watermark assigner，  如下图。source 生成的时戳为多个 partition 时戳对齐后的最小时戳。此时在一个 source 读取多个 partition，并且 partition 之间数据时戳有一定差距的情况下，因为在 source 端 watermark 在 partition 级别有对齐，不会导致数据读取较慢 partition 数据丢失</code></pre><p><img src="https://pic3.zhimg.com/v2-c6614444177e3c59dc86123748db0b4a_r.jpg" alt="flink-kafka-consumer waterMaker"></p><pre><code>8.Producer 分区   8.1 FlinkKafkaProducer 往 kafka 中写数据时，如果不单独设置 partition 策略，会默认使用 FlinkFixedPartitioner，该 partitioner 分区的方式是 task 所在的并发 id 对 topic 总 partition 数取余：parallelInstanceId % partitions.length那么如果sink 的task 为4 partition 为2 则 4/2= 2 则2个并行度往2个partion 里写。那么如果sink 的task 为4 partition 为1 则 4/1= 4 则4个并行度往1个partion 里写。那么如果sink 的task 为2 partition 为4 则 2/4    则2个并行度往2个partion 里写，剩余两个partion 将没有数据。   8.2 构建 FlinkKafkaProducer 时，partition 设置为 null，此时会使用 kafka producer 默认分区方式，非 key 写入的情况下，使用 round-robin 的方式进行分区，每个 task 都会轮循的写下游的所有 partition。该方式下游的 partition 数据会比较均衡，但是缺点是 partition 个数过多的情况下需要维持过多的网络连接，即每个 task 都会维持跟所有 partition 所在 broker 的连接。9.连接kafka容错  Flink kafka 09、010 版本下通过 setLogFailuresOnly 为 false，setFlushOnCheckpoint 为 true，能达到 at-least-once 语义。  setLogFailuresOnly，默认为 false，是控制写 kafka 失败时，是否只打印失败的 log 不抛异常让作业停止。  setFlushOnCheckpoint，默认为 true，是控制是否在 checkpoint 时 fluse 数据到 kafka，保证数据已经写到 kafka。否则数据有可能还缓存在 kafka 客户端的 buffer 中，并没有真正写出到 kafka，此时作业挂掉数据即丢失，不能做到至少一次的语义。  Flink kafka 011 版本下，通过两阶段提交的 sink 结合 kafka 事务的功能，可以保证端到端精准一次 </code></pre><p><a href="https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka" target="_blank" rel="noopener">flink到kafka精准一次</a></p><hr><blockquote><blockquote><ul><li>构建flink kafka source代码：</li></ul></blockquote></blockquote><pre class=" language-scala"><code class="language-scala"> <span class="token comment" spellcheck="true">//设置kafka</span><span class="token keyword">val</span> props <span class="token operator">=</span> <span class="token keyword">new</span> Properties<span class="token punctuation">(</span><span class="token punctuation">)</span>props<span class="token punctuation">.</span>setProperty<span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span>config<span class="token punctuation">.</span>getProperty<span class="token punctuation">(</span>BOOTSTRAP_SERVERS<span class="token punctuation">)</span><span class="token punctuation">)</span>props<span class="token punctuation">.</span>setProperty<span class="token punctuation">(</span><span class="token string">"group.id"</span><span class="token punctuation">,</span> parameters<span class="token punctuation">.</span>getRequired<span class="token punctuation">(</span><span class="token string">"groupid"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">)</span>props<span class="token punctuation">.</span>setProperty<span class="token punctuation">(</span><span class="token string">"auto.offset.reset"</span><span class="token punctuation">,</span> <span class="token string">"latest"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//获取kafka的数据源（如有特殊配制可以根据以上基本设置进行配置）</span><span class="token keyword">val</span> source <span class="token operator">=</span> env<span class="token punctuation">.</span>addSource<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token keyword">new</span> FlinkKafkaConsumer011<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>getProperty<span class="token punctuation">(</span>TOPIC<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> SimpleStringSchema<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> props<span class="token punctuation">)</span>      <span class="token punctuation">.</span>setCommitOffsetsOnCheckpoints<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>setParallelism<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>name<span class="token punctuation">(</span><span class="token string">"topic-source"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>uid<span class="token punctuation">(</span><span class="token string">"topic-source"</span><span class="token punctuation">)</span></code></pre><hr><blockquote><ol start="2"><li>flink mysql Async I/O 访问</li></ol></blockquote><blockquote><ol start="3"><li>flink redis Async I/O 访问</li></ol></blockquote><hr><blockquote><p>sink</p></blockquote><blockquote><blockquote><ol><li>flink sink kafka</li></ol></blockquote></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token keyword">val</span> stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token keyword">val</span> myProducer <span class="token operator">=</span> <span class="token keyword">new</span> FlinkKafkaProducer011<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span>        <span class="token string">"localhost:9092"</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">// broker 列表</span>        <span class="token string">"my-topic"</span><span class="token punctuation">,</span>               <span class="token comment" spellcheck="true">// 目标 topic</span>        <span class="token keyword">new</span> SimpleStringSchema<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true">// 序列化 schema</span><span class="token comment" spellcheck="true">// 0.10+ 版本的 Kafka 允许在将记录写入 Kafka 时附加记录的事件时间戳；</span><span class="token comment" spellcheck="true">// 此方法不适用于早期版本的 Kafka</span>myProducer<span class="token punctuation">.</span>setWriteTimestampToKafka<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span>stream<span class="token punctuation">.</span>addSink<span class="token punctuation">(</span>myProducer<span class="token punctuation">)</span></code></pre><hr><blockquote><blockquote><ol start="2"><li>flink sink cassandra</li></ol></blockquote></blockquote><blockquote><blockquote><ol start="3"><li>flink sink redis</li></ol></blockquote></blockquote><blockquote><blockquote><ol start="4"><li>flink sink hdfs</li></ol></blockquote></blockquote><blockquote><blockquote><ol start="5"><li>flink sink es</li></ol></blockquote></blockquote><hr><blockquote><p>side outPut的使用</p></blockquote><pre><code>除了DataStream操作产生的主流之外，您还可以产生任意数量的附加副输出结果流。结果流中的数据类型不必与主流中的数据类型匹配，并且不同侧输出的类型也可以不同。</code></pre><hr><h5 id="2-ProcessFunction-的使用"><a href="#2-ProcessFunction-的使用" class="headerlink" title="2.ProcessFunction 的使用"></a>2.ProcessFunction 的使用</h5><h5 id="3-异步加载外部数据"><a href="#3-异步加载外部数据" class="headerlink" title="3.异步加载外部数据"></a>3.异步加载外部数据</h5><h5 id="4-缓存配置文件数据"><a href="#4-缓存配置文件数据" class="headerlink" title="4.缓存配置文件数据"></a>4.缓存配置文件数据</h5><pre class=" language-scala"><code class="language-scala">env<span class="token punctuation">.</span>registerCachedFile<span class="token punctuation">(</span>path<span class="token punctuation">,</span><span class="token string">'myFileCache'</span><span class="token punctuation">)</span>在富函数的open方法中进行获取 <span class="token keyword">val</span> myFile <span class="token operator">=</span> getRuntimeContext<span class="token punctuation">.</span>getDistributedCache<span class="token punctuation">.</span>getFile<span class="token punctuation">(</span><span class="token string">'MyTestFile'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//flink支持将变量广播到worker上，以供程序运算使用。</span></code></pre><h4 id="flink-应对数据解析"><a href="#flink-应对数据解析" class="headerlink" title="flink 应对数据解析"></a>flink 应对数据解析</h4><blockquote><p>判断数据是否为空</p></blockquote><pre><code> StringUtils.isNotBlank(data)</code></pre><blockquote><p>如果有就获取，没有就为默认值</p></blockquote><pre class=" language-scala"><code class="language-scala"> person<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span>name<span class="token punctuation">)</span></code></pre><blockquote><p>时间解析 以秒为单位</p></blockquote><pre class=" language-scala"><code class="language-scala"> <span class="token comment" spellcheck="true">//解析格式以秒为单位的数字事件时间字段，如果解析异常，则用当前系统时间代替，返回秒</span> <span class="token keyword">def</span> parseTimestamp<span class="token punctuation">(</span>timestamp<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Long</span> <span class="token operator">=</span> <span class="token keyword">try</span> <span class="token punctuation">{</span>    timestamp<span class="token punctuation">.</span>toLong  <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">{</span>    <span class="token keyword">case</span> e<span class="token operator">:</span> Exception <span class="token keyword">=></span>      logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span>s<span class="token string">"Parsing event time exception: ${e.getMessage}"</span><span class="token punctuation">,</span> e<span class="token punctuation">)</span>      System<span class="token punctuation">.</span>currentTimeMillis<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span>  <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">//解析格式为yyyy-MM-dd HH:mm:ss的事件时间字段，如果解析异常，则用当前系统事件代替，返回毫秒</span> <span class="token keyword">def</span> parseDateTime<span class="token punctuation">(</span>dateTime<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Long</span> <span class="token operator">=</span> <span class="token keyword">try</span> <span class="token punctuation">{</span>    DateTimeFormat<span class="token punctuation">.</span>forPattern<span class="token punctuation">(</span><span class="token string">"yyyy-MM-dd HH:mm:ss"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>withZone<span class="token punctuation">(</span>DateTimeZone<span class="token punctuation">.</span>forID<span class="token punctuation">(</span><span class="token string">"+08:00"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>parseDateTime<span class="token punctuation">(</span>dateTime<span class="token punctuation">)</span><span class="token punctuation">.</span>getMillis  <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">{</span>    <span class="token keyword">case</span> e<span class="token operator">:</span> Exception <span class="token keyword">=></span>      logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span>s<span class="token string">"Parsing event time exception: ${e.getMessage}"</span><span class="token punctuation">,</span> e<span class="token punctuation">)</span>      System<span class="token punctuation">.</span>currentTimeMillis<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span></code></pre><blockquote><p>加载配置文件</p></blockquote><pre class=" language-scala"><code class="language-scala"> <span class="token keyword">def</span> loadConfig<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Properties <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">val</span> p <span class="token operator">=</span> <span class="token keyword">new</span> Properties<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">val</span> in <span class="token operator">=</span> <span class="token keyword">this</span><span class="token punctuation">.</span>getClass<span class="token punctuation">.</span>getClassLoader<span class="token punctuation">.</span>getResourceAsStream<span class="token punctuation">(</span><span class="token string">"config.properties"</span><span class="token punctuation">)</span>    p<span class="token punctuation">.</span>load<span class="token punctuation">(</span>in<span class="token punctuation">)</span>    in<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    p  <span class="token punctuation">}</span></code></pre><blockquote><p>切分日志 split</p></blockquote><pre class=" language-scala"><code class="language-scala">  <span class="token comment" spellcheck="true">//按照制表符切割日志</span>  <span class="token keyword">val</span> splitter <span class="token operator">=</span> Pattern<span class="token punctuation">.</span>compile<span class="token punctuation">(</span><span class="token string">"\001"</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> split<span class="token punctuation">(</span>log<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    splitter<span class="token punctuation">.</span>split<span class="token punctuation">(</span>log<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span></code></pre><blockquote><p>josn 的解析（alibaba.fastjson）</p></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token keyword">val</span> tmp <span class="token operator">=</span> <span class="token keyword">try</span><span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//将json 转化为对象，并获取name 字段的字符串值</span>  JSON<span class="token punctuation">.</span>parseObject<span class="token punctuation">(</span>template<span class="token punctuation">)</span><span class="token punctuation">.</span>getString<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token keyword">catch</span> <span class="token punctuation">{</span><span class="token keyword">case</span> e<span class="token operator">:</span>Exception <span class="token keyword">=></span>     logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span>s<span class="token string">"Parsing json from template field occur exception: ${e.getMessage}"</span><span class="token punctuation">,</span>e<span class="token punctuation">)</span>    <span class="token string">""</span><span class="token punctuation">}</span></code></pre><blockquote><p>正则表达式对设备id 的匹配</p></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token keyword">val</span> deviceIdReg <span class="token operator">=</span> <span class="token string">"^[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}$"</span><span class="token punctuation">.</span>r<span class="token keyword">def</span> filterDeviceId<span class="token punctuation">(</span>deviceId<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    deviceIdReg<span class="token punctuation">.</span>findFirstMatchIn<span class="token punctuation">(</span>deviceId<span class="token punctuation">)</span><span class="token punctuation">.</span>isDefined <span class="token operator">&amp;&amp;</span> deviceId <span class="token operator">!=</span> <span class="token string">"00000000-0000-0000-0000-000000000000"</span>  <span class="token punctuation">}</span></code></pre><blockquote><p>组合字符</p></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token keyword">private</span> <span class="token keyword">val</span> joiner <span class="token operator">=</span> Joiner<span class="token punctuation">.</span>on<span class="token punctuation">(</span><span class="token string">"^"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>useForNull<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token keyword">def</span> join<span class="token punctuation">(</span>str1<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">,</span> str2<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">,</span> str<span class="token operator">:</span> <span class="token builtin">String</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">String</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    joiner<span class="token punctuation">.</span>join<span class="token punctuation">(</span>str1<span class="token punctuation">,</span> str2<span class="token punctuation">,</span> str<span class="token operator">:</span> _<span class="token operator">*</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// result = A^B^C^D</span> <span class="token keyword">val</span> result <span class="token operator">=</span> join<span class="token punctuation">(</span><span class="token string">"A"</span><span class="token punctuation">,</span><span class="token string">"B"</span><span class="token punctuation">,</span><span class="token string">"C"</span><span class="token punctuation">,</span><span class="token string">"D"</span><span class="token punctuation">)</span></code></pre><blockquote><p>构建测试类测试数据解析是否正常</p></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token keyword">import</span> org<span class="token punctuation">.</span>scalatest<span class="token punctuation">.</span>FunSuite<span class="token keyword">import</span> scala<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Source<span class="token keyword">class</span> ScalaTest <span class="token keyword">extends</span> FunSuite <span class="token punctuation">{</span>    test<span class="token punctuation">(</span><span class="token string">"test-parser-data"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>          <span class="token keyword">val</span> dataFilter <span class="token operator">=</span> <span class="token keyword">new</span> DataFilter          <span class="token keyword">val</span> file<span class="token operator">=</span>Source<span class="token punctuation">.</span>fromFile<span class="token punctuation">(</span><span class="token string">"local_file_path"</span><span class="token punctuation">)</span>          <span class="token keyword">val</span> lines <span class="token operator">=</span> file<span class="token punctuation">.</span>getLines        <span class="token keyword">var</span> counter<span class="token punctuation">,</span>total <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span><span class="token punctuation">(</span>line <span class="token keyword">&lt;-</span> lines<span class="token punctuation">)</span><span class="token punctuation">{</span>            total <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>dataFilter<span class="token punctuation">.</span>filter<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                counter <span class="token operator">+=</span> <span class="token number">1</span>                <span class="token keyword">val</span> data <span class="token operator">=</span> 解析完的数据。                printBean<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            <span class="token punctuation">}</span>         <span class="token punctuation">}</span>         println<span class="token punctuation">(</span><span class="token string">"-"</span><span class="token operator">*</span><span class="token number">10</span><span class="token operator">+</span><span class="token string">"counter="</span><span class="token operator">+</span>counter <span class="token operator">+</span><span class="token string">",total="</span><span class="token operator">+</span> total<span class="token punctuation">)</span>         file<span class="token punctuation">.</span>close     <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><blockquote><p>加载定时加载外部数据</p></blockquote><pre class=" language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">//设置当前的线程数</span>  <span class="token keyword">var</span> countDown<span class="token operator">:</span> CountDownLatch <span class="token operator">=</span> <span class="token keyword">new</span> CountDownLatch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token annotation punctuation">@volatile</span> <span class="token keyword">var</span> resultData<span class="token operator">:</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> _  <span class="token keyword">def</span> getUccrAndKvAndDefalut<span class="token operator">:</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>countDown<span class="token punctuation">.</span>getCount <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      countDown<span class="token punctuation">.</span>await<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    resultData  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//创建调度器</span>  <span class="token keyword">var</span> executor<span class="token operator">:</span> ScheduledExecutorService <span class="token operator">=</span> Executors<span class="token punctuation">.</span>newSingleThreadScheduledExecutor<span class="token punctuation">(</span>    <span class="token keyword">new</span> ThreadFactoryBuilder<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>setDaemon<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>setNameFormat<span class="token punctuation">(</span><span class="token string">"loaderData-%d"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>build<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">// 开始调度 一开始就执行，每三分钟调度一次</span>  executor<span class="token punctuation">.</span>scheduleAtFixedRate<span class="token punctuation">(</span><span class="token keyword">new</span> Runnable <span class="token punctuation">{</span>    <span class="token keyword">override</span> <span class="token keyword">def</span> run<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>      loadData<span class="token punctuation">(</span><span class="token punctuation">)</span>      logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Sync resultData from filesystem complete"</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> TimeUnit<span class="token punctuation">.</span>MINUTES<span class="token punctuation">)</span> <span class="token keyword">def</span> loadData<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    resultData <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token string">"1"</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token string">"A"</span><span class="token punctuation">)</span>     <span class="token keyword">if</span> <span class="token punctuation">(</span>countDown<span class="token punctuation">.</span>getCount <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      countDown<span class="token punctuation">.</span>countDown<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span></code></pre><h4 id="flink-排查问题层面"><a href="#flink-排查问题层面" class="headerlink" title="flink 排查问题层面"></a>flink 排查问题层面</h4><h5 id="1-查看-flinkUI-的监控"><a href="#1-查看-flinkUI-的监控" class="headerlink" title="1.查看 flinkUI 的监控"></a>1.查看 flinkUI 的监控</h5><pre><code>1.查看当前作业运行的状态2.如果有checkpoint 快照，首先查看快照是否按照约定的时间触发。3.查看作业是否有背压。-&gt;根据拓扑结构定位错误位置，查看哪里数据产生堆积。4.在EXception 处查看是否有异常信息。5.如果on yarn 上，可以查看container 和 jobmanager的log 来定位错误。</code></pre><hr><h5 id="2-查看kafka-监控，看是否offset-没有提交数据消费延迟"><a href="#2-查看kafka-监控，看是否offset-没有提交数据消费延迟" class="headerlink" title="2.查看kafka 监控，看是否offset 没有提交数据消费延迟"></a>2.查看kafka 监控，看是否offset 没有提交数据消费延迟</h5><blockquote><p>promethus 抓取kafka 的信息，并通过grafana 展现出来。</p></blockquote><hr><h5 id="3-如果发现subTask-有问题，怎么排查"><a href="#3-如果发现subTask-有问题，怎么排查" class="headerlink" title="3.如果发现subTask 有问题，怎么排查"></a>3.如果发现subTask 有问题，怎么排查</h5><p>基本思路是根据restApi来定位:<br><a href="https://juejin.im/post/5d973af2518825096a1874f4" target="_blank" rel="noopener">Flink定位SubTask在哪台机器哪个进程执行</a></p><h5 id="4-keyby-的数据倾斜了怎么办"><a href="#4-keyby-的数据倾斜了怎么办" class="headerlink" title="4.keyby 的数据倾斜了怎么办"></a>4.keyby 的数据倾斜了怎么办</h5><pre class=" language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">//1. 将值进行hash</span><span class="token comment" spellcheck="true">//将keyData 进行hash</span>    <span class="token keyword">val</span> hashCode<span class="token operator">:</span> HashCode <span class="token operator">=</span> Hashing<span class="token punctuation">.</span>murmur3_32<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>hashString<span class="token punctuation">(</span>keyData<span class="token punctuation">,</span> Charset<span class="token punctuation">.</span>forName<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>hashCode<span class="token punctuation">.</span>hashCode<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> Integer<span class="token punctuation">.</span>MAX_VALUE<span class="token punctuation">,</span> hashCode<span class="token punctuation">.</span>toString<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink-优化篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客编写思路</title>
      <link href="/2019/12/17/%E5%8D%9A%E5%AE%A2%E7%BC%96%E5%86%99%E6%80%9D%E8%B7%AF/"/>
      <url>/2019/12/17/%E5%8D%9A%E5%AE%A2%E7%BC%96%E5%86%99%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<h4 id="博客类型分类"><a href="#博客类型分类" class="headerlink" title="博客类型分类"></a>博客类型分类</h4><h5 id="常用篇"><a href="#常用篇" class="headerlink" title="常用篇"></a>常用篇</h5><blockquote><p>主要介绍本知识点和框架常用到的命令和技巧，以及需要注意的事项。</p></blockquote><h5 id="学习篇"><a href="#学习篇" class="headerlink" title="学习篇"></a>学习篇</h5><blockquote><p>主要介绍本知识点和框架的3W， who 是什么，where 应用场景， what 怎么用比较好。</p></blockquote><h5 id="问题篇"><a href="#问题篇" class="headerlink" title="问题篇"></a>问题篇</h5><blockquote><p>在使用本知识点或框架中遇到的问题。</p></blockquote><h5 id="资料篇"><a href="#资料篇" class="headerlink" title="资料篇"></a>资料篇</h5><blockquote><p>扩展本知识点或架构的其他比较好的学习资料。</p></blockquote><h5 id="优化篇"><a href="#优化篇" class="headerlink" title="优化篇"></a>优化篇</h5><blockquote><p>主要编写的是当前组件或架构可优化点。</p></blockquote><h5 id="实战篇"><a href="#实战篇" class="headerlink" title="实战篇"></a>实战篇</h5><blockquote><p>主要演示在知识点实际搭建过程中的操作。</p></blockquote><h5 id="other"><a href="#other" class="headerlink" title="other"></a>other</h5><blockquote><p>主要是小的知识点分类，比如java-JVM篇，java-cache篇，java-多线程篇</p></blockquote><h4 id="博客思想"><a href="#博客思想" class="headerlink" title="博客思想"></a>博客思想</h4><blockquote><p>本站博客，是以练促学，以学促用的理念来进行编写，当然为了取其精华去其糟粕，博客中也会引用总结其他大牛比较好的内容，用来学习。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 博客编写思路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客编写思路 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java-cache篇</title>
      <link href="/2019/12/17/java-cache%E7%AF%87/"/>
      <url>/2019/12/17/java-cache%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p> 首先要有意识的进行缓存的使用。在某值多次使用的时候，可以考虑把当前值获取到并放到缓存中去，使得数据在内存中获取，而非外部存储，能极大提高数据获取的速度。但要注意缓存数据的容量大小。</p><h4 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h4><blockquote><p>构建缓存</p></blockquote><pre><code> var cache: Cache[String, String] = CacheBuilder.newBuilder()    .expireAfterAccess(30, TimeUnit.MINUTES)   //过期时间    .maximumSize(30000) //最大条数    .build()</code></pre><blockquote><p>获取缓存，如果没有则进行计算 或者用getIfPresent 获取缓存</p></blockquote><pre><code>try {    // If the key wasn&#39;t in the &quot;easy to compute&quot; group, we need to    // do things the hard way.    cache.get(key, () -&gt;  doThingsTheHardWay(key));} catch (ExecutionException e) {    throw new OtherException(e.getCause());}</code></pre><blockquote><p>使用cache.put(key, value)方法可以直接向缓存中插入值，这会直接覆盖掉给定键之前映射的值</p></blockquote><hr><blockquote><p>缓存收回策略 ： </p><blockquote><p>1.基于容量的回收（size-based eviction）</p></blockquote></blockquote><pre><code>使用CacheBuilder.maximumSize(long)缓存将尝试回收最近没有使用或总体上很少使用的缓存项。警告：在缓存项的数目达到限定值之前，缓存就可能进行回收操作,通常来说，这种情况发生在缓存项的数目逼近限定值时</code></pre><blockquote><blockquote><p>2.定时回收（Timed Eviction）</p></blockquote></blockquote><pre><code>1.expireAfterAccess(long, TimeUnit)：缓存项在给定时间内没有被读/写访问，则回收。请注意这种缓存的回收顺序和基于容量回收一样2.expireAfterWrite(long, TimeUnit)：缓存项在给定时间内没有被写访问（创建或覆盖），则回收。如果认为缓存数据总是在固定时候后变得陈旧不可用，这种回收方式是可取的。</code></pre><blockquote><blockquote><p>3.基于引用的回收（Reference-based Eviction）</p></blockquote></blockquote><pre><code>通过弱引用的键或者弱引用的值，或者软引用的值，guava Cache可以把缓存设置为允许垃圾回收1.CacheBuilder.weakKeys():使用过弱引用存储键值。当被垃圾回收的时候，当前键值没有其他引用的时候缓存项可以被垃圾回收。2.CacheBuilder.weakValues():使用弱引用存储值。3.CacheBuilder.softValues():使用软引用存储值。软引用就是在内存不够是才会按照顺序回收。</code></pre><hr><h5 id="缓存数据的清除"><a href="#缓存数据的清除" class="headerlink" title="缓存数据的清除"></a>缓存数据的清除</h5><pre><code>个别清除：Cache.invalidate(key)批量清除：Cache.invalidateAll(keys)清除所有缓存项：Cache.invalidateAll()</code></pre><h5 id="刷新"><a href="#刷新" class="headerlink" title="刷新"></a>刷新</h5><blockquote><p>刷新操作进行时，缓存仍然可以向其他线程返回旧值，而不像回收操作，读缓存的线程必须等待新值加载完成</p></blockquote><blockquote><p>如果刷新过程抛出异常，缓存将保留旧值，而异常会在记录到日志后被丢弃[swallowed]。</p></blockquote><blockquote><p>重载CacheLoader.reload(K, V)可以扩展刷新时的行为，这个方法允许开发者在计算新值时使用旧的值。</p></blockquote><pre><code>//有些键不需要刷新，并且我们希望刷新是异步完成的LoadingCache&lt;Key, Graph&gt; graphs = CacheBuilder.newBuilder()        .maximumSize(1000)        .refreshAfterWrite(1, TimeUnit.MINUTES)        .build(            new CacheLoader&lt;Key, Graph&gt;() {                public Graph load(Key key) { // no checked exception                    return getGraphFromDatabase(key);                }                public ListenableFuture&lt;Key, Graph&gt; reload(final Key key, Graph prevGraph) {                    if (neverNeedsRefresh(key)) {                        return Futures.immediateFuture(prevGraph);                    }else{                        // asynchronous!                        ListenableFutureTask&lt;Key, Graph&gt; task=ListenableFutureTask.create(new Callable&lt;Key, Graph&gt;() {                            public Graph call() {                                return getGraphFromDatabase(key);                            }                        });                        executor.execute(task);                        return task;                    }                }            });</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java cache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>azkaban-剖析篇</title>
      <link href="/2019/12/10/azkaban-%E5%89%96%E6%9E%90%E7%AF%87/"/>
      <url>/2019/12/10/azkaban-%E5%89%96%E6%9E%90%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Azkaban-前后台参数传递"><a href="#1-Azkaban-前后台参数传递" class="headerlink" title="1.Azkaban-前后台参数传递"></a>1.Azkaban-前后台参数传递</h4><blockquote></blockquote><h4 id="2-Azkaban-用户登录过程"><a href="#2-Azkaban-用户登录过程" class="headerlink" title="2.Azkaban-用户登录过程"></a>2.Azkaban-用户登录过程</h4><blockquote><p>1.用户登录时，首先通过LoginAbstractAzkabanServlet 中的handleAjaxLoginAction 方法进行用户信息处理和认证。</p></blockquote><pre><code>protected void handleAjaxLoginAction(HttpServletRequest req,      HttpServletResponse resp, Map&lt;String, Object&gt; ret)      throws ServletException {    if (hasParam(req, &quot;username&quot;) &amp;&amp; hasParam(req, &quot;password&quot;)) {      Session session = null;      try {              //创建session 进行用户的认证        session = createSession(req);      } catch (UserManagerException e) {        ret.put(&quot;error&quot;, &quot;Incorrect Login. &quot; + e.getMessage());        return;      }      Cookie cookie = new Cookie(SESSION_ID_NAME, session.getSessionId());      cookie.setPath(&quot;/&quot;);      resp.addCookie(cookie);      getApplication().getSessionCache().addSession(session);      ret.put(&quot;status&quot;, &quot;success&quot;);      ret.put(&quot;session.id&quot;, session.getSessionId());    } else {      ret.put(&quot;error&quot;, &quot;Incorrect Login.&quot;);    }  }</code></pre><blockquote><p>2.createSession的验证和创建过程：</p></blockquote><pre><code>private Session createSession(String username, String password, String ip)     throws UserManagerException, ServletException {   UserManager manager = getApplication().getUserManager();   User user = manager.getUser(username, password);   String randomUID = UUID.randomUUID().toString();   Session session = new Session(randomUID, user, ip);   return session; }</code></pre>]]></content>
      
      
      <categories>
          
          <category> azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>httpd-常用篇</title>
      <link href="/2019/12/10/httpd-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/12/10/httpd-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="httpd-服务简介"><a href="#httpd-服务简介" class="headerlink" title="httpd 服务简介"></a>httpd 服务简介</h4><hr><h4 id="2-httpd-作为文件服务的使用"><a href="#2-httpd-作为文件服务的使用" class="headerlink" title="2.httpd 作为文件服务的使用"></a>2.httpd 作为文件服务的使用</h4><blockquote><p>安装使用</p></blockquote><pre><code>#安装sudo yum install httpdsudo su#测试配置是否正常httpd -t#starthttpd -k start#stophttpd -k stop</code></pre><blockquote><p>服务目录    /etc/httpd</p></blockquote><blockquote><p>主配置文件    /etc/httpd/conf/httpd.conf</p></blockquote><blockquote><p>网站数据目录    /var/www/html</p></blockquote><blockquote><p>访问日志    /var/log/httpd/access_log</p></blockquote><blockquote><p>错误日志    /var/log/httpd/error_log</p></blockquote><p><a href=".httpd.png">配置结构</a></p><table><thead><tr><th>ServerRoot</th><th>服务目录</th></tr></thead><tbody><tr><td>ServerAdmin</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><blockquote><p>对服务机地址目录启动成文件访问服务</p></blockquote><hr><h4 id="3-Httpd-集成ldap"><a href="#3-Httpd-集成ldap" class="headerlink" title="3.Httpd 集成ldap"></a>3.Httpd 集成ldap</h4><blockquote><p>集成前需安装 mod_ldap</p></blockquote><pre><code>sudo suyum -y install mod_ldap</code></pre><blockquote><p>vim conf.d/auth_ldap.conf</p></blockquote><pre><code>&lt;Directory /data/dataplatform/zepplin/http_server/file/&gt;# AuthName &quot;LDAP Authentication&quot;AuthName &quot;zeppelin_file&quot;AuthType BasicAuthBasicProvider ldapAuthLDAPURL &quot;ldap://*****:389/ou=acs,dc=****,dc=com?uid?sub?(objectClass=*)&quot;AuthLDAPBindDN &quot;uid=gateway,ou=open,dc=****,dc=com&quot;AuthLDAPBindPassword &quot;********&quot;Require valid-user&lt;/Directory&gt;</code></pre><blockquote><p>测试配置是否异常 httpd -t</p></blockquote><blockquote><p>重启 httpd -k restart</p></blockquote><h4 id="相关参考"><a href="#相关参考" class="headerlink" title="相关参考"></a>相关参考</h4><blockquote><p><a href="https://www.bookstack.cn/read/linuxprobe/5bb5cdfbbed75940.md" target="_blank" rel="noopener">配置参考地址</a></p></blockquote><blockquote><p><a href="https://httpd.apache.org/docs/2.4/" target="_blank" rel="noopener">Apache http 服务器2.4 文档</a></p></blockquote><blockquote><p><a href="https://httpd.apache.org/download.cgi" target="_blank" rel="noopener">download</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> httpd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> httpd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s-问题篇</title>
      <link href="/2019/11/12/k8s-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/11/12/k8s-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>####1. Unable to connect to the server: dial tcp 192.168.99.100:8443: connect: no route to host</p><blockquote><p>问题原因：<br>     链接不上服务地址，可能是机器没有启动，或者节点已经丢失。</p></blockquote><blockquote><p>解决方案：<br>    查看运行状态 minikube status<br>     启动机器（测试虚拟机）</p></blockquote><hr><p>####2. error: unable to forward port because pod is not running. Current status=Pending</p><blockquote><p>查看问题：<br>  kubectl get nodes 首先查看node 是不是Ready 状态<br>  kubectl get pods (查看当前的pods)<br>  kubectl describe nodes （查看node 的vm 详细信息）<br>  kubectl get services (查看运行服务)</p></blockquote><hr><p>####3. Kube-proxy: error looking for path of conntrack</p><blockquote><p>kube-proxy 报错，并且 service 的 DNS 解析异常</p></blockquote><pre><code> kube-proxy[2241]: E0502 15:55:13.889842    2241 conntrack.go:42]  conntrack returned error: error looking for path of conntrack: exec: &quot;conntrack&quot;: executable file not found in $PATH</code></pre><blockquote><p>解决方式是安装 conntrack-tools 包后重启 kube-proxy 即可。</p></blockquote><hr><p>####4. “Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”</p><blockquote><p>问题原因：是因为docker服务没有启动，所以在相应的/var/run/ 路径下找不到docker的进程。<br>解决方式：<br>   1.service docker start<br>   2.查看docker-machine是否安装。<br>  <a href="https://blog.csdn.net/Aaron_80726/article/details/83676014" target="_blank" rel="noopener">其他原因及解决方案</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s-常用篇</title>
      <link href="/2019/10/25/k8s-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/10/25/k8s-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="1-kubectl-操作"><a href="#1-kubectl-操作" class="headerlink" title="1. kubectl 操作"></a>1. kubectl 操作</h4><h5 id="1-1-kubectl-配置"><a href="#1-1-kubectl-配置" class="headerlink" title="1.1 kubectl 配置"></a>1.1 kubectl 配置</h5><h5 id="1-3-kubectl-权限"><a href="#1-3-kubectl-权限" class="headerlink" title="1.3 kubectl 权限"></a>1.3 kubectl 权限</h5><pre><code>**********************# 权限问题 #*************************查看是否有权限：kubectl auth can-i &lt;list|create|edit|delete&gt; pods</code></pre><h5 id="1-4-kubectl-常用命令"><a href="#1-4-kubectl-常用命令" class="headerlink" title="1.4 kubectl 常用命令"></a>1.4 kubectl 常用命令</h5><pre><code>//批量删除所有的podkubectl get pods | grep Evicted | awk &#39;{print $1}&#39; | xargs kubectl delete pod</code></pre><h4 id="2-minikube操作"><a href="#2-minikube操作" class="headerlink" title="2. minikube操作"></a>2. minikube操作</h4><h5 id="2-1-minikube-安装"><a href="#2-1-minikube-安装" class="headerlink" title="2.1 minikube 安装"></a>2.1 minikube 安装</h5><blockquote><p>1.下载virtualbox</p></blockquote><blockquote><p>2.brew cask install minikube</p></blockquote><blockquote><p>3.minikube start –vm-driver=virtualbox</p></blockquote><blockquote><p>4.minikube config set vm-driver virtualbox</p></blockquote><blockquote><p>5.kubectl version 查看版本</p></blockquote><h5 id="2-2-minikube-常用操作"><a href="#2-2-minikube-常用操作" class="headerlink" title="2.2 minikube 常用操作"></a>2.2 minikube 常用操作</h5><pre><code>#启动并创建集群minikube start#查看仪表盘minikube dashboard#使用现有镜像kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10#访问时将端口进行公开映射kubectl expose deployment hello-minikube --type=NodePort --port=8080#查看是否已经正在运行kubectl get pods#获取公开服务的URL以查看服务详细信息minikube service hello-minikube --url#curl 获取的url 查看本地集群的详细信息curl http://192.168.99.100:30083#删除 hello-minikube 服务kubectl delete services hello-minikube#删除 hello-minikube 部署kubectl delete deployment hello-minikube#停止本地minikube 集群minikube stop#删除本地minikube 集群minikube delete</code></pre><h5 id="2-3-minikube-的应用和服务"><a href="#2-3-minikube-的应用和服务" class="headerlink" title="2.3 minikube 的应用和服务"></a>2.3 minikube 的应用和服务</h5><pre><code>********************# 应用和服务 #*************#启动minikubeminikube start#部署应用kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080#确定deploymentkubectl get deployment#查看部署的应用kubectl get pods#发布应用kubectl expose deployment hello-minikube --type=NodePort#查看发布的内容kubectl get services#访问服务1.虚拟机：curl http://ip:port2.curl $(minikube service hello-minikube --url)#获取服务url 链接minikube service --url service_name#查看控制台minikube dashboard查看所有Kubernetes Pod的部署状态kubectl get po -A#删除服务kubectl delete services hello-minikube#删除应用kubectl delete deployment hello-minikube#删除podskubectl delete pods podName#停止minikubeminikube stop#进入minikube 控制台minikube ssh</code></pre><hr><h5 id="2-4-minikube-集群相关"><a href="#2-4-minikube-集群相关" class="headerlink" title="2.4 minikube 集群相关"></a>2.4 minikube 集群相关</h5><pre><code>**********************# 集群相关 #**************************#获取集群的ipminikube ip#获取集群节点kubectl get nodes#启动第二个本地集群minikube start -p cluster2#停止本地集群minikube stop#删除本地集群minikube delete#删除所有本地集群和配置文件minikube delete --all获取网桥ipminikube ssh &quot;route -n | grep ^0.0.0.0 | awk &#39;{ print \$2 }&#39;&quot;链接到集群minikubeminikube sshtelnet ip port</code></pre><hr><h5 id="2-5-minikube-附加组件"><a href="#2-5-minikube-附加组件" class="headerlink" title="2.5 minikube 附加组件"></a>2.5 minikube 附加组件</h5><pre><code>********************************* 附加组件 ********************#查询可添加的组件minikube addons list#启用组件minikube addons enable &lt;name&gt;#与组件交互minikube addons open &lt;name&gt;#禁用组件minikube addons disable &lt;name&gt;</code></pre><hr><h5 id="2-6-minikube-调试"><a href="#2-6-minikube-调试" class="headerlink" title="2.6 minikube 调试"></a>2.6 minikube 调试</h5><pre><code>************************** 启动调试日志 *******************--v=0将输出INFO级别的日志--v=1将输出警告级别的日志--v=2将输出错误级别的日志--v=3将输出libmachine日志记录--v=7将输出libmachine –debug级日志记录minikube start --v=7 将启动minikube并将所有重要的调试日志输出到stdout#收集虚拟机日志，要调试Kubernetes部署失败的问题，收集Kubernetes pod和内核日志非常有用minikube logs#立即查看启动失败minikube logs --problems#查看所有Kubernetes Pod的部署状态kubectl get po -A</code></pre><h5 id="2-7-重用Docker-守护程序使用本地映像"><a href="#2-7-重用Docker-守护程序使用本地映像" class="headerlink" title="2.7 重用Docker 守护程序使用本地映像"></a>2.7 重用Docker 守护程序使用本地映像</h5><pre><code>eval $(minikube docker-env)docker ps</code></pre><hr><h4 id="4-学习案例"><a href="#4-学习案例" class="headerlink" title="4.学习案例"></a>4.学习案例</h4><h5 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h5><hr><h4 id="5-kubectl应用和服务"><a href="#5-kubectl应用和服务" class="headerlink" title="5.kubectl应用和服务"></a>5.kubectl应用和服务</h4><hr><p>####6. </p><hr><h4 id="10-docker-常用操作"><a href="#10-docker-常用操作" class="headerlink" title="10.docker 常用操作"></a>10.docker 常用操作</h4><h5 id="10-1-镜像的操作"><a href="#10-1-镜像的操作" class="headerlink" title="10.1 镜像的操作"></a>10.1 镜像的操作</h5><pre><code>列出所有的镜像: docker images 停止运行：docker stop iamgesId删除单个镜像：docker rmi imagesId清理所有（慎用）：docker system pruneWARNING! This will remove:  - all stopped containers  - all networks not used by at least one container  - all dangling images  - all dangling build cache 清理镜像：docker image prune 清理容器：docker container prune 删除所有停止的镜像docker image prune -f -a 删除所有停止的容器：docker container prune -f复制文件：docker cp mycontainer:/opt/file.txt /opt/local/docker cp /opt/local/file.txt mycontainer:/opt/</code></pre><hr><h4 id="11-docker-常见问题及解决方案"><a href="#11-docker-常见问题及解决方案" class="headerlink" title="11.docker 常见问题及解决方案"></a>11.docker 常见问题及解决方案</h4><h5 id="11-1-docker日志太多导致磁盘占满"><a href="#11-1-docker日志太多导致磁盘占满" class="headerlink" title="11.1 docker日志太多导致磁盘占满"></a>11.1 docker日志太多导致磁盘占满</h5><blockquote><p>在启动时遇到：No space left on device 官方解决方案：<a href="https://success.docker.com/article/no-space-left-on-device-error" target="_blank" rel="noopener">地址</a></p></blockquote><pre><code>## 1.Sort the /var/lib/docker/containersdu -d1 -h /var/lib/docker/containers | sort -h## 2. 选择要清理的容器进行清理 cat /dev/null &gt;     /var/lib/docker/containers/********## 3.限制日志文件的大小：启动容器时，可以通过参数设置日志文件的大小、日志文件的格式 docker run -it --log-opt max-size=10m --log-opt max-file=3 alpine ash</code></pre><h4 id="12-kubectl常用故障排查以及修改命令"><a href="#12-kubectl常用故障排查以及修改命令" class="headerlink" title="12.kubectl常用故障排查以及修改命令"></a>12.kubectl常用故障排查以及修改命令</h4><blockquote><p>scale </p></blockquote><pre><code>scale命令进行横向扩展，将原本为1的副本，提高到3kubectl scale --current-replicas=1 --replicas=3 deployment/nginx</code></pre><blockquote><p>autoscale</p></blockquote><pre><code>和scale不同的是autoscale则会根据负载进行调解kubectl autoscale deployment nginx --min=2 --max=5</code></pre><blockquote><p>cordon</p></blockquote><pre><code>查询nodeAddresskubectl get pods -o wide设定nodeAddress，使得nodeAddress不可使用，使用get node确认，其状态显示SchedulingDisabledkubectl cordon nodeAddress案例：设定134不可用：kubectl cordon 192.168.32.134横向扩展：kubectl scale --replicas=6 deployment/nginx发现没有pods 再执行在134这台机器上。</code></pre><blockquote><p>kubectl uncordon</p></blockquote><pre><code>解除限制kubectl uncordon nodeAddress</code></pre><blockquote><p>kubectl drain </p></blockquote><pre><code>drain命令用于对某个node进行设定，是为了设定此node为维护做准备。此命令主要执行的操作是：1. 设定此node不可以使用（cordon)2. evict（回收）了其上的两个pod</code></pre><blockquote><p>kubectl api-versions</p></blockquote><pre><code>查看当前版本的kubernetes的服务器端所支持的api版本信息</code></pre><blockquote><p>kubectl get all -o wide</p></blockquote><pre><code>列出pod services deployment replicaset 的信息</code></pre><blockquote><p>kubectl 可get 的信息</p></blockquote><pre><code>kubectl get deploymentskubectl get podskubectl get namespaces</code></pre><blockquote><p>kubectl 查看详情信息</p></blockquote><pre><code>kubectl describe node 192.168.32.132kubectl describe deployment mysql</code></pre><blockquote><p>kubectl 查看日志</p></blockquote><pre><code>kubectl logs podsName</code></pre><h4 id="13-学习案例2"><a href="#13-学习案例2" class="headerlink" title="13.学习案例2"></a>13.学习案例2</h4><pre><code>[root@node1 wordpress]# cat wordpress-db.yaml---apiVersion: apps/v1beta1kind: Deploymentmetadata:name: mysql-deploylabels:app: mysqlspec:template:metadata:labels:app: mysqlspec:containers:- name: mysqlimage: mysql:5.7imagePullPolicy: IfNotPresentports:- containerPort: 3306name: dbportenv:- name: MYSQL_ROOT_PASSWORDvalue: rootPassW0rd- name: MYSQL_DATABASEvalue: wordpress- name: MYSQL_USERvalue: wordpress- name: MYSQL_PASSWORDvalue: wordpressvolumeMounts:- name: dbmountPath: /var/lib/mysqlvolumes:- name: dbhostPath:path: /var/lib/mysql---apiVersion: v1kind: Servicemetadata:name: mysqlspec:selector:app: mysqlports:- name: mysqlportprotocol: TCPport: 3306targetPort: dbport[root@node1 wordpress]# cat wordpress.yamlapiVersion: apps/v1beta1kind: Deploymentmetadata:name: wordpress-deploylabels:app: wordpressspec:template:metadata:labels:app: wordpressspec:containers:- name: wordpressimage: wordpressimagePullPolicy: IfNotPresentports:- containerPort: 80name: wdportenv:- name: WORDPRESS_DB_HOSTvalue: mysql:3306- name: WORDPRESS_DB_USERvalue: wordpress- name: WORDPRESS_DB_PASSWORDvalue: wordpress---apiVersion: v1kind: Servicemetadata:name: wordpressspec:type: NodePortselector:app: wordpressports:- name: wordpressportprotocol: TCPport: 80targetPort: wdport#### 启动容器kubectl create -f wordpress-db.yamlkubectl create -f wordpress.yaml#### 查看集群信息kubectl get all -A -l app=wordpressNAMESPACE NAME READY STATUS RESTARTS AGEdefault pod/wordpress-deploy-f9c5cf5c6-tj2bc 1/1 Running 0 80mNAMESPACE NAME READY UP-TO-DATE AVAILABLE AGEdefault deployment.apps/wordpress-deploy 1/1 1 1 80mNAMESPACE NAME DESIRED CURRENT READY AGEdefault replicaset.apps/wordpress-deploy-f9c5cf5c6 1 1 1 80m#### 配置ingress[root@node1 wordpress]# cat ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata:name: wordpress-ingressnamespace: defaultannotations:kubernetes.io/ingress.class: &quot;nginx&quot;spec:rules:- host: wordpress.boshao.wanghttp:paths:- backend:serviceName: wordpressservicePort: 80#### 创建ingresskubectl create -f ingress.yaml#### 查看ingres信息[root@node1 wordpress]# kubectl get ing wordpress-ingressNAME HOSTS ADDRESS PORTS AGEwordpress-ingress wordpress.boshao.wang 80 78m[root@node1 wordpress]# kubectl describe ingress wordpress-ingressName: wordpress-ingressNamespace: defaultAddress:Default backend: default-http-backend:80 (&lt;none&gt;)Rules:Host Path Backends---- ---- --------wordpress.boshao.wangwordpress:80 (10.233.70.27:80)Annotations:kubernetes.io/ingress.class: nginxEvents: &lt;none&gt;#### 最后绑定域名wordpress.boshao.wang  到node节点即可。通过ingress-nginx 暴露的端口进行访问，即可。ingress-nginx service/ingress-nginx NodePort 10.233.29.94 &lt;none&gt; 80:31661/TCP,443:30250/TCP 6d2h访问方式：wordpress.boshao.wang → node ip:31661 http://wordpress.boshao.wang:31661/</code></pre><hr><h4 id="参考地址："><a href="#参考地址：" class="headerlink" title="参考地址："></a>参考地址：</h4><blockquote><p><a href="https://minikube.sigs.k8s.io/" target="_blank" rel="noopener">minikube参考</a></p></blockquote><blockquote><p><a href="https://k8smeetup.github.io/docs/user-guide/kubectl/v1.7/#-strong-getting-started-strong-" target="_blank" rel="noopener">命令查询</a> </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-常用篇</title>
      <link href="/2019/10/24/python-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/10/24/python-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="1-pyenv-的使用"><a href="#1-pyenv-的使用" class="headerlink" title="1.pyenv 的使用"></a>1.pyenv 的使用</h4><pre><code> #查看python 版本信息   pyenv versions #python 切换版本   pyenv local 版本号</code></pre><blockquote><p><a href="https://github.com/eteplus/blog/issues/4" target="_blank" rel="noopener">Mac下pyenv与pyenv-virtualenv的安装和使用</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s-学习篇</title>
      <link href="/2019/10/15/k8s-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/10/15/k8s-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="k8s-介绍"><a href="#k8s-介绍" class="headerlink" title="k8s 介绍"></a>k8s 介绍</h4><p>Kubernetes 是一个生产级的开源平台，用于协调计算机集群内部和跨计算机集群的应用程序容器的分发(调度)和运行。<br>一个 Master 是集群的调度节点。<br>nodes 是应用程序实际运行的工作节点。</p><h4 id="k8s-重要组件"><a href="#k8s-重要组件" class="headerlink" title="k8s 重要组件"></a>k8s 重要组件</h4><p>k8s核心组件：</p><blockquote><p>etcd保存了整个集群的状态；  </p></blockquote><blockquote><p>apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；  </p></blockquote><blockquote><p>controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；  </p></blockquote><blockquote><p>scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；  </p></blockquote><blockquote><p>kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；  </p></blockquote><blockquote><p>Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；  </p></blockquote><blockquote><p>kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；  </p></blockquote><p>推荐的Add-ons：</p><blockquote><p>kube-dns负责为整个集群提供DNS服务</p></blockquote><blockquote><p>Ingress Controller为服务提供外网入口</p></blockquote><blockquote><p>Heapster提供资源监控</p></blockquote><blockquote><p>Dashboard提供GUI</p></blockquote><blockquote><p>Federation提供跨可用区的集群</p></blockquote><blockquote><p>Fluentd-elasticsearch提供集群日志采集、存储与查询</p></blockquote><h4 id="k8s-部署"><a href="#k8s-部署" class="headerlink" title="k8s 部署"></a>k8s 部署</h4><p>在k8s中，通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小单元的可管理单元</p><p>在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。</p><p>创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。</p><p>在容器编排之前的时代，各种安装脚本通常用于启动应用程序，但是不能够使应用程序从机器故障中恢复。通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></p><p>nginx官方介绍：<a href="https://www.nginx.com/products/nginx/kubernetes-ingress-controller" target="_blank" rel="noopener">https://www.nginx.com/products/nginx/kubernetes-ingress-controller</a></p><h4 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h4><p><a href="https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command" target="_blank" rel="noopener">https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command</a></p><p>首先先下载相关的yaml文件，保存到本地。</p><p>deployments：<br>kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml" target="_blank" rel="noopener">https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</a></p><p>service： 这里官方提供了各种云平台，系统等相关配置。我们这里是自建的k8s集群，所以我们选择裸机版本。</p><p>Bare-metal<br>Using NodePort:</p><p>kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml" target="_blank" rel="noopener">https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml</a></p><p>安装完后。看看集群信息。</p><p>[root@node1 ingress-nginx]# kubectl get all -A -l app.kubernetes.io/name=ingress-nginx<br>NAMESPACE NAME READY STATUS RESTARTS AGE<br>ingress-nginx pod/nginx-ingress-controller-79f6884cf6-vh2w2 1/1 Running 0 5d3h</p><p>NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE<br>ingress-nginx service/ingress-nginx NodePort 10.233.29.94 <none> 80:31661/TCP,443:30250/TCP 6d2h</none></p><p>NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE<br>ingress-nginx deployment.apps/nginx-ingress-controller 1/1 1 1 6d3h</p><p>NAMESPACE NAME DESIRED CURRENT READY AGE<br>ingress-nginx replicaset.apps/nginx-ingress-controller-79f6884cf6 1 1 1 6d3h</p>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql-常用篇</title>
      <link href="/2019/10/15/sql-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/10/15/sql-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="编程规范"><a href="#编程规范" class="headerlink" title="编程规范"></a>编程规范</h4><p><a href="https://www.sqlstyle.guide/zh/" target="_blank" rel="noopener">https://www.sqlstyle.guide/zh/</a></p>]]></content>
      
      
      <categories>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell-案例篇</title>
      <link href="/2019/10/15/shell-%E6%A1%88%E4%BE%8B%E7%AF%87/"/>
      <url>/2019/10/15/shell-%E6%A1%88%E4%BE%8B%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<pre><code>## 此shell脚本只在bash解释器下测试运行过, sh解释器不保证一定正确## 关于bash和sh有啥区别, 大家可自行Google, 我建议大家使用bash## 可能大部分Linux系统, bash跟sh不会有太多区别, 但是如果你用Debian或者Ubuntu的话## 你会发现之前写的脚本sh运行会爆出一堆语法错误, 因为他们的sh等于Dash, 很蛋疼.###################PRODUCTLINE=&quot;rta&quot;PROJECT=&quot;ga-device&quot;## 自带出错邮件报警: 尽量使用邮件组而不是使用具体某个人的邮箱## 因为科比曾经说过: 朋友来来走走, 冠军旗帜永不倒## 使用具体某个人的邮箱回头离职了邮件就发不出来了ALARM_ADDR=&quot;bin.li@mobvista.com&quot;ALARM_CC=&quot;bin.li@mobvista.com&quot;## 钉钉报警有个好处是你可以建立群组, 而且可以分类, 不重要的信息你## 可以发到一个群里, 然后大家屏蔽群消息就好, 出问题的时候再去翻## dingtalkid: 钉钉群讨论的ID, 我想不需要我教你怎么获得这个IDALARM_DINGDING_TALKID=&quot;62138ba2d704500d24b21064663659e2288bdf5f6b72d8a8d5895746f2ee0f8f&quot;# https://oapi.dingtalk.com/robot/send?access_token=62138ba2d704500d24b21064663659e2288bdf5f6b72d8a8d5895746f2ee0f8fINFO_DINGDING_TALKID=&quot;d49490482e23f0780bf017ad04cb29d027755e0550578ff4324e48f989e7a4c1&quot;# https://oapi.dingtalk.com/robot/send?access_token=d49490482e23f0780bf017ad04cb29d027755e0550578ff4324e48f989e7a4c1## kibana使用的EleasticSearch的地址, 数据上报到ES, 才能用kibana展示出来## 尽量用公司统一的, 不要自己单独整## 如果觉得别人搞的不好用, 尽量推动别人优化## 而不是自己搭一个扔在那里没人管ES_ADDR=&quot;bj-report-ELB20151027-2124151593.us-east-1.elb.amazonaws.com:80&quot;ES_USER=&quot;mob_report&quot;ES_PASSWD=&quot;Mobvista_256&quot;## 顾名思义, 就是把一个JSON document推到ES## 这里面有一些概念: docid, doc, index, type(如果不知道啥意思, 自行google)## 参数:##   + docid: doc唯一的ID, 很多人喜欢让ES自己生成, 但是这个地方尽量不要,##     因为将来你的任务失败你还会补数呢, 找一个唯一的ID, 或者拼一个出来,##     将来补数原来的doc会被自动覆盖掉, 不至于数据重复##   + doc: json doc##     任何你想塞进去的信息, 如果你足够聪明, 请塞一个格式化好的时间(北京时区)和##     一个时间戳进去function doc2es() {    local __docid=&quot;$1&quot;    local __doc=&quot;$2&quot;    curl -XPUT &quot;$ES_ADDR/mp_rba/cap_updater_jobstatus/$__docid&quot; \         -H &#39;Content-Type: application/json&#39; \         -u &quot;$ES_USER:$ES_PASSWD&quot; -d&quot;$__doc&quot;}## 邮件报警, 会使用全局变量里面的收件人地址 ALARM_ADDR &amp; ALARM_CC## 两个参数:##   + sub: 邮件主题##   + body: 报警内容(别JB图省事, 多写点东西又不会死, 让别人一眼看出来发生了什么吧!)function email_alarm() {    local _sub=$1    local _body=&quot;$2&quot;    cat &quot;$_body&quot; | mail -s &quot;[$PRODUCTLINE][$PROJECT]$_sub&quot; -c &quot;$ALARM_CC&quot; &quot;$ALARM_ADDR&quot;}function send_dingding_msg() {    local _dingtalkid=&quot;$1&quot;    local _msg=&quot;$2&quot;    url=&quot;https://oapi.dingtalk.com/robot/send?access_token=$_dingtalkid&quot;    body=&quot;{         \&quot;msgtype\&quot;: \&quot;text\&quot;,         \&quot;text\&quot;: {             \&quot;content\&quot;: \&quot;[$PRODUCTLINE][$PROJECT] Failed !!! [$_msg]\&quot;         },        \&quot;at\&quot;: {             \&quot;atMobiles\&quot;: [],             \&quot;isAtAll\&quot;: false         }    }&quot;    curl &quot;$url&quot;  -H &#39;Content-Type: application/json&#39; -X POST -d &quot;$body&quot;}## 钉钉报警, 会使用全局变量里面的收件人地址 ALARM_ADDR &amp; ALARM_CC## 参数:##   + msg: 报警内容(别JB图省事, 多写点东西又不会死, 让别人一眼看出来发生了什么吧!)function dingding_alarm() {    send_dingding_msg $ALARM_DINGDING_TALKID &quot;$1&quot;}function dingding_info() {    send_dingding_msg $INFO_DINGDING_TALKID &quot;$1&quot;}## 就是想让输出多个时间而已## 参数: 你想打的消息function info() {    local _msg=&quot;$1&quot;    echo &quot;$(date +&quot;%Y-%m-%d %H:%M:%S&quot;) INFO [$_msg]&quot;}## 就是想让输出多个时间而已## 参数: 你想打的消息function error() {    local _msg=&quot;$1&quot;    echo &quot;$(date +&quot;%Y-%m-%d %H:%M:%S&quot;) ERROR [$_msg]&quot;}## 任务运行结束, 清理掉临时文件## 参数: 给个文件名function clean_file() {    local _f=&quot;$1&quot;    if [ -f $_f ]; then        info &quot;rm $_f&quot;        rm $_f    fi}## 任务运行结束, 备份重要结果文件## 参数: 给个文件名和后缀## 比如: back_file file-name 2019032013function back_file() {    local _f=&quot;$1&quot;    local _postfix=&quot;$2&quot;    if [ -f $_f ]; then        info &quot;mv $_f $_f.$_postfix&quot;        mv $_f $_f.$_postfix    fi}</code></pre><pre><code># 为了方便CTI和CVR的计算, 我们需要从s3拉取一些中间结果下来# 这个地方定义一个下载函数# 希望达到的效果是, 给定一个输入路径, 可以把这个路径下的所有文件都下载下来# 并且导到一个目标文件中# 当然还需要做一下文件大小的check, 别把本地磁盘给撑爆了function download_from_s3_dir() {    local _s3_dir=$1    local _local_file=$2    local _tmp_index=&quot;__tmp_index_for_download_s3_file__$(date +&quot;%s&quot;).$RANDOM&quot;    local _sz_max=4000000000    # local _sz_max=1000    # check file size 1st    if [ -f $_tmp_index ]; then        rm $_tmp_index    fi    run_with_check &quot;aws s3 ls $_s3_dir &gt; $_tmp_index&quot;    if [ 0 -eq $(wc -l $_tmp_index | awk &#39;{print $1}&#39;) ]; then        echo &quot;WARNING: no file in $_s3_dir&quot;        touch $_local_file    else        local _sz_tt=0        for sz in $(cat $_tmp_index | awk &#39;{print $3}&#39;)        do            echo $sz  # debug            _sz_tt=$((_sz_tt+$sz))        done        if [ $_sz_tt -gt $_sz_max ]; then            echo &quot;ERROR: files too large exit!!! [$_sz_tt &gt; $_sz_max]&quot;            exit 1        fi        if [ -f $_local_file ]; then            rm $_local_file        fi        for f in $(cat $_tmp_index | awk &#39;{print $3&quot;|&quot;$4}&#39;)        do            local _sz=$(echo &quot;$f&quot; | awk -F&#39;|&#39; &#39;{print $1}&#39;)            local _ff=$(echo &quot;$f&quot; | awk -F&#39;|&#39; &#39;{print $2}&#39;)            local _ff_local=$_ff&quot;.$(date +&quot;%s&quot;).$RANDOM&quot;            if [ x&quot;0&quot; = x&quot;$_sz&quot; ]; then                continue            fi            run_with_check &quot;aws s3 cp $_s3_dir$_ff $_ff_local&quot;            run_with_check &quot;cat $_ff_local &gt;&gt; $_local_file&quot;            if [ -f $_ff_local ]; then                rm $_ff_local            fi        done    fi    if [ -f $_tmp_index ]; then        rm $_tmp_index    fi}</code></pre><pre><code>## 这个函数我想大家都可以用到## 把你任务一些可能会失败的操作过程, 比如从s3拷个文件## 比如执行一个py脚本, 比如curl一个东西## run_with_check, 发现失败, 自动报警, 而且立马终止任务## 参数: 你要执行的命令## 比如: run_with_check &quot;python demo.py&quot;function run_with_check {    cmd=&quot;$1&quot;    info &quot;$cmd&quot;    eval &quot;$cmd&quot;    if [ 0 -ne $? ]; then        error &quot;[$cmd] failed!!!!! exit !!!!!!!!&quot;        email_alarm &quot;[$PRODUCTLINE][$PROJECT] Failed !!!&quot; &quot;command exit code not 0: $cmd&quot;        dingding_alarm &quot;command exit code not 0: $cmd&quot;        exit 1    fi}</code></pre><h4 id="微信报警接口"><a href="#微信报警接口" class="headerlink" title="微信报警接口"></a>微信报警接口</h4><pre><code>#!/usr/bin/env python# coding:utf-8import sysimport urllib2import timeimport jsonimport requestsimport redisreload(sys)sys.setdefaultencoding(&#39;utf-8&#39;)print sys.argvmessage = sys.argv[3]   # 位置参数获取title 适用于zabbixuser = sys.argv[1] # 位置参数获取content 适用于zabbixPOOL = redis.ConnectionPool(host=&#39;&#39;,port=3721,password=&#39;&#39;,db=1)rs=redis.Redis(connection_pool=POOL)def send_msg(user,message):    # 发送消息    qs_token = rs.get(&#39;weixin&#39;)    url = &quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={0}&quot;.format(        qs_token)    payload = {        &quot;touser&quot;:user,        &quot;msgtype&quot;: &quot;text&quot;,        &quot;agentid&quot;: &quot;2&quot;,        &quot;text&quot;: {                   &quot;content&quot;: message        },        &quot;safe&quot;: &quot;0&quot;    }    ret = requests.post(url, data=json.dumps(payload, ensure_ascii=False))    print ret.json()if __name__ == &#39;__main__&#39;:    send_msg(user,message)# 使用方法：python  wetchat.py mail@qq.com subject message</code></pre>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell-案例篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell-常用篇</title>
      <link href="/2019/10/15/shell-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/10/15/shell-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="shell-编程"><a href="#shell-编程" class="headerlink" title="shell 编程"></a>shell 编程</h4><h5 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h5><pre><code>=    两个字符串相等返回true!=    两个字符串不相等返回true-z    字符串长度为0返回true-n    字符串长度不为0返回true-d file    检测文件是否是目录，如果是，则返回 true-r file    检测文件是否可读，如果是，则返回 true-w file    检测文件是否可写，如果是，则返回 true-x file    检测文件是否可执行，如果是，则返回 true-s file    检测文件是否为空（文件大小是否大于0，不为空返回 true-e file    检测文件（包括目录）是否存在，如果是，则返回 true</code></pre><hr><h5 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h5><pre><code>#!/bin/bash#定义字符串mtext=&quot;hello&quot;  mtext2=&quot;world&quot;#字符串的拼接mtext3=$mtext&quot; &quot;$mtext2  #输出字符串echo $mtext3 #输出字符串长度echo ${#mtext3}  #截取字符串echo ${mtext3:1:4}  </code></pre><hr><h5 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h5><pre><code>#!/bin/bash#定义数组array=(1 2 3 4 5)  array2=(aa bb cc dd ee)  #找到某一个下标的数，然后赋值value=${array[3]}  echo $value  #找到某一个下标的数，然后赋值value2=${array2[3]} echo $value2  #获取数组长度length=${#array[*]} echo $length</code></pre><hr><h5 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h5><pre><code> #输出并且换行echo &quot;hello \nworld&quot; #重定向到文件echo &quot;hello world&quot; &gt; a.txt  #输出当前系统时间echo `date`  </code></pre><hr><h5 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h5><pre><code>#!/bin/sh#测试各种字符串比较操作。#shell中对变量的值添加单引号，爽引号和不添加的区别：对类型来说是无关的，即不是添加了引号就变成了字符串类型，#单引号不对相关量进行替换，如不对$符号解释成变量引用，从而用对应变量的值替代，双引号则会进行替代#author:tenfyguoA=&quot;$1&quot;B=&quot;$2&quot;echo &quot;输入的原始值：A=$A,B=$B&quot;#判断字符串是否相等if [ &quot;$A&quot; = &quot;$B&quot; ];thenecho &quot;[ = ]&quot;fi#判断字符串是否相等，与上面的=等价if [ &quot;$A&quot; == &quot;$B&quot; ];thenecho &quot;[ == ]&quot;fi#注意:==的功能在[[]]和[]中的行为是不同的，如下#如果$a以”a”开头(模式匹配)那么将为true if [[ &quot;$A&quot; == a* ]];thenecho &quot;[[ ==a* ]]&quot;fi#如果$a等于a*(字符匹配),那么结果为trueif [[ &quot;$A&quot; == &quot;a*&quot; ]];thenecho &quot;==/&quot;a*/&quot;&quot;fi  #File globbing(通配) 和word splitting将会发生, 此时的a*会自动匹配到对应的当前以a开头的文件#如在当前的目录中有个文件：add_crontab.sh,则下面会输出ok#if [ &quot;add_crontab.sh&quot; == a* ];then #echo &quot;ok&quot;#fiif [ &quot;$A&quot; == a* ];thenecho &quot;[ ==a* ]&quot;fi#如果$a等于a*(字符匹配),那么结果为trueif [ &quot;$A&quot; == &quot;a*&quot; ];thenecho &quot;==/&quot;a*/&quot;&quot;fi#字符串不相等if [ &quot;$A&quot; != &quot;$B&quot; ];thenecho &quot;[ != ]&quot;fi#字符串不相等if [[ &quot;$A&quot; != &quot;$B&quot; ]];thenecho &quot;[[ != ]]&quot;fi#字符串不为空，长度不为0if [ -n &quot;$A&quot; ];thenecho &quot;[ -n ]&quot;fi#字符串为空.就是长度为0.if [ -z &quot;$A&quot; ];thenecho &quot;[ -z ]&quot;fi#需要转义&lt;，否则认为是一个重定向符号if [ $A /&lt; $B ];thenecho &quot;[ &lt; ]&quot;  fiif [[ $A &lt; $B ]];thenecho &quot;[[ &lt; ]]&quot;  fi#需要转义&gt;，否则认为是一个重定向符号if [ $A /&gt; $B ];thenecho &quot;[ &gt; ]&quot;  fiif [[ $A &gt; $B ]];thenecho &quot;[[ &gt; ]]&quot;  fi</code></pre><pre><code>注意：1.if 和 [ ] 之间要有空格2.[ ] 和“ ”之间要有空格3.“ ”和 = 之间要有空格</code></pre><hr><h5 id="test-查看文件是否存在"><a href="#test-查看文件是否存在" class="headerlink" title="test 查看文件是否存在"></a>test 查看文件是否存在</h5><p><a href="https://blog.csdn.net/qq_34337272/article/details/85640050" target="_blank" rel="noopener">https://blog.csdn.net/qq_34337272/article/details/85640050</a></p><pre><code>test $[num1] -eq $[num2]  #判断两个变量是否相等test num1=num2  #判断两个数字是否相等-e file    文件存在则返回真-r file    文件存在并且可读则返回真-w file    文件存在并且可写则返回真-x file    文件存在并且可执行则返回真-s file    文件存在并且内容不为空则返回真-d file    文件目录存在则返回真</code></pre><h5 id="case…-esac"><a href="#case…-esac" class="headerlink" title="case….esac"></a>case….esac</h5><pre><code>case 值 in模式1)    command1    command2    command3    ;;模式2）    command1    command2    command3    ;;*)    command1    command2    command3    ;;esac# 匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。# ;; 与其他语言中的 break 类似，意思是不执行接下来的语句而是跳到整个 case 语句的最后。# *)与default相似，如果上面没有匹配到的模式，则执行*)里的内容。模式支持正则表达式:*       任意字串?       任意字元[abc]   a, b, 或c三字元其中之一[a-n]   从a到n的任一字元|       多重选择举例：#!/bin/sh case $1 instart | begin)    echo &quot;I am started!&quot;      ;;stop | end)    echo &quot;I am stopped!&quot;      ;;*)    echo &quot;Other command!&quot;      ;;esac</code></pre><hr><h5 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h5><pre><code>#!/bin/bashfor i in {1..5}do   echo $idonefor i in 5 6 7 8 9 do   echo $idonefor FILE in $HOME/.bash* do   echo $FILEdone</code></pre><hr><h5 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h5><pre><code>#!/bin/bashCOUNTER=0while [ $COUNTER lt 5 ]do    COUNTER=`expr $COUNTER + 1`        echo $COUNTERdoneecho &#39;请输入。。。&#39;echo &#39;ctrl + d 即可停止该程序&#39;while read FILM do    echo &quot;Yeah! great film the $FILM&quot;done</code></pre><hr><h4 id="Shell-脚本执行返回状态码："><a href="#Shell-脚本执行返回状态码：" class="headerlink" title="Shell 脚本执行返回状态码："></a>Shell 脚本执行返回状态码：</h4><p>状态码</p><table><thead><tr><th>状态码</th><th>含义</th></tr></thead><tbody><tr><td>0</td><td>命令成功完成</td></tr><tr><td>1</td><td>通常的未知错误</td></tr><tr><td>2</td><td>误用shell命令</td></tr><tr><td>126</td><td>命令无法执行</td></tr><tr><td>127</td><td>没有找到命令</td></tr><tr><td>128</td><td>无效的退出参数</td></tr><tr><td>128+x</td><td>使用Linux信号x的致命错误。</td></tr><tr><td>130</td><td>使用Ctrl-C终止的命令</td></tr><tr><td>255</td><td>规范外的退出状态</td></tr></tbody></table><h4 id="Shell-特殊变量列表"><a href="#Shell-特殊变量列表" class="headerlink" title="Shell 特殊变量列表"></a>Shell 特殊变量列表</h4><pre><code>特殊变量列表变量    含义$0    当前脚本的文件名$n    传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2。$#    传递给脚本或函数的参数个数。$*    传递给脚本或函数的所有参数。$@    传递给脚本或函数的所有参数。被双引号(“ “)包含时，与 $* 稍有不同，下面将会讲到。$?    上个命令的退出状态，或函数的返回值。$$    当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。</code></pre><h5 id="变量注意事项"><a href="#变量注意事项" class="headerlink" title="变量注意事项"></a>变量注意事项</h5><pre><code>变量名=变量值等号“=”前后不可以有空格变量名不可以直接和其他字符相连，如果想相连，必须用括号：echo “this is $(he)llo!”</code></pre>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go-常用篇</title>
      <link href="/2019/10/15/go-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/10/15/go-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="编码规范"><a href="#编码规范" class="headerlink" title="编码规范"></a>编码规范</h4><p><a href="http://docscn.studygolang.com/doc/effective_go.html" target="_blank" rel="noopener">http://docscn.studygolang.com/doc/effective_go.html</a></p><h4 id="单测样例"><a href="#单测样例" class="headerlink" title="单测样例"></a>单测样例</h4><p><a href="http://gitlab.mobvista.com/mtech/mtech/blob/master/开发-Go单元测试.md" target="_blank" rel="noopener">http://gitlab.mobvista.com/mtech/mtech/blob/master/开发-Go单元测试.md</a></p><h4 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h4><p>开发可自由选择编辑器，推荐使用IDE：<a href="https://www.jetbrains.com/go/" target="_blank" rel="noopener">https://www.jetbrains.com/go/</a></p><h4 id="常用库"><a href="#常用库" class="headerlink" title="常用库"></a>常用库</h4><p>标准库: <a href="http://docscn.studygolang.com/pkg/" target="_blank" rel="noopener">http://docscn.studygolang.com/pkg/</a><br>日志: <a href="https://github.com/cihub/seelog" target="_blank" rel="noopener">https://github.com/cihub/seelog</a><br>配置: <a href="https://github.com/spf13/viper" target="_blank" rel="noopener">https://github.com/spf13/viper</a><br>uuid: <a href="https://github.com/satori/go.uuid" target="_blank" rel="noopener">https://github.com/satori/go.uuid</a><br>leveldb: <a href="https://github.com/syndtr/goleveldb/leveldb" target="_blank" rel="noopener">https://github.com/syndtr/goleveldb/leveldb</a><br>murmurhash3: <a href="https://github.com/spaolacci/murmur3" target="_blank" rel="noopener">https://github.com/spaolacci/murmur3</a><br>redis: <a href="https://github.com/garyburd/redigo/redis" target="_blank" rel="noopener">https://github.com/garyburd/redigo/redis</a><br>redis-cluster: <a href="https://github.com/chasex/redis-go-cluster" target="_blank" rel="noopener">https://github.com/chasex/redis-go-cluster</a><br>mongo: <a href="http://gopkg.in/mgo.v2" target="_blank" rel="noopener">http://gopkg.in/mgo.v2</a><br>命令行: <a href="http://gopkg.in/alecthomas/kingpin.v2" target="_blank" rel="noopener">http://gopkg.in/alecthomas/kingpin.v2</a><br>grpc: <a href="http://www.grpc.io/docs/quickstart/go.html" target="_blank" rel="noopener">http://www.grpc.io/docs/quickstart/go.html</a></p>]]></content>
      
      
      <categories>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git-常用篇</title>
      <link href="/2019/10/14/git-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/10/14/git-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>git config –global user.name “你的名字” 让你全部的Git仓库绑定你的名字</p></blockquote><blockquote><p>git config –global user.email “你的邮箱” 让你全部的Git仓库绑定你的邮箱</p></blockquote><blockquote><p>git init 初始化你的仓库</p></blockquote><blockquote><p>git add . 把工作区的文件全部提交到暂存区</p></blockquote><blockquote><p>git add ./<file>/ 把工作区的<file>文件提交到暂存区</file></file></p></blockquote><blockquote><p>git commit -m “xxx” 把暂存区的所有文件提交到仓库区，暂存区空空荡荡</p></blockquote><blockquote><p>git remote add origin <a href="https://github.com/name/name_cangku.git" target="_blank" rel="noopener">https://github.com/name/name_cangku.git</a> 把本地仓库与远程仓库连接起来</p></blockquote><blockquote><p>git push -u origin master 把仓库区的主分支master提交到远程仓库里</p></blockquote><blockquote><p>git push -u origin &lt;其他分支&gt; 把其他分支提交到远程仓库</p></blockquote><blockquote><p>git status查看当前仓库的状态</p></blockquote><blockquote><p>git diff 查看文件修改的具体内容</p></blockquote><blockquote><p>git log 显示从最近到最远的提交历史</p></blockquote><blockquote><p>git clone + 仓库地址下载克隆文件</p></blockquote><blockquote><p>git reset –hard + 版本号 回溯版本，版本号在commit的时候与master跟随在一起</p></blockquote><blockquote><p>git reflog 显示命令历史</p></blockquote><blockquote><p>git checkout – <file> 撤销命令，用版本库里的文件替换掉工作区的文件。我觉得就像是Git世界的ctrl + z</file></p></blockquote><blockquote><p>git rm 删除版本库的文件</p></blockquote><blockquote><p>git branch 查看当前所有分支</p></blockquote><blockquote><p>git branch &lt;分支名字&gt; 创建分支</p></blockquote><blockquote><p>git checkout &lt;分支名字&gt; 切换到分支</p></blockquote><blockquote><p>git merge &lt;分支名字&gt; 合并分支</p></blockquote><blockquote><p>git branch -d &lt;分支名字&gt; 删除分支,有可能会删除失败，因为Git会保护没有被合并的分支</p></blockquote><blockquote><p>git branch -D + &lt;分支名字&gt; 强行删除，丢弃没被合并的分支</p></blockquote><blockquote><p>git log –graph 查看分支合并图</p></blockquote><blockquote><p>git merge –no-ff &lt;分支名字&gt; 合并分支的时候禁用Fast forward模式,因为这个模式会丢失分支历史信息</p></blockquote><blockquote><p>git stash 当有其他任务插进来时，把当前工作现场“存储”起来,以后恢复后继续工作</p></blockquote><blockquote><p>git stash list 查看你刚刚“存放”起来的工作去哪里了</p></blockquote><blockquote><p>git stash apply 恢复却不删除stash内容</p></blockquote><blockquote><p>git stash drop 删除stash内容</p></blockquote><blockquote><p>git stash pop 恢复的同时把stash内容也删了</p></blockquote><blockquote><p>git remote 查看远程库的信息，会显示origin，远程仓库默认名称为origin</p></blockquote><blockquote><p>git remote -v 显示更详细的信息</p></blockquote><blockquote><p>git pull 把最新的提交从远程仓库中抓取下来，在本地合并,和git push相反</p></blockquote><blockquote><p>git rebase 把分叉的提交历史“整理”成一条直线，看上去更直观</p></blockquote><blockquote><p>git tag 查看所有标签，可以知道历史版本的tag</p></blockquote><blockquote><p>git tag <name> 打标签，默认为HEAD。比如git tag v1.0</name></p></blockquote><blockquote><p>git tag <tagname> &lt;版本号&gt; 把版本号打上标签，版本号就是commit时，跟在旁边的一串字母数字</tagname></p></blockquote><blockquote><p>git show <tagname> 查看标签信息</tagname></p></blockquote><blockquote><p>git tag -a <tagname> -m “&lt;说明&gt;” 创建带说明的标签。-a指定标签名，-m指定说明文字</tagname></p></blockquote><blockquote><p>git tag -d <tagname> 删除标签</tagname></p></blockquote><blockquote><p>git push origin <tagname> 推送某个标签到远程</tagname></p></blockquote><blockquote><p>git push origin –tags 一次性推送全部尚未推送到远程的本地标签</p></blockquote><blockquote><p>git push origin :refs/tags/<tagname> 删除远程标签<tagname></tagname></tagname></p></blockquote><blockquote><p>git config –global color.ui true 让Git显示颜色，会让命令输出看起来更醒目</p></blockquote><blockquote><p>git add -f <file> 强制提交已忽略的的文件</file></p></blockquote><blockquote><p>git check-ignore -v <file> 检查为什么Git会忽略该文件</file></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>elasticsearch-学习篇</title>
      <link href="/2019/10/11/elasticsearch-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/10/11/elasticsearch-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>介绍：<br>一个分布式的实时文档存储，每个字段 可以被索引与搜索<br>一个分布式实时分析搜索引擎<br>能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据</p>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>职业知识图谱</title>
      <link href="/2019/10/10/%E8%81%8C%E4%B8%9A%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
      <url>/2019/10/10/%E8%81%8C%E4%B8%9A%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</url>
      
        <content type="html"><![CDATA[<h4 id="大数据通用处理平台"><a href="#大数据通用处理平台" class="headerlink" title="大数据通用处理平台"></a>大数据通用处理平台</h4><p>Spark<br>Flink<br>Hadoop</p><h4 id="分布式存储"><a href="#分布式存储" class="headerlink" title="分布式存储"></a>分布式存储</h4><p>HDFS</p><h4 id="资源调度"><a href="#资源调度" class="headerlink" title="资源调度"></a>资源调度</h4><p>1、Yarn<br>2、Mesos</p><h4 id="机器学习工具"><a href="#机器学习工具" class="headerlink" title="机器学习工具"></a>机器学习工具</h4><p>Mahout</p><p>Spark Mlib</p><p>TensorFlow (Google 系)</p><p>Amazon Machine Learning</p><p>DMTK (微软分布式机器学习工具)</p><h4 id="数据分析-数据仓库-SQL类"><a href="#数据分析-数据仓库-SQL类" class="headerlink" title="数据分析/数据仓库(SQL类)"></a>数据分析/数据仓库(SQL类)</h4><p>Pig</p><p>Hive</p><p>kylin</p><p>Spark SQL,</p><p>Spark DataFrame</p><p>Impala</p><p>Phoenix</p><p>ELK（ElasticSearch，Logstash，Kibana）</p><h4 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h4><p>Kafka(纯日志类，大吞吐量)</p><p>RocketMQ</p><p>ZeroMQ</p><p>ActiveMQ</p><p>RabbitMQ</p><h4 id="流式计算"><a href="#流式计算" class="headerlink" title="流式计算"></a>流式计算</h4><p>Storm/JStorm</p><p>Spark Streaming</p><p>Flink</p><h4 id="日志收集"><a href="#日志收集" class="headerlink" title="日志收集"></a>日志收集</h4><p>Scribe</p><p>Flume</p><h4 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h4><p>Java</p><p>Python</p><p>R</p><p>Ruby</p><p>Scala</p><h4 id="数据分析挖掘"><a href="#数据分析挖掘" class="headerlink" title="数据分析挖掘"></a>数据分析挖掘</h4><p>MATLAB</p><p>SPSS</p><p>SAS</p><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><p>R</p><p>D3.js</p><p>ECharts</p><p>Excle</p><p>Python</p><h4 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h4><p>机器学习基础</p><p>聚类</p><p>时间序列</p><p>推荐系统</p><p>回归分析</p><p>文本挖掘</p><p>决策树</p><p>支持向量机</p><p>贝叶斯分类</p><p>神经网络</p><p>机器学习工具</p><p>Mahout</p><p>Spark Mlib</p><p>TensorFlow (Google 系)</p><p>Amazon Machine Learning</p><p>DMTK (微软分布式机器学习工具)</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>一致性</p><p>paxos</p><p>raft</p><p>gossip</p><p>数据结构</p><p>栈，队列，链表</p><p>散列表</p><p>二叉树，红黑树，B树</p><p>图</p><p>常用算法<br>1.排序</p><p>插入排序</p><p>桶排序</p><p>堆排序</p><p>2.快速排序</p><p>3,最大子数组</p><p>4.最长公共子序列</p><p>5.最小生成树</p><p>最短路径</p><p>6.矩阵的存储和运算</p><h4 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h4><p>云服务</p><p>SaaS</p><p>PaaS</p><p>IaaS</p><p>Openstack</p><p>Docker</p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 职业知识图谱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ambari-资料篇</title>
      <link href="/2019/10/09/ambari-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/10/09/ambari-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="Ambari参考"><a href="#Ambari参考" class="headerlink" title="Ambari参考"></a>Ambari参考</h4><p>Ambari官网: <a href="http://ambari.apache.org/" target="_blank" rel="noopener">http://ambari.apache.org/</a></p><p>Ambari-Github: <a href="https://github.com/apache/ambari/tree/branch-2.7" target="_blank" rel="noopener">https://github.com/apache/ambari/tree/branch-2.7</a></p><p>Ambari参考: <a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-ambari-metrics/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-ambari-metrics/index.html</a></p><p>Ambari官网-2.7.0: <a href="https://docs.hortonworks.com/HDPDocuments/Ambari/Ambari-2.7.0.0/index.html" target="_blank" rel="noopener">https://docs.hortonworks.com/HDPDocuments/Ambari/Ambari-2.7.0.0/index.html</a></p><p>Apache-Ambari-2.6.2 <a href="http://www.apache.org/dyn/closer.cgi/ambari/ambari-2.6.2" target="_blank" rel="noopener">http://www.apache.org/dyn/closer.cgi/ambari/ambari-2.6.2</a></p><p>Apache-Ambari-2.6.2: <a href="https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.6.2" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.6.2</a></p><p>Apache-Ambari-guide: <a href="https://cwiki.apache.org/confluence/display/AMBARI/Ambari" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/AMBARI/Ambari</a></p><p>Apache-Ambari-2.7.0: <a href="https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.7.0" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.7.0</a></p><p>Ambari添加Stack参考：<a href="http://www.itkeyword.com/doc/617736624192567x360" target="_blank" rel="noopener">http://www.itkeyword.com/doc/617736624192567x360</a></p><p>Ambari扩展Stack参考：<a href="https://my.oschina.net/u/2277929/blog/666180?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">https://my.oschina.net/u/2277929/blog/666180?tdsourcetag=s_pctim_aiomsg</a></p><hr><h4 id="Ambari安装参考"><a href="#Ambari安装参考" class="headerlink" title="Ambari安装参考"></a>Ambari安装参考</h4><blockquote><p>Ambari2.7.0安装: <a href="https://blog.csdn.net/zsj777/article/details/81052859" target="_blank" rel="noopener">https://blog.csdn.net/zsj777/article/details/81052859</a></p></blockquote><h4 id="Ambari学习链接"><a href="#Ambari学习链接" class="headerlink" title="Ambari学习链接"></a>Ambari学习链接</h4><p>Ambari参考教程：<a href="https://www.aliyun.com/jiaocheng/topic_25417_1.html" target="_blank" rel="noopener">https://www.aliyun.com/jiaocheng/topic_25417_1.html</a></p><p>Ambari操作指南：<a href="https://blog.csdn.net/devalone/article/details/80781652" target="_blank" rel="noopener">https://blog.csdn.net/devalone/article/details/80781652</a></p><p>Ambari操作指南：<a href="https://blog.csdn.net/devalone/article/details/80800262" target="_blank" rel="noopener">https://blog.csdn.net/devalone/article/details/80800262</a></p><p>Ambari基础参考：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari/index.html</a></p><p>Ambari基础应用：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari/index.html</a></p><p>Ambari-Metrics：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-ambari-metrics/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-ambari-metrics/</a></p><p>Ambari-Alerts：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari3/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari3/</a></p><p>Ambari-RoadMap：<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=30755705" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=30755705</a></p><p>Hortonworks链接：<a href="https://hortonworks.com/apache/ambari/" target="_blank" rel="noopener">https://hortonworks.com/apache/ambari/</a></p>]]></content>
      
      
      <categories>
          
          <category> ambari </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ambari-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ambari-学习篇</title>
      <link href="/2019/10/09/ambari-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/10/09/ambari-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><pre><code>(1).Ambari的作用:就是创建、管理、监视 Hadoop 的集群，这里的Hadoop是广义，指的是Hadoop整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等）Ambari就是为了让Hadoop以及相关的大数据软件更容易使用的一个工具(2).Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。</code></pre>]]></content>
      
      
      <categories>
          
          <category> ambari </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ambari-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase-资料篇</title>
      <link href="/2019/10/09/hbase-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/10/09/hbase-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="HBase参考文档："><a href="#HBase参考文档：" class="headerlink" title="HBase参考文档："></a>HBase参考文档：</h4><p>HBase官网链接: <a href="https://hbase.apache.org/" target="_blank" rel="noopener">https://hbase.apache.org/</a><br>HBase官方文档: <a href="http://hbase.apache.org/book.html" target="_blank" rel="noopener">http://hbase.apache.org/book.html</a><br>HBase+Spark整合参考: <a href="http://hbase.apache.org/book.html#spark" target="_blank" rel="noopener">http://hbase.apache.org/book.html#spark</a><br>HBase中文文档: <a href="https://www.w3cschool.cn/hbase_doc/" target="_blank" rel="noopener">https://www.w3cschool.cn/hbase_doc/</a><br>HBase教程文档: <a href="https://www.yiibai.com/hbase/" target="_blank" rel="noopener">https://www.yiibai.com/hbase/</a><br>hbase-help：<a href="http://hbase-help.com/" target="_blank" rel="noopener">http://hbase-help.com/</a><br>HBase：<a href="https://pan.baidu.com/s/1jILzgns" target="_blank" rel="noopener">https://pan.baidu.com/s/1jILzgns</a><br>CSDN HBase资料库：<a href="http://lib.csdn.net/hbase/node/734" target="_blank" rel="noopener">http://lib.csdn.net/hbase/node/734</a></p><p>知乎HBase讨论：<a href="https://www.zhihu.com/topic/19600820/hot" target="_blank" rel="noopener">https://www.zhihu.com/topic/19600820/hot</a></p><h4 id="HBase架构参考："><a href="#HBase架构参考：" class="headerlink" title="HBase架构参考："></a>HBase架构参考：</h4><p>深入HBase架构参考: <a href="https://blog.csdn.net/xiaolang85/article/details/70054783" target="_blank" rel="noopener">https://blog.csdn.net/xiaolang85/article/details/70054783</a><br>深入理解HBase架构: <a href="https://blog.csdn.net/Yaokai_AssultMaster/article/details/72877127" target="_blank" rel="noopener">https://blog.csdn.net/Yaokai_AssultMaster/article/details/72877127</a><br>HBase底层存储参考: <a href="http://www.cnblogs.com/bonelee/p/6279248.html" target="_blank" rel="noopener">http://www.cnblogs.com/bonelee/p/6279248.html</a><br>HBase高性能分析: <a href="https://mp.weixin.qq.com/s/VCZz8rG9hIK7IKvbYLz0pQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/VCZz8rG9hIK7IKvbYLz0pQ</a></p><h4 id="HBase-AWS参考："><a href="#HBase-AWS参考：" class="headerlink" title="HBase-AWS参考："></a>HBase-AWS参考：</h4><p>HBase-AWS部署: <a href="https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hbase.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hbase.html</a><br>HBase-AWS-S3: <a href="https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hbase-s3.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hbase-s3.html</a><br>HBase+Hive参考: <a href="https://aws.amazon.com/cn/blogs/big-data/combine-nosql-and-massively-parallel-analytics-using-apache-hbase-and-apache-hive-on-amazon-emr/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/big-data/combine-nosql-and-massively-parallel-analytics-using-apache-hbase-and-apache-hive-on-amazon-emr/</a></p><h4 id="HBase下载参考："><a href="#HBase下载参考：" class="headerlink" title="HBase下载参考："></a>HBase下载参考：</h4><p>官网版本：<a href="http://archive.apache.org/dist/hbase/" target="_blank" rel="noopener">http://archive.apache.org/dist/hbase/</a></p><p>CDH版本(稳定,推荐)：<a href="http://archive.cloudera.com/cdh5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/</a></p><h4 id="Phoenix参考链接"><a href="#Phoenix参考链接" class="headerlink" title="Phoenix参考链接"></a>Phoenix参考链接</h4><p>Phoenix官网: <a href="http://phoenix.apache.org/" target="_blank" rel="noopener">http://phoenix.apache.org/</a><br>Phoenix下载: <a href="http://apache.fayea.com/phoenix/" target="_blank" rel="noopener">http://apache.fayea.com/phoenix/</a><br>Phoenix下载: <a href="https://archive.apache.org/dist/phoenix/" target="_blank" rel="noopener">https://archive.apache.org/dist/phoenix/</a><br>Phoenix-Github: <a href="https://github.com/apache/phoenix.git" target="_blank" rel="noopener">https://github.com/apache/phoenix.git</a></p><p>Phoenix加载数据：<a href="http://phoenix.apache.org/bulk_dataload.html" target="_blank" rel="noopener">http://phoenix.apache.org/bulk_dataload.html</a><br>Phoenix二级索引: <a href="http://phoenix.apache.org/secondary_indexing.html" target="_blank" rel="noopener">http://phoenix.apache.org/secondary_indexing.html</a><br>Phoenix安装参考: <a href="https://phoenix.apache.org/download.html#Installation" target="_blank" rel="noopener">https://phoenix.apache.org/download.html#Installation</a></p><p>ALI-Phoenix入门: <a href="https://help.aliyun.com/document_detail/53716.html?spm=a2c4g.11186623.4.1.mfLv99" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/53716.html?spm=a2c4g.11186623.4.1.mfLv99</a><br>ALI-HBase指南: <a href="https://help.aliyun.com/document_detail/52209.html?spm=a2c4g.11186623.6.559.YisTAt" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/52209.html?spm=a2c4g.11186623.6.559.YisTAt</a></p><h4 id="HBase-Phoenix参考"><a href="#HBase-Phoenix参考" class="headerlink" title="HBase+Phoenix参考"></a>HBase+Phoenix参考</h4><p>Phoenix入门到精通: <a href="https://yq.aliyun.com/articles/574090?spm=a2c4g.11186623.2.3.CCfNRU" target="_blank" rel="noopener">https://yq.aliyun.com/articles/574090?spm=a2c4g.11186623.2.3.CCfNRU</a><br>Phoenix链接HBase: <a href="https://blog.csdn.net/u010429286/article/details/70054232" target="_blank" rel="noopener">https://blog.csdn.net/u010429286/article/details/70054232</a><br>HBase整合Phoenix01: <a href="https://www.cnblogs.com/ballwql/p/8371234.html" target="_blank" rel="noopener">https://www.cnblogs.com/ballwql/p/8371234.html</a><br>Phoenix命令参考: <a href="https://blog.csdn.net/high2011/article/details/72772462" target="_blank" rel="noopener">https://blog.csdn.net/high2011/article/details/72772462</a></p><h4 id="HBase-Spark整合应用"><a href="#HBase-Spark整合应用" class="headerlink" title="HBase+Spark整合应用"></a>HBase+Spark整合应用</h4><h5 id="Spark访问HBase参考"><a href="#Spark访问HBase参考" class="headerlink" title="Spark访问HBase参考"></a>Spark访问HBase参考</h5><p>Spark访问HBase01: <a href="https://www.cnblogs.com/xing901022/p/8486290.html" target="_blank" rel="noopener">https://www.cnblogs.com/xing901022/p/8486290.html</a></p><p>hortonworks-spark/shc github：<a href="https://github.com/hortonworks-spark/shc" target="_blank" rel="noopener">https://github.com/hortonworks-spark/shc</a><br>maven仓库地址: <a href="http://mvnrepository.com/artifact/org.apache.hbase/hbase-spark" target="_blank" rel="noopener">http://mvnrepository.com/artifact/org.apache.hbase/hbase-spark</a><br>Hbase spark sql/ dataframe官方文档：<a href="https://hbase.apache.org/book.html#_sparksql_dataframes" target="_blank" rel="noopener">https://hbase.apache.org/book.html#_sparksql_dataframes</a><br>Hbase-spark 2.0.0-alpha4已经公开在maven仓库: <a href="http://mvnrepository.com/artifact/org.apache.hbase/hbase-spark" target="_blank" rel="noopener">http://mvnrepository.com/artifact/org.apache.hbase/hbase-spark</a></p><h5 id="Spark读写HBase参考"><a href="#Spark读写HBase参考" class="headerlink" title="Spark读写HBase参考"></a>Spark读写HBase参考</h5><p>HDP-SHC: <a href="https://github.com/hortonworks-spark/shc" target="_blank" rel="noopener">https://github.com/hortonworks-spark/shc</a></p><p>HBase-Java-API: <a href="https://www.cnblogs.com/liuwei6/p/6842536.html" target="_blank" rel="noopener">https://www.cnblogs.com/liuwei6/p/6842536.html</a><br>HBase-MapReduce: <a href="http://www.cnblogs.com/liuwei6/p/6855467.html" target="_blank" rel="noopener">http://www.cnblogs.com/liuwei6/p/6855467.html</a><br>HBase-Client-API: <a href="https://blog.csdn.net/vori2010/article/details/78536327" target="_blank" rel="noopener">https://blog.csdn.net/vori2010/article/details/78536327</a><br>HBaseClient基本操作: <a href="https://www.cnblogs.com/wzzkaifa/p/7323279.html" target="_blank" rel="noopener">https://www.cnblogs.com/wzzkaifa/p/7323279.html</a></p><p>HBase-Put操作: <a href="https://blog.csdn.net/dongbeiMan/article/details/51768251" target="_blank" rel="noopener">https://blog.csdn.net/dongbeiMan/article/details/51768251</a><br>HBase-Put操作: <a href="https://blog.csdn.net/mianshui1105/article/details/53305966" target="_blank" rel="noopener">https://blog.csdn.net/mianshui1105/article/details/53305966</a></p><p>Spark读写HBase: <a href="https://blog.csdn.net/u011812294/article/details/72553150" target="_blank" rel="noopener">https://blog.csdn.net/u011812294/article/details/72553150</a><br>Spark读写数据库: <a href="https://blog.csdn.net/zilong_zilong/article/details/52529263" target="_blank" rel="noopener">https://blog.csdn.net/zilong_zilong/article/details/52529263</a><br>Spark统计HBase条数: <a href="https://blog.csdn.net/u012871493/article/details/52701289" target="_blank" rel="noopener">https://blog.csdn.net/u012871493/article/details/52701289</a><br>SparkSQL+HBase: <a href="https://hbase.apache.org/book.html#_sparksql_dataframes" target="_blank" rel="noopener">https://hbase.apache.org/book.html#_sparksql_dataframes</a><br>Spark-DataFrame-HBase: <a href="https://www.cnblogs.com/xing901022/p/8486290.html" target="_blank" rel="noopener">https://www.cnblogs.com/xing901022/p/8486290.html</a></p><p>HBase自定义比较器: <a href="http://lucky-xingxing.iteye.com/blog/2185072" target="_blank" rel="noopener">http://lucky-xingxing.iteye.com/blog/2185072</a></p><h5 id="异常参考"><a href="#异常参考" class="headerlink" title="异常参考"></a>异常参考</h5><p>异常参考01: <a href="http://www.aboutyun.com/forum.php?mod=viewthread&amp;action=printable&amp;tid=17450" target="_blank" rel="noopener">http://www.aboutyun.com/forum.php?mod=viewthread&amp;action=printable&amp;tid=17450</a></p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kudu-资料篇</title>
      <link href="/2019/10/09/kudu-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/10/09/kudu-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> kudu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kudu-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kudu-学习篇</title>
      <link href="/2019/10/09/kudu-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/10/09/kudu-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><blockquote><p>Kudu是基于Hadoop平台的列式存储系统。</p></blockquote><h4 id="Kudu使用场景"><a href="#Kudu使用场景" class="headerlink" title="Kudu使用场景"></a>Kudu使用场景</h4><blockquote><p>适用于既有随机访问，也有批量数据扫描的复合场景</p></blockquote><blockquote><p>适用于高计算量的场景</p></blockquote><blockquote><p>充分利用高性能存储设备</p></blockquote><blockquote><p>支持数据更新，避免数据反复迁移</p></blockquote><blockquote><p>支持跨地域的实时数据备份和查询</p></blockquote><h4 id="Kudu的优势"><a href="#Kudu的优势" class="headerlink" title="Kudu的优势"></a>Kudu的优势</h4><blockquote><p>支持update和upsert操作</p></blockquote><blockquote><p>结构化数据模型</p></blockquote><blockquote><p>与imapla或spark集成后，可通过sql操作，使用方便</p></blockquote><blockquote><p>一个table由多个tablet组成，对分区查看、扩容和数据高可用支持非常好</p></blockquote><h4 id="性能压测："><a href="#性能压测：" class="headerlink" title="性能压测："></a>性能压测：</h4><h5 id="写入性能测试"><a href="#写入性能测试" class="headerlink" title="写入性能测试"></a>写入性能测试</h5><pre><code> executor-cores=2      num-executors=30    executor-memory=4G</code></pre><p>小时级别写（LZ4 ）：源数据2000W行，38G，耗时1.4min，平均23w条/s。</p><p>小时级别写（ZLIB）：源数据1940W行，34.9G，耗时2min，平均16.2w条/s。</p><h5 id="读取性能测试"><a href="#读取性能测试" class="headerlink" title="读取性能测试"></a>读取性能测试</h5><pre><code> executor-cores=2      num-executors=50    executor-memory=4G</code></pre><p>天级别读（LZ4 ）：源数据2.8亿行，541.7 G，耗时10min，平均46.6w条/s。</p><p>天级别读（ZLIB）：源数据3.2亿行，581.7 G，耗时15min，平均35.7w条/s。</p><h5 id="增加新字段性能测试"><a href="#增加新字段性能测试" class="headerlink" title="增加新字段性能测试"></a>增加新字段性能测试</h5><pre><code> executor-cores=2      num-executors=30    executor-memory=4G</code></pre><p>天级别增加（LZ4 ）：源数据4.74亿行，10个新列，耗时17min，平均46.5w条/s。（先读hive表，再选10列插入kudu中。所以其中包括读837G的text格式hive表全量读的时间，真实插入时间理应更少）</p><p>天级别增加（ZLIB）：源数据3.2亿行，10个新列，耗时12min，平均44.7w条/s。（包括读581G的text格式hive表全量读的时间）</p><h4 id="Kudu接口"><a href="#Kudu接口" class="headerlink" title="Kudu接口"></a>Kudu接口</h4><p>在KuduHandle.scala中封装了spark操作kudu的常用接口，在KuduHandleExample.scala中给出了Kudu接口的使用示例。</p><p>业务开发时可直接使用封装在 train_data_flow的dataflow-sdk中的方法，包括saveDFInsertKuduTable()、saveDFUpsertKuduTable()两种插入数据的方法，和读取kudu表数据的 kudu() 方法，以及addKuduColumns()加列方法。</p><h5 id="接口使用注意事项："><a href="#接口使用注意事项：" class="headerlink" title="接口使用注意事项："></a>接口使用注意事项：</h5><pre><code>1、通过读数据 kudu() 接口返回的df，注册临时视图后可直接用sparkSQL读取，   但必须使用between或者in来指定range分区字段的范围（目前字段是hour_id），   否则会变成全表扫描，效率极低！！！2、kudu引擎暂不支持sparkSQL中 &lt;、&gt;、or 这些谓词下推，支持like，   但仅限于“201907%”这种形式，不支持“201907%02”这种形式；  （只要正确指定了range分区字段，其他字段对读取效率影响不大）3、saveDFInsertKuduTable()用于base特征的小时级入库，会根据小时时间新建range分区，   saveDFUpsertKuduTable()用于插入新列数据，除base数据的小时级入库外，皆用此方法插入数据；4、待插入数据df必须包含所有的主键，且主键不可为空值；5、删除kudu分区会同时删除分区内的数据，慎用。</code></pre><h4 id="Alluxio-FUSE-测试"><a href="#Alluxio-FUSE-测试" class="headerlink" title="Alluxio FUSE 测试"></a>Alluxio FUSE 测试</h4><ol><li>IO基本性能测试，拷贝一个768M的文件。</li></ol><blockquote><p>读    130M/s    90M/s</p></blockquote><blockquote><p>写    80M/s    194M/s</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> kudu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kudu-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>clickhouse-资料篇</title>
      <link href="/2019/10/09/clickhouse-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/10/09/clickhouse-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> clickhouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clickhouse-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kylin-问题篇</title>
      <link href="/2019/10/09/kylin-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/10/09/kylin-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="远程连接服务器数据库报错：Host-‘XXXXXX’-is-blocked-because-of-many-connection-errors"><a href="#远程连接服务器数据库报错：Host-‘XXXXXX’-is-blocked-because-of-many-connection-errors" class="headerlink" title="远程连接服务器数据库报错：Host ‘XXXXXX’ is blocked because of many connection errors"></a>远程连接服务器数据库报错：Host ‘XXXXXX’ is blocked because of many connection errors</h4><pre><code>问题描述：Kylin聚合任务MR作业运行时，报错：Host ‘XXXXXX’ is blocked because of many connection errors错误原因：同一ip在短时间内产生太多（超过mysql数据库max_connection_errors的最大值）终端的数据库连接而导致的阻塞；解决方案（根据实际情况来选择解决方案）：1、可以更改max_connection_errors的值，即提高允许的max_connection_errors的数量（1）首先查看该属性设置为多大：命令：show global variables like &#39;%max_connect_errors%&#39;;如果需要永久生效，得去修改mysql配置文件里相应属性。可能配置文件里没有这个属性，需要自己手动添加（2）注意： 当客户端连接服务端超时(超过connect_timeout), 服务端就会给这个客户端记录一次error，当出错的次数达到max_connect_errors的时候，这个客户端就会被锁定。所以根据业务来尽量把这个值设置大一点，mysql默认值为10，我们可以根据具体需要设置大一点，这里设置为1000.（并非越大越好，越大被攻击时安全性越低）；2、使用清除缓存的方法，这样就会把计数清理掉，进入mysql控制台，执行：flush hosts;参考链接：https://jingyan.baidu.com/album/9f7e7ec087dcbe6f2815542d.html</code></pre><hr>]]></content>
      
      
      <categories>
          
          <category> kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kylin-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-资料篇</title>
      <link href="/2019/10/09/python-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/10/09/python-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400" target="_blank" rel="noopener">廖雪峰</a></p></blockquote><blockquote><p><a href="http://www.icourse163.org/learn/BIT-268001?tid=1002001005#/learn/content" target="_blank" rel="noopener">中国大学mocc</a></p></blockquote><blockquote><p><a href="https://github.com/jackfrued/Python-100-Days" target="_blank" rel="noopener">Python - 100天从新手到大师</a></p></blockquote><blockquote><p><a href="https://mlog.club/article/42724" target="_blank" rel="noopener">超实用的 30 段 Python 案例</a>  </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-学习篇</title>
      <link href="/2019/10/09/python-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/10/09/python-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="编码规范"><a href="#编码规范" class="headerlink" title="编码规范"></a>编码规范</h4><p><a href="http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/" target="_blank" rel="noopener">http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/</a></p><h4 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h4><p><a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/</a></p><h4 id="常用库"><a href="#常用库" class="headerlink" title="常用库:"></a>常用库:</h4><blockquote><p>pymysql</p></blockquote><blockquote><p>httplib2</p></blockquote><blockquote><p>kafka</p></blockquote><blockquote><p>kafka-python</p></blockquote><blockquote><p>pymongo</p></blockquote><blockquote><p>psycopg2</p></blockquote><blockquote><p>elasticsearch</p></blockquote><blockquote><p>murmurhash3</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-资料篇</title>
      <link href="/2019/10/09/scala-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/10/09/scala-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>官网：<a href="https://www.scala-lang.org/" target="_blank" rel="noopener">scala官网</a></p></blockquote><blockquote><p>易百：<a href="https://www.yiibai.com/scala/scala_strings.html" target="_blank" rel="noopener">scala教程</a></p></blockquote><blockquote></blockquote><blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scala-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-学习篇</title>
      <link href="/2019/10/09/scala-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/10/09/scala-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scala-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码库-表达式scala校验</title>
      <link href="/2019/10/09/%E4%BB%A3%E7%A0%81%E5%BA%93-%E8%A1%A8%E8%BE%BE%E5%BC%8Fscala%E6%A0%A1%E9%AA%8C/"/>
      <url>/2019/10/09/%E4%BB%A3%E7%A0%81%E5%BA%93-%E8%A1%A8%E8%BE%BE%E5%BC%8Fscala%E6%A0%A1%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h4 id="Regex常用的方法"><a href="#Regex常用的方法" class="headerlink" title="Regex常用的方法"></a>Regex常用的方法</h4><pre><code>findAllIn 方法找到line中所有符合正则的字符串，可以通过 toArray 方法来将匹配到的结果存放到 Array 中。也可以方法哦for循环中，遍历出符合条件的匹配。findFirstIn 只匹配第一个符合正则的字符串就停止了</code></pre><h4 id="设备id-校验"><a href="#设备id-校验" class="headerlink" title="设备id 校验"></a>设备id 校验</h4><pre><code>val deviceIdReg = &quot;^[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}$&quot;.r# EF453E92-B6D0-48B9-912B-A156B5C6B90Fdef validateDeviceId(deviceId: String): Boolean = {    deviceIdReg.findFirstMatchIn(deviceId).isDefined     }</code></pre><h4 id="查找匹配部分"><a href="#查找匹配部分" class="headerlink" title="查找匹配部分"></a>查找匹配部分</h4><pre><code>val p = &quot;[0-9]+&quot;.r  p.findAllIn(&quot;2 ad 12ab ab21 23&quot;).toList // List(2, 12, 21, 23)p.findFirstMatchIn(&quot;abc123xyz&quot;).get // scala.util.matching.Regex.Match = 123</code></pre><p>参考资料：<br>    1.<a href="http://www.runoob.com/scala/scala-regular-expressions.html" target="_blank" rel="noopener">正则表达式（regular expression）以及常用语法</a></p>]]></content>
      
      
      <categories>
          
          <category> 代码库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码库-表达式scala校验 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java-设计模式</title>
      <link href="/2019/09/30/java-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
      <url>/2019/09/30/java-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java-设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>elasticsearch-资料篇</title>
      <link href="/2019/09/30/elasticsearch-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/30/elasticsearch-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/laoyang360/deep_elasticsearch/wiki/%E3%80%8A%E6%AD%BB%E7%A3%95Elasticsearch%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%8B%E2%80%94%E2%80%94%E9%93%AD%E6%AF%85%E5%A4%A9%E4%B8%8B%E5%87%BA%E5%93%81" target="_blank" rel="noopener">死磕Elasticsearch方法论</a></p>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>课程学习</title>
      <link href="/2019/09/30/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/09/30/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><h5 id="深入JVM内核-原理、诊断与优化"><a href="#深入JVM内核-原理、诊断与优化" class="headerlink" title="深入JVM内核 - 原理、诊断与优化"></a>深入JVM内核 - 原理、诊断与优化</h5><pre><code>链接：http://pan.baidu.com/s/1geQsMcF 密码：wins解压密码：WxIqq%VJ8sQE9#Mq@WWW.MuKeDaBa.COM</code></pre><h4 id="python"><a href="#python" class="headerlink" title="python"></a>python</h4><h5 id="马哥Python全栈-爬虫-高端自动化课程"><a href="#马哥Python全栈-爬虫-高端自动化课程" class="headerlink" title="马哥Python全栈+爬虫+高端自动化课程"></a>马哥Python全栈+爬虫+高端自动化课程</h5><pre><code>链接：https://pan.baidu.com/s/1GbGnix9-_TDFIrCUuRFcBA 提取码：saf9解压密码：www.mukedaba.com </code></pre><hr><h5 id="python编程快速上手，让繁琐工作自动化"><a href="#python编程快速上手，让繁琐工作自动化" class="headerlink" title="python编程快速上手，让繁琐工作自动化"></a>python编程快速上手，让繁琐工作自动化</h5><pre><code>链接：http://pan.baidu.com/s/1c1M5vhA 密码：4hmk</code></pre><hr><h5 id="python实战：四周实现爬虫系统"><a href="#python实战：四周实现爬虫系统" class="headerlink" title="python实战：四周实现爬虫系统"></a>python实战：四周实现爬虫系统</h5><pre><code>链接：http://pan.baidu.com/s/1qYiqJmc 密码：hf20解压密码：www.mukedaba.com</code></pre><hr><h5 id="python自动化运维-web监控系统"><a href="#python自动化运维-web监控系统" class="headerlink" title="python自动化运维+web监控系统"></a>python自动化运维+web监控系统</h5><pre><code>链接: http://pan.baidu.com/s/1nuxcuJv 密码: 7qnk解压密码：www.mukedaba.com_pymaintenance</code></pre><h4 id="面试课程"><a href="#面试课程" class="headerlink" title="面试课程"></a>面试课程</h4><pre><code>2016年面试班BAT、阿里、谷歌视频教程14课高清长课时附ppt链接：http://pan.baidu.com/s/1gf1aVfH 密码：d0gp解压密码：www.17zixueba.com7ysfmis14%</code></pre>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>idea-使用篇</title>
      <link href="/2019/09/30/idea-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/30/idea-%E4%BD%BF%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><p>1.文件之间的跳转</p><pre><code>Ctrl+E　　 定位到最近浏览过的文件Ctrl+Shift+E　　最近更改的文件</code></pre><p>2.位置的跳转</p><pre><code></code></pre><h4 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h4><blockquote></blockquote><p><a href="https://www.lovesofttech.com/general/IDEA/" target="_blank" rel="noopener">IntelliJ IDEA，使用心得汇总</a><br><a href="https://github.com/judasn/IntelliJ-IDEA-Tutorial" target="_blank" rel="noopener"></a></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> idea-使用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>haproxy-资料篇</title>
      <link href="/2019/09/29/haproxy-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/29/haproxy-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.ilanni.com/?p=10009" target="_blank" rel="noopener">高负载均衡学习haproxy之关键词介绍</a><br><a href="https://www.ilanni.com/?p=10016" target="_blank" rel="noopener">高负载均衡学习haproxy之配置文件详解</a><br><a href="https://www.cnblogs.com/ilanni/p/4750081.html" target="_blank" rel="noopener">高负载均衡学习haproxy之安装与配置</a></p></blockquote><blockquote><p><a href="https://www.ilanni.com/?p=10026" target="_blank" rel="noopener">高负载均衡学习haproxy之TCP应用</a></p></blockquote><blockquote><p><a href="https://www.ilanni.com/?p=10676" target="_blank" rel="noopener">haproxy与nginx、zabbix集成</a><br><a href="https://www.ilanni.com/?p=10641" target="_blank" rel="noopener">haproxy学习之https配置</a><br><a href>haproxy学习之手机规则匹配</a></p></blockquote><blockquote><p><a href="http://www.ttlsa.com/linux/haproxy-study-tutorial/" target="_blank" rel="noopener">HAProxy用法详解 全网最详细中文文档</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> haproxy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> haproxy-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>haproxy-常用篇</title>
      <link href="/2019/09/29/haproxy-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/29/haproxy-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><blockquote><p>mac 安装</p></blockquote><pre><code> 安装：brew install haproxy 查看版本：haproxy -v 创建haproxy.cfg 文件 检测文件：haproxy -c -f /usr/local/etc/haproxy.cfg   执行文件： haproxy -f /usr/local/etc/haproxy.cfg -d</code></pre><blockquote><p>配置文件模版</p></blockquote><pre><code># 全局配置global    log 127.0.0.1 local0         # 设置日志    log 127.0.0.1 local1 notice    maxconn 4000                 # 最大连接数    chroot /usr/local/haproxy    # 安装目录    user haproxy    group haproxy    daemon                       # 守护进程运行    #nbproc 1                    # 进程数量，只能用于守护进程模式的haproxy；默认启动一个进程，一般只在单进程仅能打开少数文件描述符的场景中才使用多进程模式；    pidfile /var/run/haproxy.pid# 默认配置defaults    log     global    mode    http                 # 默认的模式mode { tcp|http|health }，tcp是4层，http是7层，health只会返回OK    option  httplog              # http 日志格式    option dontlognull           # 不记录健康检查日志信息；    option  redispatch           # serverId对应的服务器挂掉后,强制定向到其他健康的服务器    option http-server-close    #option  abortonclose        # 当服务器负载很高的时候，自动结束掉当前队列处理比较久的链接；    #option  forwardfor          # 如果后端服务器需要获得客户端真实ip需要配置的参数，可以从Http Header中获得客户端ip；    #option  httpclose           # 主动关闭http通道,每次请求完毕后主动关闭http通道,ha-proxy不支持keep-alive,只能模拟这种模式的实现;      balance roundrobin           # 负载均衡算法,轮询；    retries 3                    # 重试次数；    timeout http-request  10s    # 客户端建立连接但不请求数据时，关闭客户端连接；    timeout queue         1m     # 等待最大时长；    timeout connect 10s          # 定义haproxy将客户端请求转发至后端服务器所等待的超时时间；    timeout client 1m            # 客户端非活动状态的超长时间(默认毫秒)    timeout server 1m            # 客户端与服务器建立连接后，等待服务器端的超时时长(默认毫秒)    timeout http-keep-alive 10s  # 定义保持连接的超时时长；    timeout check 10s            # 心跳检测超时；    maxconn 3000                 # 每个server最大的连接数；# 统计页面配置listen admin_stats      bind 0.0.0.0:50000           # 监听IP和端口，为了安全可以设置本机的局域网IP及端口；    mode http    option httplog               # 采用http日志格式      stats refresh 30s            # 统计页面自动刷新时间      stats uri /haproxy?stats     # 状态管理页面，通过/haproxy?stats来访问    stats realm Haproxy Manager  # 统计页面密码框上提示文本      stats auth admin:psadmin     # 统计页面用户名和密码设置      #stats hide-version          # 隐藏统计页面上HAProxy的版本信息    #errorfile 403 /usr/local/haproxy/examples/errorfiles/   #设置haproxy 错误页面#前端配置（多个frontend 必须绑定不同的IP或者端口，否则数据会串，导致映射到不同的后端而报错）frontend http_main    bind 0.0.0.0:80              # http请求的端口，会被转发到设置的ip及端口    # 转发规则    #acl url_yuming   path_beg www.yuming.com    #use_backend server_yuming if url_yuming    # 默认跳转项，当上面都没有匹配上，就转到backend的http_default上；    default_backend http_default    # 提升失败的时候的用户体验    #errorfile 502 /usr/local/haproxy/examples/errorfiles/502.http    #errorfile 503 /usr/local/haproxy/examples/errorfiles/503.http    #errorfile 504 /usr/local/haproxy/examples/errorfiles/504.http# 后端配置backend http_default    # 额外的一些设置，按需使用    option forwardfor    option forwardfor header Client-IP    option http-server-close    option httpclose    # 负载均衡方式    #source 根据请求源IP    #static-rr 根据权重    #leastconn 最少连接先处理;在有着较长时间会话的场景中推荐使用此算法，如LDAP、SQL等，其并不太适用于较短会话的应用层协议，如HTTP；此算法是动态的，    #uri 根据请求的uri    #url_param 根据请求的url参数    #rdp-cookie 据据cookie(name)来锁定并哈希每一次请求    #hdr(name) 根据HTTP请求头来锁定每一次HTTP请求    #roundrobin 轮询方式    balance roundrobin           # 负载均衡的方式,轮询方式    # 设置健康检查页面    #option httpchk GET /index.html    #传递客户端真实IP    option forwardfor header X-Forwarded-For    # 需要转发的ip及端口    # inter 2000 健康检查时间间隔2秒    # rise 3 检测多少次才认为是正常的    # fall 3 失败多少次才认为是不可用的    # weight 30 权重    server node1 192.168.1.101:8080 check inter 2000 rise 3 fall 3 weight 30    server node2 192.168.1.101:8081 check inter 2000 rise 3 fall 3 weight 30</code></pre><blockquote><p>配置介绍</p></blockquote><pre><code>global: nbproc 默认是 1 ，我的 CPU 是 4 核 8 线程，这里就设置成了 8backend: &lt;your server ip&gt; 、 &lt;port&gt; 替换成你的 SS 服务器 IP 和端口backend: check 健康检查backend: inter 心跳频率backend: rise 将服务器标记为可用 的次数backend: fall 将服务器标记为不可用 的次数</code></pre><blockquote><p>实战</p></blockquote><blockquote><blockquote><p>1.启动hiveServer</p></blockquote></blockquote><pre><code>nohup hive --service hiveserver2 --hiveconf hive.fetch.task.conversion=none --hiveconf hive.exec.scratchdir=/tmp/hive-hadoop-offline --hiveconf hive.exec.local.scratchdir=/tmp/hive-root --hiveconf hive.server2.enable.doAs=false --hiveconf hive.server2.long.polling.timeout=50000 --hiveconf hive.root.logger=INFO,console --hiveconf hive.exec.mode.local.auto=false --hiveconf hadoop.proxyuser.root.groups=* --hiveconf hadoop.proxyuser.root.hosts=* --hiveconf hive.server2.webui.port=10034 --hiveconf hive.server2.thrift.port=10035 --hiveconf hive.mapred.reduce.tasks.speculative.execution=false --hiveconf hive.mapred.map.tasks.speculative.execution=false --hiveconf mapreduce.job.reduce.slowstart.completedmaps=1 &gt; ./hiveServer_10035.log&amp;</code></pre><blockquote><blockquote><p>2.配置haproxy.cfg</p></blockquote></blockquote><pre><code>global                daemon                nbproc 1                pidfile /data/hiveServer_service/haproxy/pid                ulimit-n 65535defaults                mode tcp                retries 2                option redispatch                option abortonclose                maxconn 1024                timeout connect 1d                timeout client 1d                timeout server 1d                timeout check 2000                log 127.0.0.1 local0 errlisten admin_stats                bind 0.0.0.0:10021                mode http                maxconn 10                stats refresh 30s                stats uri /                stats realm Hive\ Haproxy                stats auth admin:123456listen hive_common                bind 0.0.0.0:10025                mode tcp                balance roundrobin                maxconn 1024                server hive_common_01_10009 127.0.0.1:10009 check inter 180000 rise 1 fall 2                server hive_common_02_10011 127.0.0.1:10011 check inter 180000 rise 1 fall 2                server hive_common_03_10013 127.0.0.1:10013 check inter 180000 rise 1 fall 2listen hive_dataplatform_offline                bind 0.0.0.0:10023                mode tcp                balance roundrobin                maxconn 1024                server hive_dataplatform_offline_01_10015 127.0.0.1:10015 check inter 180000 rise 1 fall 2                server hive_dataplatform_offline_02_10017 127.0.0.1:10017 check inter 180000 rise 1 fall 2listen hive_scientist_offline                bind 0.0.0.0:10026                mode tcp                balance roundrobin                maxconn 1024                server hive_scientist_offline_01_10031 127.0.0.1:10031 check inter 180000 rise 1 fall 2                server hive_scientist_offline_02_10033 127.0.0.1:10033 check inter 180000 rise 1 fall 2</code></pre>]]></content>
      
      
      <categories>
          
          <category> haproxy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> haproxy-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-常用篇</title>
      <link href="/2019/09/29/mysql-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/29/mysql-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="mysql-mac-安装"><a href="#mysql-mac-安装" class="headerlink" title="mysql-mac 安装"></a>mysql-mac 安装</h4><pre><code>brew install mysqlunset TMPDIRmysql_install_db --verbose --user=`whoami` --basedir=&quot;$(brew --prefix mysql)&quot; --datadir=/usr/local/var/mysql --tmpdir=/tmp</code></pre><h4 id="配置信息"><a href="#配置信息" class="headerlink" title="配置信息"></a>配置信息</h4><blockquote><p>conf</p></blockquote><pre><code> /usr/local/etc/my.cnf</code></pre>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka-常用篇</title>
      <link href="/2019/09/29/kafka-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/29/kafka-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h5 id="启动kafka"><a href="#启动kafka" class="headerlink" title="启动kafka"></a>启动kafka</h5><pre><code> sh bin/kafka-server-start.sh config/server.properties</code></pre><hr><h5 id="创建topic-分区数"><a href="#创建topic-分区数" class="headerlink" title="创建topic 分区数"></a>创建topic 分区数</h5><pre><code>sh kafka-topics.sh --create --topic kafkatopic --replication-factor 1 --partitions 1 --zookeeper localhost:2181</code></pre><h5 id="查看topic"><a href="#查看topic" class="headerlink" title="查看topic"></a>查看topic</h5><pre><code>bash bin/kafka-topics.sh --list --zookeeper localhost:2181</code></pre><hr><h5 id="启动producer"><a href="#启动producer" class="headerlink" title="启动producer"></a>启动producer</h5><pre><code>sh kafka-console-producer.sh --broker-list localhost:9092 --sync --topic kafkatopic</code></pre><hr><h5 id="启动consumer"><a href="#启动consumer" class="headerlink" title="启动consumer"></a>启动consumer</h5><pre><code>sh kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafkatopic --from-beginning</code></pre>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>心灵笔录</title>
      <link href="/2019/09/29/%E5%BF%83%E7%81%B5%E7%AC%94%E5%BD%95/"/>
      <url>/2019/09/29/%E5%BF%83%E7%81%B5%E7%AC%94%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<pre><code>凡事都有因果，万事皆有源头    </code></pre><hr><pre><code>不是要做到对，而是要做到位</code></pre><hr><pre><code>正因为难所以值钱</code></pre><hr><pre><code>简单粗暴高效，慢的话你连地铁都挤不上</code></pre><hr><pre><code>编程不编不成</code></pre><hr><pre><code>不会，立刻马上去学</code></pre><hr><pre><code>天上飞的理念，必有落地实现。</code></pre><hr><pre><code>自律方能自由。</code></pre><hr><pre><code>用进废退.</code></pre><hr><pre><code>小孩子才有正错，成年人的世界只有利弊成败。</code></pre><hr><pre><code>选择重于努力，方向重于速度。</code></pre><hr><pre><code>人家有背景，你有的是背影。</code></pre><hr><pre><code>今天的最后表现是明天的最低要求。自古成功靠强迫。</code></pre><hr><pre><code>刻意的训练+考点的重复。</code></pre><pre><code>今天的优势会被以后的趋势所取代。</code></pre><hr><pre><code>千羊在外，不如一鸟在手。</code></pre><hr><pre><code>多于同好争高下，不与傻逼论长短。</code></pre><hr><pre><code>没有无所谓，只有做到位。</code></pre><hr><pre><code>成大事的男人：第一不要脸，第二坚持不要脸，第三坚持长期不要脸。</code></pre><hr><pre><code>烧不死的鸟才是火凤凰。</code></pre><hr><pre><code>适用用尽则祸必来。</code></pre><hr><pre><code>好的开始是成功的全部。</code></pre><hr><pre><code>今天没有学习，明天就没有竞争力。</code></pre><hr><pre><code>不要写问好的人，只要写句号的人。</code></pre><hr><pre><code>职场上只有0和100。</code></pre><hr><pre><code>程序员分类：1.功能型程序员 2.性能型程序员</code></pre><hr><pre><code>男人的尊严是打出来的</code></pre><hr><pre><code>职业化，专业化。</code></pre><hr><pre><code>深深的水，静静地流。</code></pre><hr><pre><code>狼性生存法则。</code></pre><hr><pre><code>复杂复杂才是真。</code></pre><hr><pre><code>认真就赢了。</code></pre><hr><pre><code>怕啰嗦就一次做到位。</code></pre><hr><pre><code>少就事多（专一）。</code></pre><hr><pre><code>出来混，野蛮生长。</code></pre><hr><pre><code>帮你是情分，不帮是本分。</code></pre><hr><pre><code>不会游泳，不停地换游泳池是没用的。</code></pre><hr><pre><code>脾气不要打过你的本事。</code></pre><hr><pre><code>加以时日你也行,但这些时日必须做到值得你铭记。</code></pre><hr><pre><code>工作起来像蚂蚁，生活起来像蝴蝶。</code></pre><hr><pre><code>若儿女情长，必英雄气短。</code></pre><hr><pre><code>跟对人，才能做对事。</code></pre><hr><pre><code>要想人前显贵，必定人后受罪。</code></pre><hr><pre><code>年轻人时间最重要。</code></pre><hr><pre><code>先投资自己的脑袋，再考虑自己的口袋</code></pre><hr><pre><code>你做不到不一定别人做不到。</code></pre><hr><pre><code>先顺着别人来，让他满意，然后再说特殊情况，来让他吃惊。</code></pre><hr><pre><code>雄心的一半是耐心</code></pre><hr><pre><code>打败你的一定不是硬硬的困难，一定是软绵绵的诱惑。</code></pre><hr><pre><code>态度、行动、精神。</code></pre><hr><pre><code>深度的思考，重于刻苦的勤奋</code></pre><hr><pre><code>程序员的懒是高度、深度、深刻的懒（深度思考）</code></pre><hr><pre><code>不要用你战术上的勤奋掩盖你战略上的懒惰。</code></pre><hr><pre><code>最舒服的日子永远是昨天。</code></pre><hr><pre><code>莫把行情当能力。</code></pre><hr><pre><code>拿钱来赚时间。</code></pre><hr><pre><code>人弱万事难。</code></pre><hr><pre><code>当你没有见过最好的，你就常常以为自己所拥有的就是最好的，从而将自己禁锢在自己的小世界里，忘记了世界之大。</code></pre><hr><pre><code>一个见过大世面的人，会讲究能将就，能享受最好的，也能承受最坏的。</code></pre><hr><pre><code>人生不能省去的三件事：学习、旅游、健身。</code></pre><hr><pre><code>昨天的最好表现，是今天的最低要求</code></pre><hr><pre><code>以后你流的泪是当初脑子进的水</code></pre><hr><pre><code>生于忧患，死于安乐</code></pre><hr><pre><code>在中国干事有四个字：前提条件。</code></pre><hr><pre><code>跟女人的香水一样，闻闻可以，千万别喝。</code></pre><hr><pre><code>既然球场上要有人赢，为什么不是我。</code></pre><hr><pre><code>你还能比现在还差吗？</code></pre><hr><pre><code>职场上只有线程直播没有彩排</code></pre><hr><pre><code>互联网只有作战和备战状态</code></pre><hr><pre><code>伸头一刀缩头也是一刀，怕什么，伸头砍。</code></pre><hr><pre><code>地狱的尽头是天堂。</code></pre><hr><pre><code>想跳得高，要蹲着久(攒足了劲，一猛子跳起来)</code></pre><hr><pre><code>北京专治各种不服</code></pre><hr><pre><code>领导给一个？要竭尽全力还他一个。（简单粗暴）</code></pre><hr><pre><code>日工一足，怕的是放弃</code></pre><hr><pre><code>一个人至少拥有一个梦想，有一个理由去坚强。心若没有栖息的地方，到哪里都是在流浪</code></pre><hr><pre><code>不停的走下去就对了，人不可能一辈子走背字。</code></pre><hr><pre><code>都是一肩膀扛着一个脑袋，别人能做到的，你也能做到，做不到那时你还不够努力.</code></pre><hr><pre><code>不攀，（不攀比权贵，如果你想有，可以通过自己的努力），不嫉（不嫉妒别人），不恶（不毁），不弃（不放弃提升自己）</code></pre><hr><pre><code>要想有机会，首先你得从人群中冒出来，要想冒出来，你就必须做到与众不同，要做到与众不同，你就要做得更多！</code></pre><hr><pre><code>芝兰生于深谷，不以无人而不芳。君子修身养德，不以穷困而改志。</code></pre><hr><pre><code>生活太安逸了，工作就会被生活所累</code></pre><hr><pre><code>管住嘴，迈开腿</code></pre>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心灵笔录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试资料</title>
      <link href="/2019/09/25/%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99/"/>
      <url>/2019/09/25/%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<h4 id="面试必刷"><a href="#面试必刷" class="headerlink" title="面试必刷"></a>面试必刷</h4><blockquote><p><a href="https://mlog.club/article/42710" target="_blank" rel="noopener">揭秘：月薪30~40K的程序员，在业内算什么水平？</a></p></blockquote><blockquote><p><a href="https://cyc2018.github.io/CS-Notes" target="_blank" rel="noopener">Tech Interview Guide 技术面试必备基础知识、Leetcode 题解、设计模式、Java、C++、Python、后端面试、操作系统、计算机网络、系统设计</a></p></blockquote><blockquote><p><a href="https://mlog.club/article/40687" target="_blank" rel="noopener">flink-40问</a></p></blockquote><h4 id="语言类"><a href="#语言类" class="headerlink" title="语言类"></a>语言类</h4><blockquote><p><a href="http://calvin1978.blogcn.com/articles/collection.html" target="_blank" rel="noopener">java</a></p><blockquote><p><a href="https://blog.csdn.net/qq_34337272/article/details/85640050" target="_blank" rel="noopener">java-常见面试集合</a></p></blockquote></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/11vry9estQtQmQnRnbTksQ" target="_blank" rel="noopener">scala</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/VIUlROU2hhz7oeOFTVC5sA" target="_blank" rel="noopener">scala</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/n7u4nIvpyjMjTA4Yo1w5JQ" target="_blank" rel="noopener">sql</a></p></blockquote><h4 id="知识扩展："><a href="#知识扩展：" class="headerlink" title="知识扩展："></a>知识扩展：</h4><blockquote><p><a href="https://mp.weixin.qq.com/s/hccUo170_8lhdv6C3OzGzw" target="_blank" rel="noopener">推荐系统实现</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/tek6kQb7t8tRQi8OiOj3Zw" target="_blank" rel="noopener">用户行为数据收集系统-(1)简介</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/OTNSptAO01y5gAwnSRpZZQ" target="_blank" rel="noopener">用户行为数据收集系统–(2)客户端SDK设计</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/rQnqsALpqPnN0RoTOyFJow" target="_blank" rel="noopener">用户行为数据收集系统–(3)数据接收端设计</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/ffFy6eE_N5QKn_-o6mqQ5g" target="_blank" rel="noopener">大数据世界</a></p></blockquote><h4 id="思想指导："><a href="#思想指导：" class="headerlink" title="思想指导："></a>思想指导：</h4><blockquote><p><a href="https://www.lovesofttech.com/job/adviseForCandidates/" target="_blank" rel="noopener">指导</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/qRwDowetBkJqpeMeAZsIpA" target="_blank" rel="noopener">招聘面试的套路与原则</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/3f8hGAQ-auLdkxkQ8XG3CQ" target="_blank" rel="noopener">程序员，为什么写不好一份简历？</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 面试资料 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试资料 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>promethus-问题篇</title>
      <link href="/2019/09/25/promethus-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/09/25/promethus-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="address-already-in-use"><a href="#address-already-in-use" class="headerlink" title="address already in use"></a>address already in use</h4><blockquote><p>报错信息：</p></blockquote><pre><code>error starting web server: listen tcp 0.0.0.0:9090: bind: address already in use</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>lsof -i :9090sudo kill -9  prot</code></pre><h4 id><a href="#" class="headerlink" title></a></h4>]]></content>
      
      
      <categories>
          
          <category> promethus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> promethus-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>promethus-学习篇</title>
      <link href="/2019/09/25/promethus-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/promethus-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="prometheus-介绍"><a href="#prometheus-介绍" class="headerlink" title="prometheus 介绍"></a>prometheus 介绍</h4><pre><code>1.prometheus负责从pushgateway和job中采集数据， 存储到后端Storatge中，可以通过PromQL进行查询,推送alerts信息到AlertManager。 AlertManager根据不同的路由规则进行报警通知.2.可以很好地记录任何纯数字时间序列。它既适合以机器为中心的监视，也适合高度动态的面向服务的体系结构的监视。在微服务的世界中，它对多维数据收集和查询的支持是一个特别的优势。普罗米修斯是为可靠性而设计的，它是您在停机期间使用的系统，允许您快速诊断问题。每台普罗米修斯服务器都是独立的，不依赖于网络存储或其他远程服务。当您的基础设施的其他部分被破坏时，您可以依赖它，并且您不需要设置广泛的基础设施来使用它。</code></pre><h4 id="Metrics-类型"><a href="#Metrics-类型" class="headerlink" title="Metrics 类型"></a>Metrics 类型</h4><h5 id="Counter（计数器）"><a href="#Counter（计数器）" class="headerlink" title="Counter（计数器）"></a>Counter（计数器）</h5><pre><code>#获取样本数据node_cpu{cpu=&quot;cpu0&quot;,mode=&quot;idle”} #通过rate()函数获取HTTP请求量的增长率rate(http_requests_total[5m])# 查询当前系统中，访问量前10的HTTP地址：topk(10, http_requests_total)</code></pre><h5 id="Gauge-可增可减的仪表盘"><a href="#Gauge-可增可减的仪表盘" class="headerlink" title="Gauge 可增可减的仪表盘"></a>Gauge 可增可减的仪表盘</h5><pre><code>预测系统磁盘空间在4个小时之后的剩余情况：predict_linear(node_filesystem_free{job=&quot;node&quot;}[1h], 4 * 3600)</code></pre><h5 id="Histogram和Summary分析数据分布情况"><a href="#Histogram和Summary分析数据分布情况" class="headerlink" title="Histogram和Summary分析数据分布情况"></a>Histogram和Summary分析数据分布情况</h5><hr><h4 id="PromQL基础"><a href="#PromQL基础" class="headerlink" title="PromQL基础"></a>PromQL基础</h4><h5 id="常用函数参考"><a href="#常用函数参考" class="headerlink" title="常用函数参考"></a>常用函数参考</h5><blockquote><p><a href="https://prometheus.io/docs/prometheus/latest/querying/functions/" target="_blank" rel="noopener">https://prometheus.io/docs/prometheus/latest/querying/functions/</a></p></blockquote><h5 id="基本函数语法"><a href="#基本函数语法" class="headerlink" title="基本函数语法"></a>基本函数语法</h5><pre><code>prometheus_http_requests_total  等价于prometheus_http_requests_total{}</code></pre><p>#####条件筛选</p><pre><code>prometheus_http_requests_total{instance=&quot;localhost:9090”}不满足条件prometheus_http_requests_total{instance!=&quot;localhost:9090”}</code></pre><pre><code>支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离：使用label=~regx表示选择那些标签符合正则表达式定义的时间序列； 反之使用label!~regx进行排除http_requests_total{environment=~&quot;staging|testing|development&quot;,method!=&quot;GET”}</code></pre><h5 id="范围查询"><a href="#范围查询" class="headerlink" title="范围查询"></a>范围查询</h5><pre><code>例如：查询最近5分钟http_request_total{}[5m] s - 秒 m - 分钟 h - 小时 d - 天 w - 周 y - 年</code></pre><pre><code>瞬时向量表达式或者区间向量表达式中，都是以当前时间为基准http_request_total{} # 瞬时向量表达式，选择当前最新的数据http_request_total{}[5m]  # 区间向量表达式，选择以当前时间为基准，5分钟内的数据</code></pre><pre><code>如果我们想查询，5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据呢?，一天前5分钟内的样本数据？ 这个时候我们就可以使用位移操作，位移操作的关键字为offsethttp_request_total{} offset 5m http_request_total{}[1d] offset 1dhttp_request_total{}[5m] offset 1d</code></pre><pre><code>获取前3个top 值topk(3,prometheus_http_requests_total{instance=~&quot;$node&quot;}[1m])</code></pre><h5 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h5><pre><code> 查询系统所有http请求的总量sum(http_request_total)</code></pre><pre><code> 按照mode计算主机CPU的平均使用时间avg(node_cpu) by (mode)</code></pre><pre><code> 按照主机查询各个主机的CPU使用率sum(sum(irate(node_cpu{mode!=&#39;idle&#39;}[5m])) / sum(irate(node_cpu[5m]))) by (instance)</code></pre><pre><code>irate平均值 节点中每分钟请求的平均值irate（prometheus_http_requests_total{instance=~&quot;$node&quot;}[1m]）</code></pre><h5 id="支持的算法："><a href="#支持的算法：" class="headerlink" title="支持的算法："></a>支持的算法：</h5><ul><li>(加法) - (减法) * (乘法) / (除法) % (求余) ^ (幂运算)</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> promethus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> promethus-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cassandra-资料篇</title>
      <link href="/2019/09/25/cassandra-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/cassandra-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="Youte-视频"><a href="#Youte-视频" class="headerlink" title="Youte 视频"></a>Youte 视频</h4><pre><code>DataStax Accelerate 2019 — Full Session Recordings： https://www.youtube.com/playlist?list=PLm-EPIkBI3YpJbuKUGDlZVNHzT0umcBSl 大概85个视频，涉及到cassandra各个方面。下次会议是 https://apachecon.com/acna19/s/#/schedule?search=cassandra 可以关注下https://issues.apache.org/jira/browse/CASSANDRA-11815</code></pre><h4 id="在线文档"><a href="#在线文档" class="headerlink" title="在线文档"></a>在线文档</h4><p><a href="https://www.tutorialspoint.com/cassandra/index.htm" target="_blank" rel="noopener">Cassandra文档</a></p><h4 id="前辈实践"><a href="#前辈实践" class="headerlink" title="前辈实践"></a>前辈实践</h4><p><a href="https://zqhxuyuan.github.io/tags/cassandra/" target="_blank" rel="noopener">https://zqhxuyuan.github.io/tags/cassandra/</a></p>]]></content>
      
      
      <categories>
          
          <category> cassandra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cassandra-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zeppelin-问题篇</title>
      <link href="/2019/09/25/zeppelin-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/09/25/zeppelin-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="hive-task-失败"><a href="#hive-task-失败" class="headerlink" title="hive task 失败"></a>hive task 失败</h4><blockquote><p>报错信息：<br>    <code>ERROR [bc60fbe1-ee2a-49e5-8932-231b92250134 main] ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</code><br>问题原因：<br>    <code>作业被kill 掉后，会出现以上信息。</code></p></blockquote><h4 id="Apache-zeppelin-process-died"><a href="#Apache-zeppelin-process-died" class="headerlink" title="Apache zeppelin process died"></a>Apache zeppelin process died</h4><blockquote><p>报错信息：<br>    <code>Apache zeppelin process died</code><br>解决方法:<br>    <code>服务失败，重启服务    ps -ef | grep &quot;zeppelin&quot;   kill -9 pid   sudo bin/zeppelin-daemon.sh restart</code></p></blockquote><h4 id="权限问题，无法执行"><a href="#权限问题，无法执行" class="headerlink" title="权限问题，无法执行"></a>权限问题，无法执行</h4><blockquote><p>报错信息：<br>    <code>shell_web_path is: http://zeppeline.com:8070/file/. shell/xianchang.yueJob 20190815-140750_2122347662 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-shell::2EHURB1RG-shared_session</code><br>解决方法：<br>    <code>切换当前，shell的目录为可执行的目录 chown -R XXX:XXX shell/</code></p></blockquote><h4 id="connection-refused-错误"><a href="#connection-refused-错误" class="headerlink" title="connection refused 错误"></a>connection refused 错误</h4><blockquote><p>报错原因：</p></blockquote><pre><code>可能正在使用过期的开发终端节点。尝试创建新的开发终端节点并重新连接。</code></pre><blockquote><p>解决方法：(推荐第一种，不可以后再选择第二种)</p></blockquote><pre><code>1.重新创建新的note 进行执行。会建立新的连接。2.连接超时或由于任何原因停止工作，则您可能需要采取以下步骤来还原它：    (1). 在 Zeppelin 中，在页面右上角的下拉菜单中，选择 Interpreters (解释器)。在解释器页面上，搜索 spark。选择 edit，然后清除 Connect to existing process 复选框。在页面底部选择 Save。    (2). 如前所述启动 SSH 端口转发。    (3). 在 Zeppelin 中，重新启用 spark 解释器的 Connect to existing process 设置，然后再次保存。像这样重置解释器应该会恢复连接。另一种实现方法是在 Interpreters 页面上为 Spark 解释器选择 restart。然后，等待最多 30 秒，以确保远程解释器已重新启动。</code></pre><h4 id><a href="#" class="headerlink" title></a></h4>]]></content>
      
      
      <categories>
          
          <category> zeppelin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zeppelin-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>grafana-常用篇</title>
      <link href="/2019/09/25/grafana-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/grafana-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="grafana-安装和配置"><a href="#grafana-安装和配置" class="headerlink" title="grafana 安装和配置"></a>grafana 安装和配置</h4><p>#####常用配置地址</p><pre><code>配置：    配置文件应位于/usr/local/etc/grafana/grafana.ini。日志：    日志文件应位于/usr/local/var/log/grafana/grafana.log。插件：    如果你想手动安装一个插件，请点击此处：/usr/local/var/lib/grafana/plugins。数据库：    默认的sqlite数据库位于 /usr/local/var/lib/grafana</code></pre><h5 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h5><pre><code>wget https://dl.grafana.com/oss/release/grafana-6.3.5.linux-amd64.tar.gztar -zxvf grafana-6.3.5.linux-amd64.tar.gzconf/grafana.ini./bin/grafana-server web</code></pre><h5 id="mac-安装方式"><a href="#mac-安装方式" class="headerlink" title="mac 安装方式"></a>mac 安装方式</h5><pre><code>安装：brew install grafana执行：brew services start grafana重启：brew services restart grafana</code></pre><hr><h4 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h4><pre><code>1. 鼠标放在图标上，按 ‘e’ 直接启动编辑模式2. </code></pre><h4 id="grafana-setting"><a href="#grafana-setting" class="headerlink" title="grafana setting"></a>grafana setting</h4><h5 id="General"><a href="#General" class="headerlink" title="General"></a>General</h5><blockquote><p>仪表盘的基本信息</p></blockquote><h5 id="Annotations"><a href="#Annotations" class="headerlink" title="Annotations"></a>Annotations</h5><blockquote><p>注解，一般很少使用</p></blockquote><h5 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h5><p>当前定义的值将在制作仪表盘的时候使用。</p><blockquote><p>变量 和定义</p></blockquote><table><thead><tr><th>Name</th><th>Description</th><th>demo</th></tr></thead><tbody><tr><td>label_names()</td><td>Returns a list of label names.</td><td></td></tr><tr><td>label_values(label)</td><td>Returns a list of label values for the label in every metric.</td><td></td></tr><tr><td>label_values(metric, label)</td><td>Returns a list of label values for the label in the specified metric.</td><td>label_values(grafana_instance_start_total, instance) 返回所有granfana的instance 的值</td></tr><tr><td>metrics(metric)</td><td>Returns a list of metrics matching the specified metric regex.</td><td></td></tr><tr><td>query_result(query)</td><td>Returns a list of Prometheus query result for the query.</td><td></td></tr></tbody></table><h5 id="Dashboard-Links"><a href="#Dashboard-Links" class="headerlink" title="Dashboard Links"></a>Dashboard Links</h5><h5 id="Versions"><a href="#Versions" class="headerlink" title="Versions"></a>Versions</h5><blockquote><p>记录用户修改版本时间</p></blockquote><h5 id="Permissions"><a href="#Permissions" class="headerlink" title="Permissions"></a>Permissions</h5><h5 id="JSON-Model"><a href="#JSON-Model" class="headerlink" title="JSON Model"></a>JSON Model</h5><h4 id="grafana-集成ldap"><a href="#grafana-集成ldap" class="headerlink" title="grafana 集成ldap"></a>grafana 集成ldap</h4><p><a href="https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_liunx_51_ldap_for_grafana.html" target="_blank" rel="noopener">grafana集成ldap</a></p>]]></content>
      
      
      <categories>
          
          <category> grafana </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grafana-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx-常用篇</title>
      <link href="/2019/09/25/nginx-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/nginx-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>zabbix-常用篇</title>
      <link href="/2019/09/25/zabbix-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/zabbix-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>启动zabbix</p></blockquote><pre><code>/usr/local/zabbix/sbin/zabbix_server</code></pre><blockquote><p>zabbix 安装配置</p></blockquote><p>链接：<a href="http://blog.itttl.com/blog/zabbix_server_install.html" target="_blank" rel="noopener">安装配置</a><br><a href="http://blog.itttl.com/blog/zabbix_agent_install.html" target="_blank" rel="noopener">agent 安装配置</a></p>]]></content>
      
      
      <categories>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix 常用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>promethus-资料篇</title>
      <link href="/2019/09/25/promethus-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/promethus-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.kancloud.cn/nicefo71/prometheus-doc-zh" target="_blank" rel="noopener">Prometheus2.13官方文档中文翻译</a></p><p><a href="https://www.cnblogs.com/zhaojiedi1992/tag/prometheus/" target="_blank" rel="noopener">prometheus-系列篇</a></p><p><a href="https://www.bookstack.cn/books/prometheus_practice" target="_blank" rel="noopener">prometheus-实战篇</a></p><p><a href="https://yunlzheng.gitbook.io/prometheus-book/introduction" target="_blank" rel="noopener">prometheus-操作篇</a></p><p><a href="https://www.kancloud.cn/huyipow/prometheus" target="_blank" rel="noopener">prometheus以优雅的姿势监控kubernetes</a></p>]]></content>
      
      
      <categories>
          
          <category> promethus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> promethus-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>promethus-常用篇</title>
      <link href="/2019/09/25/promethus-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/promethus-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="1-模版案例"><a href="#1-模版案例" class="headerlink" title="1.模版案例"></a>1.模版案例</h4><pre><code>scrape_configs:  - job_name:       &#39;example-random&#39;    # Override the global default and scrape targets from this job every 5 seconds.    scrape_interval: 5s    static_configs:      - targets: [&#39;localhost:8070&#39;, &#39;localhost:8071&#39;]        labels:          group: &#39;production&#39;      - targets: [&#39;localhost:8072&#39;]        labels:          group: ‘canary&#39;</code></pre><h4 id="2-基本使用"><a href="#2-基本使用" class="headerlink" title="2.基本使用"></a>2.基本使用</h4><pre><code>查看进程端口：    pgrep -f prometheus解决端口占用：          lsof -i :9090     sudo kill -9  prot    启动进程：     ./prometheus --config.file=prometheus.yml</code></pre>]]></content>
      
      
      <categories>
          
          <category> promethus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> promethus-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop-问题篇</title>
      <link href="/2019/09/25/hadoop-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/09/25/hadoop-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="hadoop-streaming"><a href="#hadoop-streaming" class="headerlink" title="hadoop streaming"></a>hadoop streaming</h4><h5 id="执行-hadoop-streaming-的时候，加载了太多的文件而导致-内存溢出错误：java-lang-OutOfMemoryError-Java-heap-space"><a href="#执行-hadoop-streaming-的时候，加载了太多的文件而导致-内存溢出错误：java-lang-OutOfMemoryError-Java-heap-space" class="headerlink" title="执行 hadoop streaming 的时候，加载了太多的文件而导致 内存溢出错误：java.lang.OutOfMemoryError:Java heap space"></a>执行 hadoop streaming 的时候，加载了太多的文件而导致 内存溢出错误：java.lang.OutOfMemoryError:Java heap space</h5><pre><code>解决方法：对Mapper/Reducer阶段JVM堆内存溢出参数调优 Maper: mapreduce.map.java.opts=-Xmx2048m(默认参数，表示jvm堆内存,注意是mapreduce不是mapred)mapreduce.map.memory.mb=2304(container的内存）Reducer:mapreduce.reduce.java.opts=-Xmx2048m(默认参数，表示jvm堆内存)mapreduce.reduce.memory.mb=2304(container的内存)注意：因为在yarn container这种模式下，map/reduce task是运行在Container之中的，所以上面提到的mapreduce.map(reduce).memory.mb大小都大于mapreduce.map(reduce).java.opts值的大小。mapreduce.{map|reduce}.java.opts能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的memory.mb，因为需要为java code等预留些空间</code></pre><h5 id="MapReduce保持以下异常信息时："><a href="#MapReduce保持以下异常信息时：" class="headerlink" title="MapReduce保持以下异常信息时："></a>MapReduce保持以下异常信息时：</h5><p>报错信息：</p><pre><code>Container [pid=81565,containerID=container_1489372757489_79086_01_000031] is running beyond physical memory limits. Current usage: 1.8 GB of 1.8 GB physical memory used; 3.4 GB of 8.8 GB virtual memory used. Killing container.</code></pre><p>解决方案：</p><pre><code>说明你的代码存在内存泄露的风险，尝试优化代码，如果确定不存在内存泄露的情况，可以在你代码里设置以下参数，增加map端或者reduce端的内存大小：mapreduce.map.memory.mb=3072mapreduce.reduce.memory.mb=3072</code></pre><h5 id="Mapreduce在运行时，在map阶段出现以下内存溢出问题：Error-Java-heap-space"><a href="#Mapreduce在运行时，在map阶段出现以下内存溢出问题：Error-Java-heap-space" class="headerlink" title="Mapreduce在运行时，在map阶段出现以下内存溢出问题：Error: Java heap space"></a>Mapreduce在运行时，在map阶段出现以下内存溢出问题：Error: Java heap space</h5><p>解决方案：</p><pre><code>提交作业时增加以下两个参数设置，可以根据程序使用内存的大小调整参数的大小：-Dmapreduce.map.memory.mb=3072 -Dmapred.map.child.java.opts=-Xmx3072m</code></pre><h5 id="如何设置MR作业推测执行"><a href="#如何设置MR作业推测执行" class="headerlink" title="如何设置MR作业推测执行"></a>如何设置MR作业推测执行</h5><pre><code>mapreduce.map.speculative=true// 打开map端推测执行mapreduce.reduce.speculative=true// 打开reduce端推测执行推测执行的原理：    当某一个map或者reduce执行比较慢时，集群会再开启一个计算单元，    计算该map或者reduce正在计算的数据，哪一个先计算完成，就使用先计算完成的结果，    没有计算完成的map或者reduce会被kill掉。开启推测执行，有一点需要注意下：&gt; map端或者reduce端不能产生结果性的数据。&gt; “结果性的数据”，没有一个明确性的定义，这里举个栗子：map端或者reduce端往目标数据库（例如RDS）里写入了数据，这就是产生了结果性的数据。</code></pre>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop-常用篇</title>
      <link href="/2019/09/25/hadoop-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/hadoop-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="hadoop-命令"><a href="#hadoop-命令" class="headerlink" title="hadoop 命令"></a>hadoop 命令</h4><blockquote><p>cat</p></blockquote><pre><code>将路径指定文件的内容输出到stdout。示例：  hadoop fs -cat hdfs://host1:port1/file1hdfs://host2:port2/file2  hadoop fs -cat file:///file3 /user/hadoop/file4</code></pre><blockquote><p>dus</p></blockquote><pre><code>  使用方法：hadoop fs -dus &lt;args&gt;显示文件的大小</code></pre><blockquote><p>test</p></blockquote><pre><code>使用方法：hadoop fs -test -[ezd] URI选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。 -d 如果路径是个目录，则返回1，否则返回0。示例：  hadoop fs -test -e filename</code></pre><blockquote><p>查看.gz 的文件内容</p></blockquote><pre><code>*无需解压整个文件：      hadoop fs -cat /hdfs_location/part-00000.gz | zcat | head -n 20  或者 hadoop fs -cat /hdfs_location/part-00000.gz | zmore*需要解压整个文件：    hadoop fs -text /myfolder/part-r-00024.gz | tail</code></pre><blockquote><p>查看.bz2 的文件内容</p></blockquote><pre><code>  类似查看.gz的方法，只需将zcat换为bzcat， 或者将zmore换为bzmore即可</code></pre><h4 id="yarn-命令"><a href="#yarn-命令" class="headerlink" title="yarn 命令"></a>yarn 命令</h4><h5 id="查看-application列表"><a href="#查看-application列表" class="headerlink" title="查看 application列表 :"></a>查看 application列表 :</h5><pre><code> yarn application -list </code></pre><h5 id="查看单独的logs-日志："><a href="#查看单独的logs-日志：" class="headerlink" title="查看单独的logs 日志："></a>查看单独的logs 日志：</h5><pre><code> yarn logs -applicationId  application_1493700892407_0007 yarn logs -applicationId application_1542870632001_26426 &gt; ./application.log</code></pre><h5 id="查看当前作业的状态："><a href="#查看当前作业的状态：" class="headerlink" title="查看当前作业的状态："></a>查看当前作业的状态：</h5><pre><code> yarn application -status application_1542870632001_26426</code></pre><h5 id="kill-当前作业"><a href="#kill-当前作业" class="headerlink" title="kill 当前作业"></a>kill 当前作业</h5><pre><code> yarn application -kill application_1542870632001_26426</code></pre><p>#####查看yarn-site.xml，确定log配置目录以及集群ip</p><hr><h4 id="yarn-rest-API"><a href="#yarn-rest-API" class="headerlink" title="yarn rest API"></a>yarn rest API</h4><h5 id="作业状态"><a href="#作业状态" class="headerlink" title="作业状态"></a>作业状态</h5><h5 id="资源使用情况"><a href="#资源使用情况" class="headerlink" title="资源使用情况"></a>资源使用情况</h5><h5 id><a href="#" class="headerlink" title></a></h5><h4 id="hadoop-集群配置"><a href="#hadoop-集群配置" class="headerlink" title="hadoop 集群配置"></a>hadoop 集群配置</h4><h5 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h5><pre><code>相关参数：key : mapreduce.jobhistory.max-age-msvalue : 2592000000 (30天)descrption：负责清理hdfs路径下日志 /var/hadoop/mapred/mr-history/donekey : yarn.log-aggregation.retain-secondsvalue : 2592000 （30天）description：负责清理hdfs路径下日志 /var/hadoop/yarn/${user}/logs服务器上的本地日志在任务执行完进行日志聚合之后会自动进行删除，不过老数据目前还没有清理。</code></pre><h6 id="日志相关参数"><a href="#日志相关参数" class="headerlink" title="日志相关参数"></a>日志相关参数</h6><pre><code>1) yarn.log-aggregation-enable    是否开启日志聚合功能2) yarn.log-aggregation.retain-seconds    hdfs上的日志保留多久。当前配置路径为：/var/hadoop/yarn/${user}/logs3) yarn.log-aggregation.retain-check-interval-seconds    多长时间检查一次日志，并将满足条件的删除，如果是0或者负数，则为上一个值的1/10，已经配置为：1296000（15天）4) yarn.nodemanager.remote-app-log-dir    前缀目录：/var/hadoop/yarn5) yarn.nodemanager.remote-app-log-dir-suffix    后缀目录：logs6) yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds    每隔一段时间进行日志的聚合，当前配置为：3600。如果配置为-1，则会等待任务执行完还会聚合</code></pre><h6 id="日志聚合参数配置"><a href="#日志聚合参数配置" class="headerlink" title="日志聚合参数配置"></a>日志聚合参数配置</h6><pre><code>yarn.log-aggregation-enable参数说明：是否启用日志聚合功能，日志聚合开启后保存到HDFS上。默认值：falseyarn.log-aggregation.retain-seconds参数说明：聚合后的日志在HDFS上保存多长时间，单位为s。默认值：-1（不启用日志聚合），例如设置为86400，24小时yarn.log-aggregation.retain-check-interval-seconds参数说明：删除任务在HDFS上执行的间隔，执行时候将满足条件的日志删除（超过参数2设置的时间的日志），如果是0或者负数，则为参数2设置值的1/10，上例值在此处为8640s。默认值：-1yarn.nodemanager.log.retain-seconds参数说明：当不启用日志聚合此参数生效，日志文件保存在本地的时间，单位为s。默认值：10800yarn.nodemanager.remote-app-log-dir参数说明：当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚集功能时有效），修改为保存的日志文件夹。默认值：/tmp/logsyarn.nodemanager.remote-app-log-dir-suffix参数说明：远程日志目录子目录名称（启用日志聚集功能时有效）。默认值：logs 日志将被转移到目录${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam}下</code></pre>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>airflow-常用篇</title>
      <link href="/2019/09/25/airflow-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/airflow-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> airflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> airflow-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>azkaban-常用篇</title>
      <link href="/2019/09/25/azkaban-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/azkaban-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> azkaban-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kylin-常用篇</title>
      <link href="/2019/09/25/kylin-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/kylin-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="kylin-配置文件参考"><a href="#kylin-配置文件参考" class="headerlink" title="kylin 配置文件参考"></a>kylin 配置文件参考</h4><pre><code>配置文件位置：/home/hadoop/local/kylin/kylin.properties##### METADATA | ENV ###### The metadata store in hbasekylin.metadata.url=hbase:tb_kylin_metadata@hbase  ##Kylin元数据存储在hbase的数据库hbase:tb_kylin_metadata中### metadata cache sync retry timeskylin.metadata.sync-retries=3### Working folder in HDFS, better be qualified absolute path, make sure user has the right permission to this directory# kylin.env.hdfs-working-dir=/mnt/kylin#### Kylin的中间数据存储在S3中kylin.env.hdfs-working-dir=s3a://mob-emr-test/kylin/workkylin.storage.hbase.cluster-fs=s3a://mob-emr-testkylin.source.hive.redistribute-flat-table=false### DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.#kylin.env=QA##### Kylin节点间的协调基于ZK kylin.env.zookeeper-base-path=/zk-olap-kylin##### SERVER | WEB | RESTCLIENT ###### Kylin server mode, valid value [all, query, job]kylin.server.mode=all### List of web servers in use, this enables one web server instance to sync up with other servers.### Kylin中的cluster节点kylin.server.cluster-servers=ip-172-31-23-207:7070,ip-172-31-28-59:7070,ip-172-31-31-99:7070#kylin.server.cluster-servers=ip-172-31-31-99:7070### Display timezone on UI,format like[GMT+N or GMT-N]kylin.web.timezone=GMT+8kylin.web.dashboard-enabled=true#kylin.cube.cubeplanner.enabled=truekylin.server.query-metrics2-enabled=truekylin.metrics.reporter-query-enabled=truekylin.metrics.reporter-job-enabled=truekylin.metrics.monitor-enabled=true### Timeout value for the queries submitted through the Web UI, in milliseconds#kylin.web.query-timeout=300000##kylin.web.cross-domain-enabled=true###allow user to export query result#kylin.web.export-allow-admin=true#kylin.web.export-allow-other=true### Hide measures in measure list of cube designer, separate by comma#kylin.web.hide-measures=RAW###max connections of one route#kylin.restclient.connection.default-max-per-route=20###max connections of one rest-client#kylin.restclient.connection.max-total=200##### PUBLIC CONFIG ###### Kylin中默认的聚合引擎2表示MR，4表示Sparkkylin.engine.default=4kylin.storage.default=2##kylin.web.hive-limit=20#kylin.web.help.length=4#kylin.web.help.0=start|Getting Started|http://kylin.apache.org/docs/tutorial/kylin_sample.html#kylin.web.help.1=odbc|ODBC Driver|http://kylin.apache.org/docs/tutorial/odbc.html#kylin.web.help.2=tableau|Tableau Guide|http://kylin.apache.org/docs/tutorial/tableau_91.html#kylin.web.help.3=onboard|Cube Design Tutorial|http://kylin.apache.org/docs/howto/howto_optimize_cubes.html#kylin.web.link-streaming-guide=http://kylin.apache.org/#kylin.htrace.show-gui-trace-toggle=false#kylin.web.link-hadoop=#kylin.web.link-diagnostic=#kylin.web.contact-mail=#kylin.server.external-acl-provider=##### SOURCE ####kylin.source.hive.quote-enabled=true### Hive client, valid value [cli, beeline]### Kylin连接Hive数据库的方式kylin.source.hive.client=beeline#kylin.source.hive.client=cli### Absolute path to beeline shell, can be set to spark beeline instead of the default hive beeline on PATHkylin.source.hive.beeline-shell=/data/hadoop-alternative/hive/bin/beeline### Parameters for beeline client, only necessary if hive client is beeline### Kylin连接Hive的hiveserver2服务的参数配置##kylin.source.hive.beeline-params=-u jdbc:hive2://ip-172-31-31-99:10000 -n hadoop --hiveconf hive.security.authorization.sqlstd.confwhitelist.append=&#39;mapreduce.job.*|dfs.*&#39;kylin.source.hive.beeline-params=-u jdbc:hive2://ip-172-31-8-144:10000 -n hadoop --hiveconf hive.security.authorization.sqlstd.confwhitelist.append=&#39;mapreduce.job.*|dfs.*&#39;### While hive client uses above settings to read hive table metadata,## table operations can go through a separate SparkSQL command line, given SparkSQL connects to the same Hive metastore.kylin.source.hive.enable-sparksql-for-table-ops=true###kylin.source.hive.sparksql-beeline-shell=/data/hadoop-alternative/hive/bin/beelinekylin.source.hive.sparksql-beeline-shell=/data/hadoop-alternative/spark/bin/beeline### Kylin通过SparkSQL连接的hiveserver2服务的参数配置##kylin.source.hive.sparksql-beeline-params=-u jdbc:hive2://ip-172-31-31-99:10000 -n hadoop --hiveconf hive.security.authorization.sqlstd.confwhitelist.append=&#39;mapreduce.job.*|dfs.*&#39;kylin.source.hive.sparksql-beeline-params=-u jdbc:hive2://ip-172-31-8-144:10000 -n hadoop --hiveconf hive.security.authorization.sqlstd.confwhitelist.append=&#39;mapreduce.job.*|dfs.*&#39;#kylin.source.hive.keep-flat-table=false### Hive database name for putting the intermediate flat tableskylin.source.hive.database-for-flat-table=db_olap_kylin### Whether redistribute the intermediate flat table before buildingkylin.source.hive.redistribute-flat-table=false#kylin.source.hive.config-override.hive.mapred.mode=nonstrictkylin.source.hive.config-override.mapreduce.job.queuename=dataplatform###### STORAGE ###### The storage for final cube file in hbasekylin.storage.url=hbase### The prefix of hbase tablekylin.storage.hbase.table-name-prefix=TB_KYLIN_### The namespace for hbase storagekylin.storage.hbase.namespace=ns_olap_kylin##kylin.storage.hbase.namespace=NS_OLAP_KYLIN### Compression codec for htable, valid value [none, snappy, lzo, gzip, lz4]kylin.storage.hbase.compression-codec=snappy### HBase Cluster FileSystem, which serving hbase, format as hdfs://hbase-cluster:8020## Leave empty if hbase running on same cluster with hive and mapreduce##kylin.storage.hbase.cluster-fs=### The cut size for hbase region, in GB.kylin.storage.hbase.region-cut-gb=5### The hfile size of GB, smaller hfile leading to the converting hfile MR has more reducers and be faster.## Set 0 to disable this optimization.kylin.storage.hbase.hfile-size-gb=2#kylin.storage.hbase.min-region-count=5kylin.storage.hbase.max-region-count=500##kylin.hbase.default.compression.codec=snappy### Optional information for the owner of kylin platform, it can be your team&#39;s email## Currently it will be attached to each kylin&#39;s htable attribute#kylin.storage.hbase.owner-tag=whoami@kylin.apache.org##kylin.storage.hbase.coprocessor-mem-gb=3### By default kylin can spill query&#39;s intermediate results to disks when it&#39;s consuming too much memory.## Set it to false if you want query to abort immediately in such condition.#kylin.storage.partition.aggr-spill-enabled=true### The maximum number of bytes each coprocessor is allowed to scan.## To allow arbitrary large scan, you can set it to 0.#kylin.storage.partition.max-scan-bytes=3221225472### The default coprocessor timeout is (hbase.rpc.timeout * 0.9) / 1000 seconds,## You can set it to a smaller value. 0 means use default.## kylin.storage.hbase.coprocessor-timeout-seconds=0###### JOB ####kylin.job.scheduler.default=2kylin.job.lock=org.apache.kylin.storage.hbase.util.ZookeeperJobLock### Max job retry on error, default 0: no retrykylin.job.retry=2### Max count of concurrent jobs runningkylin.job.max-concurrent-jobs=10### The percentage of the sampling, default 100%kylin.job.sampling-percentage=70### kylin.job.mr.lib.dir=/data/hadoop-alternative/hive/lib### kylin.job.mr.config.override.mapreduce.map.java.opts=-Xmx4g## kylin.job.mr.config.override.mapreduce.map.memory.mb=4096## kylin.job.mr.config.override.mapreduce.job.queuename=dataplatform### If true, will send email notification on job complete## kylin.job.notification-enabled=true## kylin.job.notification-mail-enable-starttls=true## kylin.job.notification-mail-host=smtp.office365.com## kylin.job.notification-mail-port=587## kylin.job.notification-mail-username=kylin@example.com## kylin.job.notification-mail-password=mypassword## kylin.job.notification-mail-sender=kylin@example.com#kylin.source.kafka.config-override.partition.assignment.strategy=org.apache.kafka.clients.consumer.RoundRobinAssignor##### ENGINE ####kylin.engine.mr.config-override.mapreduce.job.queuename=dataplatformkylin.engine.mr.config-override.mapreduce.map.java.opts=-Xmx4g -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMarkkylin.engine.mr.config-override.mapreduce.map.memory.mb=4096##kylin.engine.mr.lib-dir=/data/hadoop-alternative/hbase/lib#kylin.engine.mr.lib-dir=/data/hadoop-alternative/hive/libkylin.engine.mr.lib-dir=/home/hadoop/local/kylin/ext### Time interval to check hadoop job statuskylin.engine.mr.yarn-check-interval-seconds=10#kylin.engine.mr.reduce-input-mb=500#kylin.engine.mr.min-reducer-number=5kylin.engine.mr.max-reducer-number=300#kylin.engine.mr.mapper-input-rows=500000### Enable dictionary building in MR reducerkylin.engine.mr.build-dict-in-reducer=true### Number of reducers for fetching UHC column distinct valueskylin.engine.mr.uhc-reducer-count=5### Whether using an additional step to build UHC dictionarykylin.engine.mr.build-uhc-dict-in-additional-step=true###### CUBE | DICTIONARY #####kylin.cube.cuboid-scheduler=org.apache.kylin.cube.cuboid.DefaultCuboidScheduler#kylin.cube.segment-advisor=org.apache.kylin.cube.CubeSegmentAdvisor### &#39;auto&#39;, &#39;inmem&#39;, &#39;layer&#39; or &#39;random&#39; for testing#kylin.cube.algorithm=layer### A smaller threshold prefers layer, a larger threshold prefers in-mem#kylin.cube.algorithm.layer-or-inmem-threshold=7##kylin.cube.aggrgroup.max-combination=32768##kylin.snapshot.max-mb=300##kylin.cube.cubeplanner.enabled=true#kylin.cube.cubeplanner.enabled-for-existing-cube=true#kylin.cube.cubeplanner.expansion-threshold=15.0#kylin.cube.cubeplanner.recommend-cache-max-size=200#kylin.cube.cubeplanner.mandatory-rollup-threshold=1000#kylin.cube.cubeplanner.algorithm-threshold-greedy=8#kylin.cube.cubeplanner.algorithm-threshold-genetic=23###### QUERY ###### Controls the maximum number of bytes a query is allowed to scan storage.## The default value 0 means no limit.## The counterpart kylin.storage.partition.max-scan-bytes sets the maximum per coprocessor.#kylin.query.max-scan-bytes=0##kylin.query.cache-enabled=true### Controls extras properties for Calcite jdbc driver## all extras properties should undder prefix &quot;kylin.query.calcite.extras-props.&quot;## case sensitive, default: true, to enable case insensitive set it to false## @see org.apache.calcite.config.CalciteConnectionProperty.CASE_SENSITIVE#kylin.query.calcite.extras-props.caseSensitive=true## how to handle unquoted identity, defualt: TO_UPPER, available options: UNCHANGED, TO_UPPER, TO_LOWER## @see org.apache.calcite.config.CalciteConnectionProperty.UNQUOTED_CASING#kylin.query.calcite.extras-props.unquotedCasing=TO_UPPER## quoting method, default: DOUBLE_QUOTE, available options: DOUBLE_QUOTE, BACK_TICK, BRACKET## @see org.apache.calcite.config.CalciteConnectionProperty.QUOTING#kylin.query.calcite.extras-props.quoting=DOUBLE_QUOTE## change SqlConformance from DEFAULT to LENIENT to enable group by ordinal## @see org.apache.calcite.sql.validate.SqlConformance.SqlConformanceEnum#kylin.query.calcite.extras-props.conformance=LENIENT### TABLE ACL#kylin.query.security.table-acl-enabled=true### Usually should not modify this#kylin.query.interceptors=org.apache.kylin.rest.security.TableInterceptor##kylin.query.escape-default-keyword=false### Usually should not modify this#kylin.query.transformers=org.apache.kylin.query.util.DefaultQueryTransformer,org.apache.kylin.query.util.KeywordDefaultDirtyHack##### SECURITY ###### Spring security profile, options: testing, ldap, saml## with &quot;testing&quot; profile, user can use pre-defined name/pwd like KYLIN/ADMIN to loginkylin.security.profile=ldap### Admin roles in LDAP, for ldap and saml#kylin.security.acl.default-role=ROLE_ANALYST,ROLE_MODELERkylin.security.acl.default-role=ROLE_ADMIN#kylin.security.acl.admin-role=data_platformkylin.security.acl.admin-role=common### LDAP authentication configurationkylin.security.ldap.connection-server=ldap://acsldap.mobvista.com:389kylin.security.ldap.connection-username=uid=gateway,ou=open,dc=mobvista,dc=comkylin.security.ldap.connection-password=5U0dB4Ir9g3mdJgUdiwrFUhXZZkUDPNE7s7OvSXioSk=### LDAP user account directory;kylin.security.ldap.user-search-base=ou=acs,dc=mobvista,dc=comkylin.security.ldap.user-search-pattern=uid={0}kylin.security.ldap.user-group-search-base=ou=groups,dc=mobvista,dc=com#kylin.security.ldap.user-group-search-filter=(&amp;(uid=gateway)(|(memberOf=cn=data_platform,ou=groups,dc=mobvista,dc=com)(memberOf=cn=data_platform_admin,ou=groups,dc=mobvista,dc=com)))### LDAP service account directorykylin.security.ldap.service-search-base=ou=acs,dc=mobvista,dc=comkylin.security.ldap.service-search-pattern=uid={0}kylin.security.ldap.service-group-search-base=ou=groups,dc=mobvista,dc=com#### SAML configurations for SSO## SAML IDP metadata file location#kylin.security.saml.metadata-file=classpath:sso_metadata.xml#kylin.security.saml.metadata-entity-base-url=https://localhost:7070/kylin#kylin.security.saml.keystore-file=classpath:samlKeystore.jks#kylin.security.saml.context-scheme=https#kylin.security.saml.context-server-name=localhost#kylin.security.saml.context-server-port=443#kylin.security.saml.context-path=/kylin#kylin.server.auth-user-cache.expire-seconds=500kylin.server.auth-user-cache.max-entries=0#### SPARK ENGINE CONFIGS ###### Hadoop conf folder, will export this as &quot;HADOOP_CONF_DIR&quot; to run spark-submit## This must contain site xmls of core, yarn, hive, and hbase in one folderkylin.env.hadoop-conf-dir=/data/hadoop-alternative/hadoop/etc/hive-hadoop## Enable Spark for Distinctkylin.engine.spark-fact-distinct=false### Estimate the RDD partition numbers#kylin.engine.spark.rdd-partition-cut-mb=10### Minimal partition numbers of rdd#kylin.engine.spark.min-partition=5### Max partition numbers of rdd#kylin.engine.spark.max-partition=5000### Spark conf (default is in spark/conf/spark-defaults.conf)#kylin.engine.spark-conf.spark.master=yarn#kylin.engine.spark-conf.spark.submit.deployMode=cluster#kylin.engine.spark-conf.spark.yarn.queue=default#kylin.engine.spark-conf.spark.driver.memory=4G#kylin.engine.spark-conf.spark.executor.memory=4G#kylin.engine.spark-conf.spark.executor.instances=40#kylin.engine.spark-conf.spark,yarn.executor.memoryOverhead=1024#kylin.engine.spark-conf.spark.shuffle.service.enabled=true#kylin.engine.spark-conf.spark.eventLog.enabled=true#kylin.engine.spark-conf.spark.eventLog.dir=hdfs\:///mnt/kylin/spark-history#kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///mnt/kylin/spark-history#kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false#kylin.engine.spark-conf.spark.kryo.registrationRequired=false### Spark conf for specific job#kylin.engine.spark-conf-mergedict.spark.executor.memory=6G#kylin.engine.spark-conf-mergedict.spark.memory.fraction=0.2### manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar at runtime#kylin.engine.spark-conf.spark.yarn.archive=hdfs://datacluster/mnt/kylin/dataplatform/spark_lib.zip#kylin.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec#kylin.engine.spark-conf.spark.master=yarnkylin.engine.spark-conf.spark.submit.deployMode=clusterkylin.engine.spark-conf.spark.shuffle.service.enabled=truekylin.engine.spark-conf.spark.dynamicAllocation.enabled=truekylin.engine.spark-conf.spark.dynamicAllocation.minExecutors=10kylin.engine.spark-conf.spark.dynamicAllocation.maxExecutors=150kylin.engine.spark-conf.spark.dynamicAllocation.initialExecutors=10kylin.engine.spark-conf.spark.dynamicAllocation.executorIdleTimeout=180kylin.engine.spark-conf.spark.yarn.queue=dataplatformkylin.engine.spark-conf.spark.driver.memory=6Gkylin.engine.spark-conf.spark.executor.memory=8Gkylin.engine.spark-conf.spark.executor.cores=1kylin.engine.spark-conf.spark.executor.instances=100kylin.engine.spark-conf.spark.executor.memoryOverhead=4096kylin.engine.spark-conf.spark.eventLog.enabled=falsekylin.engine.spark-conf.spark.hadoop.dfs.replication=2kylin.engine.spark-conf.spark.hadoop.mapreduce.output.fileoutputformat.compress=truekylin.engine.spark-conf.spark.hadoop.mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodeckylin.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec#kylin.engine.spark-conf.spark.eventLog.dir=hdfs\:///mnt/kylin/spark-historykylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///mnt/kylin/spark-historykylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=falsekylin.engine.spark-conf.spark.kryo.registrationRequired=falsekylin.engine.spark-conf.spark.default.parallelism=500kylin.engine.spark-conf.spark.speculation=truekylin.engine.spark-conf.spark.speculation.multiplier=2kylin.engine.spark-conf.spark.speculation.quantile=0.5#kylin.engine.spark-conf.spark.network.timeout=600kylin.engine.spark-conf.spark.rpc.askTimeout=600kylin.engine.spark-conf.spark.rpc.lookupTimeout=600kylin.engine.spark-conf.spark.executor.heartbeatInterval=300kylin.engine.spark-conf.spark.shuffle.io.maxRetries=30kylin.engine.spark-conf.spark.shuffle.io.retryWait=60kylin.engine.spark-conf.spark.shuffle.registration.timeout=30000kylin.engine.spark-conf.spark.shuffle.file.buffer=64kkylin.engine.spark-conf.spark.shuffle.memoryFraction=0.3kylin.engine.spark-conf.spark.shuffle.compress=true##kylin.engine.spark-conf.spark.shuffle.manager=hashkylin.engine.spark-conf.spark.shuffle.consolidateFiles=truekylin.engine.spark-conf.spark.sql.shuffle.partitions=1000### uncomment for HDP#kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current#kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current#kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current#kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=3.0.1.0-187kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=3.0.1.0-187kylin.engine.spark-conf.spark.executor.extraJavaOptions=&quot;-Dhdp.version=3.0.1.0-187 -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark&quot;###### QUERY PUSH DOWN ######kylin.query.pushdown.runner-class-name=org.apache.kylin.query.adhoc.PushDownRunnerJdbcImpl###kylin.query.pushdown.update-enabled=false##kylin.query.pushdown.jdbc.url=jdbc:hive2://sandbox:10000/default##kylin.query.pushdown.jdbc.driver=org.apache.hive.jdbc.HiveDriver##kylin.query.pushdown.jdbc.username=hive##kylin.query.pushdown.jdbc.password=###kylin.query.pushdown.jdbc.pool-max-total=8##kylin.query.pushdown.jdbc.pool-max-idle=8##kylin.query.pushdown.jdbc.pool-min-idle=0##### JDBC Data Source##kylin.source.jdbc.connection-url=##kylin.source.jdbc.driver=##kylin.source.jdbc.dialect=##kylin.source.jdbc.user=##kylin.source.jdbc.pass=##kylin.source.jdbc.sqoop-home=##kylin.source.jdbc.filed-delimiter=</code></pre>]]></content>
      
      
      <categories>
          
          <category> kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kylin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kylin-资料篇</title>
      <link href="/2019/09/25/kylin-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/kylin-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<ol><li><a href="http://kylin.apache.org/" target="_blank" rel="noopener">Kylin官网</a> | <a href="http://kylin.apache.org/cn/" target="_blank" rel="noopener">中文</a></li><li><a href="http://kylin.apache.org/cn/docs23/install/configuration.html" target="_blank" rel="noopener">Kylin配置</a></li><li><a href="http://kylin.apache.org/cn/docs23/tutorial/kylin_sample.html" target="_blank" rel="noopener">Cube样例</a></li><li><a href="http://kylin.apache.org/cn/docs23/howto/howto_optimize_build.html" target="_blank" rel="noopener">Cube构建</a></li><li><a href="http://lxw1234.com/archives/category/kylin" target="_blank" rel="noopener">Kylin参考</a></li><li><a href="https://github.com/Kyligence/calcite/releases" target="_blank" rel="noopener">Kyllin-Calcite</a></li><li><a href="http://kylin.apache.org/docs/howto/howto_use_restapi.html" target="_blank" rel="noopener">Kylin-API参考</a></li><li><a href="https://github.com/apache/kylin" target="_blank" rel="noopener">Kylin的Github</a></li><li><a href="https://builds.apache.org/job/Kylin-Master-JDK-1.8/" target="_blank" rel="noopener">Kylin的Jenkins</a></li><li><a href="https://issues.apache.org/jira/projects/KYLIN/issues/KYLIN-3319?filter=allopenissues" target="_blank" rel="noopener">Kylin的jira链接</a></li><li><a href="http://kylin.apache.org/cn/development/index.html" target="_blank" rel="noopener">Kylin开发指南</a></li><li><a href="http://kylin.apache.org/cn/development/howto_package.html" target="_blank" rel="noopener">Kylin打包二进制文件</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kylin-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kylin-学习篇</title>
      <link href="/2019/09/25/kylin-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/kylin-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>官网:<a href="http://kylin.apache.org/" target="_blank" rel="noopener">链接</a></p><h4 id="KOD参考链接"><a href="#KOD参考链接" class="headerlink" title="KOD参考链接"></a>KOD参考链接</h4><p>基于Druid的Kylin存储引擎实践: <a href="https://mp.weixin.qq.com/s/y0wxLqtCkYcc-kLMmtawYA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/y0wxLqtCkYcc-kLMmtawYA</a><br>Kylin Cube构建原理: <a href="https://blog.bcmeng.com/post/kylin-cube.html" target="_blank" rel="noopener">https://blog.bcmeng.com/post/kylin-cube.html</a><br>Druid Storage原理: <a href="https://blog.bcmeng.com/post/druid-storage.html" target="_blank" rel="noopener">https://blog.bcmeng.com/post/druid-storage.html</a><br>Kylin On Druid Storage原理与实践: <a href="https://blog.bcmeng.com/post/kylin-on-druid-storage.html" target="_blank" rel="noopener">https://blog.bcmeng.com/post/kylin-on-druid-storage.html</a> </p>]]></content>
      
      
      <categories>
          
          <category> kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kylin-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>superset-常用篇</title>
      <link href="/2019/09/25/superset-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/superset-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> superset </category>
          
      </categories>
      
      
        <tags>
            
            <tag> superset-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>superset-资料篇</title>
      <link href="/2019/09/25/superset-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/superset-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> superset </category>
          
      </categories>
      
      
        <tags>
            
            <tag> superset-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zeppelin-学习篇</title>
      <link href="/2019/09/25/zeppelin-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/zeppelin-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>zeppelin-资料篇</title>
      <link href="/2019/09/25/zeppelin-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/zeppelin-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>clickhouse-学习篇</title>
      <link href="/2019/09/25/clickhouse-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/clickhouse-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p> ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)。</p><p><a href="https://clickhouse.yandex/docs/zh/" target="_blank" rel="noopener">ClickHouse官方中文文档</a><br>| <a href="https://clickhouse.yandex/docs/en/" target="_blank" rel="noopener">ClickHouse官方英文文档</a></p><p><a href="https://clickhouse.yandex/benchmark.html" target="_blank" rel="noopener">Performance comparison of analytical DBMS</a></p><h4 id="OLAP场景的关键特征"><a href="#OLAP场景的关键特征" class="headerlink" title="OLAP场景的关键特征"></a>OLAP场景的关键特征</h4><p>大多数是读请求<br>数据总是以相当大的批(&gt; 1000 rows)进行写入<br>不修改已添加的数据<br>每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列<br>宽表，即每个表包含着大量的列<br>较少的查询(通常每台服务器每秒数百个查询或更少)<br>对于简单查询，允许延迟大约50毫秒<br>列中的数据相对较小： 数字和短字符串(例如，每个URL 60个字节)<br>处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行）<br>事务不是必须的<br>对数据一致性要求低<br>每一个查询除了一个大表外都很小<br>查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中</p><p>列式数据库更适合于OLAP场景(对于大多数查询而言，处理速度至少提高了100倍)，下面详细解释了原因(通过图片更有利于直观理解）：</p><p>行式：<br><img src="https://clickhouse.yandex/docs/zh/images/row_oriented.gif" alt></p><p>列式:<br><img src="https://clickhouse.yandex/docs/zh/images/column_oriented.gif" alt></p><h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><p>官方测试工具：clickhouse benchmark<br>远程测试工具：ab -n 50 -c 10 ‘<a href="http://ip:8123/&#39;" target="_blank" rel="noopener">http://ip:8123/&#39;</a></p><p>Spark 读取性能测试<br>1.m_sys_mode（340个字段）：</p><p>写入一天 m_sys_model，每小时约 1000万~2000万条记录，平均耗费时长约 2min ~ 3min</p><p>读取一小时数据耗费时长约 1min ~ 2min，20w/s</p><p>2.ods_dmp_user_info（9个字段，23亿条记录）：</p><p>写入耗费时长约 30~40min</p><p>通过Spark 读取性能较低，读取 20w/s，三台机器CPU长时间飙至 1500%，影响其他分析类查询，故不适合做大批量读操作。</p><p>Cassandra 读取性能测试<br>40~50w/s<br>num-executors 50 executor-cores 4 executor-memeory 8G</p><p>16min<br>num-executors 50 executor-cores 4 executor-memeory 8G</p><p>[安装部署参考](<a href="https://clickhouse.yandex/docs/en/getting_started/）" target="_blank" rel="noopener">https://clickhouse.yandex/docs/en/getting_started/）</a></p>]]></content>
      
      
      <categories>
          
          <category> clickhouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clickhouse-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>maven-问题篇</title>
      <link href="/2019/09/25/maven-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/09/25/maven-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="maven-thread-“main”-java-lang-StackOverflowError"><a href="#maven-thread-“main”-java-lang-StackOverflowError" class="headerlink" title="maven thread “main” java.lang.StackOverflowError"></a>maven thread “main” java.lang.StackOverflowError</h4><blockquote><p>保存信息：</p></blockquote><pre><code>maven thread &quot;main&quot; java.lang.StackOverflowErrorat org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext(AbstractArchiver.java:481)</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>方法一（亲测可以）：  https://www.jianshu.com/p/5444c660ea87  Maven -&gt; Runner -&gt;VM Options ,设置-Xmx512m -Xms128m -Xss2m即可方法二：在pom.xml 中加上&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>maven-常用篇</title>
      <link href="/2019/09/25/maven-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/maven-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="maven-常用命令"><a href="#maven-常用命令" class="headerlink" title="maven 常用命令"></a>maven 常用命令</h4><h5 id="打包并跳过测试"><a href="#打包并跳过测试" class="headerlink" title="打包并跳过测试"></a>打包并跳过测试</h5><pre><code>mvn package -Dmaven.test.skip=true</code></pre><h5 id="编译调试"><a href="#编译调试" class="headerlink" title="编译调试"></a>编译调试</h5><pre><code>mvn -X package -Dmaven.test.skip=true如果你想控制Maven的日志级别，你可以使用下面三个命令行选项：-e， --errors 产生执行错误相关消息-X， --debug 产生执行调试信息-q， --quiet 仅仅显示错误</code></pre><h5 id="安装jar"><a href="#安装jar" class="headerlink" title="安装jar"></a>安装jar</h5><pre><code>mvn install:install-file -Dfile=jar包的位置 -DgroupId=org.apache.flink -DartifactId=flink-connector-cassandra_2.11 -Dversion=1.8.0 -Dpackaging=jarmvn install --define maven.test.skip=true</code></pre><h4 id="mave-插件使用"><a href="#mave-插件使用" class="headerlink" title="mave 插件使用"></a>mave 插件使用</h4><h5 id="maven-shade-plugin"><a href="#maven-shade-plugin" class="headerlink" title="maven-shade-plugin"></a>maven-shade-plugin</h5><pre class=" language-xml"><code class="language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-shade-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                <span class="token comment" spellcheck="true">&lt;!-- put your configurations here --></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>                 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>                     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>                     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>shade<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>                 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><blockquote><p>将该工程依赖的部分 Jar 包 include/exclude 掉</p></blockquote><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-shade-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>shade<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactSet</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>excludes</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>classworlds:classworlds<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>junit:junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>jmock:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>*:xml-apis<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>org.apache.maven:lib:tests<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>log4j:log4j:jar:<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>excludes</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactSet</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><blockquote><p>将依赖的某个 Jar 包内部的类或者资源 include/exclude 掉。</p></blockquote><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-shade-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>shade<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filters</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filter</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifact</span><span class="token punctuation">></span></span>junit:junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifact</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>includes</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>include</span><span class="token punctuation">></span></span>junit/framework/**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>include</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>include</span><span class="token punctuation">></span></span>org/junit/**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>include</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>includes</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>excludes</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>org/junit/experimental/**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>org/junit/runners/**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>excludes</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filter</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filter</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifact</span><span class="token punctuation">></span></span>*:*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifact</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>excludes</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.SF<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.DSA<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclude</span><span class="token punctuation">></span></span>META-INF/*.RSA<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclude</span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>excludes</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filter</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filters</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><blockquote><p>自动将所有不使用的类全部排除掉</p></blockquote><pre class=" language-xml"><code class="language-xml">  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-shade-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>shade<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>minimizeJar</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>minimizeJar</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><blockquote><p>通过设置 MainClass 创建一个可执行 Jar 包。</p></blockquote><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-shade-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.4.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>shade<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>transformers</span><span class="token punctuation">></span></span>             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>transformer</span> <span class="token attr-name">implementation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.apache.maven.plugins.shade.resource.ManifestResourceTransformer<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mainClass</span><span class="token punctuation">></span></span>org.sonatype.haven.HavenCli<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mainClass</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>transformer</span><span class="token punctuation">></span></span>              <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>transformers</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span></code></pre><h5 id><a href="#" class="headerlink" title></a></h5>]]></content>
      
      
      <categories>
          
          <category> maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-资料篇</title>
      <link href="/2019/09/25/redis-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/redis-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive-常用篇</title>
      <link href="/2019/09/25/hive-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/hive-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>redis-常用篇</title>
      <link href="/2019/09/25/redis-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/25/redis-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>java-guava篇</title>
      <link href="/2019/09/25/java-guava%E7%AF%87/"/>
      <url>/2019/09/25/java-guava%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java-多线程篇</title>
      <link href="/2019/09/25/java-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AF%87/"/>
      <url>/2019/09/25/java-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>java-JVM篇</title>
      <link href="/2019/09/25/java-JVM%E7%AF%87/"/>
      <url>/2019/09/25/java-JVM%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-资料篇</title>
      <link href="/2019/09/25/mysql-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/mysql-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive-资料篇</title>
      <link href="/2019/09/25/hive-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/09/25/hive-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive-问题篇</title>
      <link href="/2019/09/25/hive-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/09/25/hive-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>####1.hql出现类似以下异常</p><p>异常信息：</p><pre><code>Starting to launch local task to process map join; maximum memory = 301465600 Execution failed with exit status: 2</code></pre><p>解决方案：尝试增加以下设置：</p><pre><code>set hive.auto.convert.join=false;关闭自动转化MapJoin，默认为true;set hive.ignore.mapjoin.hint=false; 关闭忽略mapjoin的hints（不忽略，hints有效），默认为true(忽略hints)。</code></pre><p>####2.执行hql的时，加载了太多的文件而导致 内存溢出错误：java.lang.OutOfMemoryError:Java heap space</p><p>解决方法： </p><pre><code>在你的程序里设置以下环境变量值：export HADOOP_CLIENT_OPTS=&quot;-Xmx2048m&quot;</code></pre><p>####3.hive在切换或者升级metadata元数据信息库时，增加分区任务可能会出现以下问题：</p><pre><code>任务执行是成功的，但是切换了metadata数据库后，发现该分区数据丢失。原因：造成此问题的原因是由于执行成功的任务把元数据信息写入到旧的元数据库里，切换时没有同步到新的元数据信息库里。解决方法：切换新的元数据信息库后，需要人工检查下分区数据信息，保证分区数据不丢失。</code></pre><p>####4.Failed to create temp property file cause: java.io.IOException: 设备上没有空间</p><pre><code>原因：服务器的磁盘空间已满。解决方法：清理磁盘。</code></pre><p>####5.自定义SerDe的使用方法：</p><pre><code>创建SerDe:mobvista.dataplatform.text.TextSerde hive表的使用方法：CREATE EXTERNAL TABLE dm_campaign_tag_likePARTITIONED BY (year string,month string,day string) ROW FORMAT SERDE &#39;com.text.TextSerde&#39; WITH SERDEPROPERTIES (&#39;schema.url&#39;=&#39;s3://tag.avsc&#39;)STORED AS TEXTFILELOCATION &#39;s3://DataWareHouse/data/dwh/dm_campaign_tag_like&#39; TBLPROPERTIES (&#39;schema.url&#39;=&#39;s3://DataWareHouse/schema/dwh/tag.avsc&#39;);修改hive表的属性：ALTER TABLE ods_adn_trackingnew_click_like SET TBLPROPERTIES (&#39;schema.url&#39; = &#39;s3://click.avsc&#39;);修改表的serde和属性ALTER TABLE ods_adn_trackingnew_click_like SET SERDE &#39;mobvista.dataplatform.text.TextSerde&#39;;ALTER TABLE ods_adn_trackingnew_click_like SET SERDEPROPERTIES (&#39;schema.url&#39; = &#39;s3://DataWareHouse/schema/dwh/ods_adn_trackingv3_click/ods_adn_trackingv3_click.avsc&#39;);修改表分区的serde属性ALTER TABLE ods_adn_trackingnew_click_like partition(yyyy=&#39;2017&#39;,mm=&#39;06&#39;,dd=&#39;16&#39;,re=&#39;virginia&#39;,hh=&#39;00&#39;) SET SERDEPROPERTIES (&#39;schema.url&#39; = &#39;s3://click.avsc&#39;);</code></pre><p>####6.hive添加分区的时候偶尔会出现以下问题：</p><p>case 1:</p><pre><code>Caused by: java.sql.BatchUpdateException: Duplicate entry &#39;admin&#39; for key &#39;ROLEENTITYINDEX&#39;这主要是由于hive的metastore数据被修改造成的。只需要把失败的任务重新执行即可，这是因为metastore在并发比较高时，客户端无法在超时时间内获取到结果信息而导致的异常，重新执行即可。</code></pre><p>case 2:</p><pre><code>Caused by: MetaException(message:Metastore contains multiple versions)首先删除matestore数据库当中重复的版本信息：mysql -ip -u* -p* -Dhive_db -e &#39;delete from VERSION where VER_ID &lt;&gt; 1;commit;&#39;然后重新执行失败的任务，错误原因同case1一样，也是由于metastore在并发比较高时，客户端无法获取到hive的版本信息，从而插入VERSION表一条版本信息数据，而造成的版本信息冗余，从而导致的异常。以上两种case偶尔会发生，说明在某一个时间节点，metastore的性能达到了瓶颈，需要优化和提升metastore的性能，可以从以下几点入手解决该问题：1，提升metastore数据库服务器的性能指标，例如：io，memory等；2，优化metastore的写入和查询性能，可以从缓存和事务入手；3，修改hive客户端配置，hive的conf配置文件里有一项是配置读取metastore的超时时间的，参数项为：hive.stats.jdbc.timeout，默认参数值为30s，可以适当放到该参数项的值。</code></pre><p>####7.Caused by: java.lang.ClassNotFoundException: org.apache.commons.logging.impl.SLF4JLocationAwareLog</p><pre><code>造成该问题的主要原因是由于系统环境变量里找不到这个jar：jcl-over-slf4j-1.7.16.jar需要在执行hql之前手动添加下,添加命令为：add jar /data/hadoop-alternative/spark/jars/jcl-over-slf4j-1.7.16.jar;</code></pre><p>####8.如何使用hive模拟mysql的自增id</p><pre><code>INSERT INTO TABLE customer_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.customer_number) + t2.sk_max, t1.customer_number, t1.customer_name FROM ( SELECT t1.* FROM customer t1 LEFT JOIN customer_dim t2 ON t1.customer_number = t2.customer_number WHERE t2.customer_sk IS NULL ) t1 CROSS JOIN (SELECT COALESCE(MAX(customer_sk),0) sk_max FROM customer_dim) t2;</code></pre>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive-学习篇</title>
      <link href="/2019/09/25/hive-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/hive-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-学习篇</title>
      <link href="/2019/09/25/mysql-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/mysql-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>linux-学习篇</title>
      <link href="/2019/09/25/linux-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/09/25/linux-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="学习资料参考"><a href="#学习资料参考" class="headerlink" title="学习资料参考"></a>学习资料参考</h4><blockquote><p><a href="https://my-study-linux.readthedocs.io/en/latest/%E9%A9%AC%E5%93%A5%E6%95%99%E8%82%B2%E5%86%85%E9%83%A8%E8%AE%B2%E4%B9%89/index.html" target="_blank" rel="noopener">马哥教育内部讲义</a></p></blockquote><blockquote><p><a href="https://www.bookstack.cn/read/linuxprobe/56c362db0a3501fd.md" target="_blank" rel="noopener">linux就该这么学</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-问题篇</title>
      <link href="/2019/09/25/mysql-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/09/25/mysql-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="报错信息笔记（引用链接）"><a href="#报错信息笔记（引用链接）" class="headerlink" title="报错信息笔记（引用链接）"></a>报错信息笔记（引用链接）</h4><blockquote><p>以下链接是各路豪杰整理</p></blockquote><p><a href="http://weikeqin.cn/2017/06/21/mysql-error-notes/" target="_blank" rel="noopener">沧海一粟-MySQL错误笔记</a></p><h4 id="RROR-2002-HY000-Can’t-connect-to-local-MySQL-server-through-socket-‘-tmp-mysql-sock’-2"><a href="#RROR-2002-HY000-Can’t-connect-to-local-MySQL-server-through-socket-‘-tmp-mysql-sock’-2" class="headerlink" title="RROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/tmp/mysql.sock’ (2)"></a>RROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/tmp/mysql.sock’ (2)</h4><blockquote><p>报错信息：</p></blockquote><pre><code>Can&#39;t connect to local MySQL server through socket &#39;/tmp/mysql.sock&#39; (2)</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>主要原因是mysql 服务没有启动解决：mysql.server start</code></pre><hr><h4 id="om-mysql-jdbc-exceptions-jdbc4-CommunicationsException"><a href="#om-mysql-jdbc-exceptions-jdbc4-CommunicationsException" class="headerlink" title="om.mysql.jdbc.exceptions.jdbc4.CommunicationsException:"></a>om.mysql.jdbc.exceptions.jdbc4.CommunicationsException:</h4><blockquote><p>报错信息：</p></blockquote><pre><code>om.mysql.jdbc.exceptions.jdbc4.CommunicationsException:The last packet successfully received from the server was 44,024,462 milliseconds ago.  The last packet sent successfully to the server was 44,024,462 milliseconds ago. is longer than the server configured value of &#39;wait_timeout&#39;. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property &#39;autoReconnect=true&#39; to avoid this problem.</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>1.在连接上添加这个设置&amp;autoReconnect=true&amp;failOverReadOnly=false2.每次使用完记得归还连接</code></pre><hr><h4 id="ERROR-The-server-quit-without-updating-PID-file"><a href="#ERROR-The-server-quit-without-updating-PID-file" class="headerlink" title="ERROR! The server quit without updating PID file"></a>ERROR! The server quit without updating PID file</h4><blockquote><p>报错信息：</p></blockquote><pre><code>$ mysql.server startStarting MySQL. ERROR! The server quit without updating PID file (/usr/local/var/mysql/localhost.pid).</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>0.用sudo 的权限执行。 sudo mysql.server start1.可能是/usr/local/mysql/data/rekfan.pid文件没有写的权限解决方法 ：给予权限，执行 “chown -R mysql:mysql /var/data” “chmod -R 755 /usr/local/mysql/data”  然后重新启动mysqld！2.可能进程里已经存在mysql进程解决方法：用命令“ps -ef|grep mysqld”查看是否有mysqld进程，如果有使用“kill -9  进程号”杀死，然后重新启动mysqld！3.可能是第二次在机器上安装mysql，有残余数据影响了服务的启动。解决方法：去mysql的数据目录/data看看，如果存在mysql-bin.index，就赶快把它删除掉吧，它就是罪魁祸首了。本人就是使用第三条方法解决的 ！http://blog.rekfan.com/?p=1864.mysql在启动时没有指定配置文件时会使用/etc/my.cnf配置文件，请打开这个文件查看在[mysqld]节下有没有指定数据目录(datadir)。解决方法：请在[mysqld]下设置这一行：datadir = /usr/local/mysql/data5.skip-federated字段问题解决方法：检查一下/etc/my.cnf文件中有没有没被注释掉的skip-federated字段，如果有就立即注释掉吧。6.错误日志目录不存在解决方法：使用“chown” “chmod”命令赋予mysql所有者及权限7.selinux惹的祸，如果是centos系统，默认会开启selinux解决方法：关闭它，打开/etc/selinux/config，把SELINUX=enforcing改为SELINUX=disabled后存盘退出重启机器试试。</code></pre><hr><h4 id="Formula-mysql-is-not-installed"><a href="#Formula-mysql-is-not-installed" class="headerlink" title="Formula mysql is not installed"></a>Formula mysql is not installed</h4><blockquote><p>报错信息：</p></blockquote><pre><code>brew services restart mysqlStopping `mysql`... (might take a while)==&gt; Successfully stopped `mysql` (label: homebrew.mxcl.mysql)Error: Formula `mysql` is not installed.</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>    查看mysql 的版本    brew services start mysql@5.7==&gt; Successfully started `mysql@5.7` (label: homebrew.mxcl.mysql@5.7)</code></pre><h4 id="mysql-密码忘记或修改解决方案"><a href="#mysql-密码忘记或修改解决方案" class="headerlink" title="mysql 密码忘记或修改解决方案"></a>mysql 密码忘记或修改解决方案</h4><pre><code>方法1： 用SET PASSWORD命令 首先登录MySQL。 格式：mysql&gt; set password for 用户名@localhost = password(‘新密码’); 例子：mysql&gt; set password for root@localhost = password(‘123’);方法2：用mysqladmin 格式：mysqladmin -u用户名 -p旧密码 password 新密码 例子：mysqladmin -uroot -p123456 password 123方法3：用UPDATE直接编辑user表 首先登录MySQL。 mysql&gt; use mysql; mysql&gt; update user set password=password(‘123’) where user=’root’ and host=’localhost’; mysql&gt; flush privileges;方法4：在忘记root密码的时候，可以这样 以windows为例： 1. 关闭正在运行的MySQL服务。 2. 打开DOS窗口，转到mysql\bin目录。 3. 输入mysqld –skip-grant-tables 回车。–skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证。 4. 再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），转到mysql\bin目录。 5. 输入mysql回车，如果成功，将出现MySQL提示符 &gt;。 6. 连接权限数据库： use mysql; 。 6. 改密码：update user set password=password(“123”) where user=”root”;（别忘了最后加分号） 。 7. 刷新权限（必须步骤）：flush privileges;　。 8. 退出 quit。 9. 注销系统，再进入，使用用户名root和刚才设置的新密码123登录。</code></pre>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zeppelin-常用篇</title>
      <link href="/2019/09/15/zeppelin-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/09/15/zeppelin-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="安装解释器"><a href="#安装解释器" class="headerlink" title="安装解释器"></a>安装解释器</h4><pre><code>#解释器所有安装./bin/install-interpreter.sh --all#解释器可安装列表./bin/install-interpreter.sh --list#解释器安装单独的./bin/install-interpreter.sh --name kylin ./bin/install-interpreter.sh --name livy</code></pre><h4 id="hive-使用"><a href="#hive-使用" class="headerlink" title="hive 使用"></a>hive 使用</h4><h5 id="作业名设置"><a href="#作业名设置" class="headerlink" title="作业名设置"></a>作业名设置</h5><pre><code>```%hiveset mapred.job.name=zeppelin_hive_xianchang.yue;```</code></pre><h5 id><a href="#" class="headerlink" title></a></h5><hr><h4 id="shell-使用"><a href="#shell-使用" class="headerlink" title="shell 使用"></a>shell 使用</h4><h5 id="参数优化"><a href="#参数优化" class="headerlink" title="参数优化"></a>参数优化</h5><pre><code>```%shell set mapred.child.java.opts; mapred.child.java.opts=-Xmx1536m -Xms1536m -Xmn256m -XX:SurvivorRatio=6 -XX:MaxPermSize=128m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80  -XX:GCTimeLimit=90 -XX:GCHeapFreeLimit=10 -XX:ParallelGCThreads=8```</code></pre><h5 id="执行hive"><a href="#执行hive" class="headerlink" title="执行hive"></a>执行hive</h5><pre><code>```%shell export HADOOP_CLIENT_OPTS=&quot;-Xmx4096m $HADOOP_CLIENT_OPTS&quot;hive -e &quot;    select * from test    &quot;&gt;&gt;./template_size.txt```</code></pre><hr><h4 id="spark-使用"><a href="#spark-使用" class="headerlink" title="spark 使用"></a>spark 使用</h4><h5 id="spark-执行优化"><a href="#spark-执行优化" class="headerlink" title="spark 执行优化"></a>spark 执行优化</h5><pre><code>```%spark.confspark.app.name zeppeline_custom_confspark.driver.memory 3gspark.executor.memory 2g    spark.dynamicAllocation.enabled truespark.dynamicAllocation.minExecutors 10spark.dynamicAllocation.maxExecutors 50spark.executor.cores 3spark.shuffle.service.enabled true```    </code></pre><h5 id="spark-Streaming-执行"><a href="#spark-Streaming-执行" class="headerlink" title="spark Streaming 执行"></a>spark Streaming 执行</h5><pre><code>```%spark import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.StreamingContext._import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.Durationimport org.apache.spark.streaming.Secondsval ssc = new StreamingContext(sc, Seconds(1))```</code></pre><h5 id="执行后数据保存文件"><a href="#执行后数据保存文件" class="headerlink" title="执行后数据保存文件"></a>执行后数据保存文件</h5><pre><code>%sparkimport org.apache.spark.sql.types._val sql_str =&quot;select * from test&quot;val df = sqlContext.sql(sql_str)val out_put_path = &quot;s3://tmp/tag_1&quot;df.repartition(1).write.format(&quot;csv&quot;).option(&quot;delimiter&quot;, &quot;\t&quot;).option(&quot;header&quot;, true).option(&quot;inferSchema&quot;, true).mode(&quot;overwrite&quot;).save(out_put_path)</code></pre><h4 id="python-使用"><a href="#python-使用" class="headerlink" title="python 使用"></a>python 使用</h4><h5 id="保存文件"><a href="#保存文件" class="headerlink" title="保存文件"></a>保存文件</h5><pre><code>```%pythonimport commandsprint commands.getoutput(&quot;sh /var/tools/hadoop/download_file.sh&quot;+&quot; &quot;+z.input(&quot;s3下载目录:&quot;)+&quot; &quot;+z.input(&quot;保存目录:如 zhangsan/test01&quot;)+&quot; &quot; +z.input(&quot;保存文件名&quot;))```</code></pre><h5 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h5><pre><code>%pythonimport urllib2data = urllib2.urlopen(&quot;http://zeppeline.com:8060/file/shell/product/shellppp_demo.xls&quot;)for line in data:    print line</code></pre><p>####kyline的安装</p><pre class=" language-shell"><code class="language-shell"></code></pre><h4 id="Livy的集成"><a href="#Livy的集成" class="headerlink" title="Livy的集成"></a>Livy的集成</h4><p>Livy-env.sh</p><pre><code>livy.spark.master = yarn  livy.spark.deploy-mode = cluster  livy.repl.enable-hive-context = true </code></pre><ul><li><strong>livy.server.host</strong> 主机地址，默认为 0.0.0.0；</li><li><strong>livy.server.port</strong> 端口号，默认为 8998；</li><li><strong>livy.server.session.timeout-check</strong> 是否检测会话超时，默认为 true；</li><li><strong>livy.server.session.timeout</strong> 会话超时时间，默认为 1h；</li><li><strong>livy.server.session.state-retain.sec</strong> 已完成会话保留时间，默认为 600s；</li><li><strong>livy.rsc.jars</strong> RSC JAR 包位置，缓存在 HDFS 上，可以加速会话的启动速度；</li><li><strong>livy.repl.jars</strong> REPL JAR 包位置，缓存在 HDFS 上，可以加速会话的启动速度；</li><li><strong>livy.server.yarn.poll-interval</strong> YARN 状态刷新频率，默认为 5s；</li><li><strong>livy.ui.enabled</strong> 是否启动 UI 界面，默认为 true；</li></ul><blockquote><p>部署中遇到的问题：Spark 问题：</p><p><a href="https://www.oipapio.com/question-713829" target="_blank" rel="noopener">https://www.oipapio.com/question-713829</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> zeppelin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zeppelin-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指导思想</title>
      <link href="/2019/08/05/%E6%8C%87%E5%AF%BC%E6%80%9D%E6%83%B3/"/>
      <url>/2019/08/05/%E6%8C%87%E5%AF%BC%E6%80%9D%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>学习方法：<br>  <a href="http://xugongli.club/2018/10/26/how_to_be_proficient_in_some_field/" target="_blank" rel="noopener">3 w 学习</a></p></blockquote><blockquote><p>提问的智慧：<br><a href="https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/master/README-zh_CN.md" target="_blank" rel="noopener">连接</a></p></blockquote><blockquote><p>一万小时定论：<br><a href="https://mp.weixin.qq.com/s/Z05VcYfXp5tExvdqidmkgw" target="_blank" rel="noopener">一万小时定论</a></p></blockquote><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><blockquote><ol><li><a href="https://tech.meituan.com/2018/04/16/study-vs-work.html" target="_blank" rel="noopener">工作中如何做好技术积累</a></li><li><a href="http://openskill.cn/article/488" target="_blank" rel="noopener">学习新技术的10个技巧</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTU2MTQwNA==&mid=2650653854&idx=1&sn=9f83c53094d19ad6da457aeb6c45b3d7&chksm=beffce4d8988475b88e35d9358d37198aa3050d5056899c3e56ba0595c361638a7feecf22a76&scene=21#wechat_redirect" target="_blank" rel="noopener">如何快速处理线上故障</a></li><li><a href></a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 指导思想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase-学习篇</title>
      <link href="/2019/07/29/hbase-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/07/29/hbase-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>####HBase的简介</p><blockquote><ol><li>HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”；</li><li>就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力；</li><li>HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库；</li><li>另一个不同的是HBase基于列的而不是基于行的模式；<br>一张表的列簇不会超过5个，每个列簇中的列数没有限制；</li></ol></blockquote><p>####HBase的特点</p><blockquote><ol><li>容量大：HBase单表可以有上百亿行、百万列，数据矩阵横向和纵向两个维度所支持的数据量级都非常具有弹性；</li><li>面向列：HBase是面向列的存储和权限控制，并支持独立检索。列式存储，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段的时候，能大大减少读取的数据量；</li><li>多版本：HBase每一个列的数据存储有多个Version（version）；</li><li>稀疏性：为空的列并不占用存储空间，表可以设计的非常稀疏；</li><li>扩展性：底层依赖于HDFS；</li><li>高可靠性：WAL机制保证了数据写入时不会因集群异常而导致写入数据丢失，Replication机制保证了在集群出现严重的问题时，数据不会发生丢失或损坏。而且HBase底层使用HDFS，HDFS本身也有备份；</li><li>高性能：底层的LSM数据结构和Rowkey有序排列等架构上的独特设计，使得HBase具有非常高的写入性能。region切分、主键索引和缓存机制使得HBase在海量数据下具备一定的随机读取性能，该性能针对Rowkey的查询能够达到毫秒级别；</li></ol></blockquote><h4 id="HBase架构图"><a href="#HBase架构图" class="headerlink" title="HBase架构图"></a>HBase架构图</h4><p><img src="/2019/07/29/hbase-学习篇/" alt></p><h4 id="HBase各部分的作用"><a href="#HBase各部分的作用" class="headerlink" title="HBase各部分的作用"></a>HBase各部分的作用</h4><h5 id="客户端Client"><a href="#客户端Client" class="headerlink" title="客户端Client"></a>客户端Client</h5><pre><code>  * 整个HBase集群的访问入口；  * 使用HBase RPC机制与HMaster和HRegionServer进行通信；  * 与HMaster进行通信，进行管理类操作；  * 与HRegionServer进行数据读写类操作；  * 包含访问HBase的接口，并维护cache来加快对HBase的访问</code></pre><h5 id="协调服务组件Zookeeper"><a href="#协调服务组件Zookeeper" class="headerlink" title="协调服务组件Zookeeper"></a>协调服务组件Zookeeper</h5><pre><code>  * 保证任何时候，集群中只有一个HMaster；  * 存储所有HRegionServer的寻址入口；  * 实时监控HRegionServer的上线和下线信息，并实时通知给HMaster；  * 存储HBase的schema和table元数据；  * Zookeeper Quorum存储.META.表地址、HMaster地址。</code></pre><h5 id="主节点HMaster"><a href="#主节点HMaster" class="headerlink" title="主节点HMaster"></a>主节点HMaster</h5><pre><code>  * HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master在运行，主要负责Table和Region的管理工作；  * 管理用户对table的增删改查操作。  * 管理HRegionServer的负载均衡，调整Region分布；  * Region Split后，负责新Region的分布；  * 在HRegionServer停机后，负责失效HRegionServer上Region迁移工作。</code></pre><h5 id="Region节点HRegionServer"><a href="#Region节点HRegionServer" class="headerlink" title="Region节点HRegionServer"></a>Region节点HRegionServer</h5><pre><code> HRegionServer：   * 维护HRegion，处理对这些HRegion的IO请求，向HDFS文件系统中读写数据；   * 负责切分在运行过程中变得过大的HRegion；   * Client访问HBase上数据的过程并不需要Master参与（寻址访问Zookeeper和HRegionServer，数据读写访问HRegionServer），   HMaster仅仅维护着table和Region的元数据信息，负载很低。</code></pre><h5 id="HBase-amp-amp-Zookeeper"><a href="#HBase-amp-amp-Zookeeper" class="headerlink" title="HBase &amp;&amp; Zookeeper"></a>HBase &amp;&amp; Zookeeper</h5><pre><code>  * HBase依赖Zookeeper；  * 默认情况下，HBase管理Zookeeper实例，比如，启动或停止Zookeeper；  * HMaster与HRegionServer启动时会向Zookeeper注册。  * Zookeeper的引入使得HMaster不再是单点故障。</code></pre><h4 id="hbase-rowkey-设计"><a href="#hbase-rowkey-设计" class="headerlink" title="hbase rowkey 设计"></a>hbase rowkey 设计</h4><pre><code>HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有以下几种方式：通过get方式，指定rowkey获取唯一一条记录；通过scan方式，设置startRow和stopRow参数进行范围匹配；全表扫描，即直接扫描整张表中所有行记录；较新的hbase还可以通过column和values 进行索引，但是不走rowkey索引速度比较慢；</code></pre><p>#####rowkey 设计三原则</p><h6 id="rowkey长度原则"><a href="#rowkey长度原则" class="headerlink" title="rowkey长度原则"></a>rowkey长度原则</h6><pre><code>rowkey是一个二进制码流，可以是任意字符串，最大长度 64kb ，实际应用中一般为10-100bytes，以 byte[] 形式保存，一般设计成定长；建议越短越好，不要超过16个字节，原因如下：1.数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；2.MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率；3.目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性；</code></pre><h6 id="rowkey散列原则"><a href="#rowkey散列原则" class="headerlink" title="rowkey散列原则"></a>rowkey散列原则</h6><pre><code>1.如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。2.如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。</code></pre><h6 id="rowkey唯一原则"><a href="#rowkey唯一原则" class="headerlink" title="rowkey唯一原则"></a>rowkey唯一原则</h6><pre><code>必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</code></pre><h5 id="什么是热点？"><a href="#什么是热点？" class="headerlink" title="什么是热点？"></a>什么是热点？</h5><pre><code>1.HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan，然而糟糕的rowkey设计是热点的源头。2.热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。3.大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。#为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。</code></pre><hr><p>下面是一些常见的避免热点的方法以及它们的优缺点：</p><h6 id="加盐"><a href="#加盐" class="headerlink" title="加盐"></a>加盐</h6><pre><code>1.这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。2.分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。3.加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。</code></pre><h6 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h6><pre><code>1.哈希会使同一行永远用一个前缀加盐。2.哈希也可以使负载分散到整个集群，但是读却是可以预测的。3.使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据</code></pre><h6 id="反转"><a href="#反转" class="headerlink" title="反转"></a>反转</h6><pre><code>1.第三种防止热点的方法时反转固定长度或者数字格式的rowkey。2.这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。3.这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。#反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题</code></pre><h6 id="时间戳反转"><a href="#时间戳反转" class="headerlink" title="时间戳反转"></a>时间戳反转</h6><pre><code>一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾；例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计rowkey的时候，可以这样设计，[userId反转][Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指定反转后的userId，startRow是[userId反转][000000000000],stopRow是[userId反转][Long.Max_Value - timestamp]，如果需要查询某段时间的操作记录，startRow是[user反转][Long.Max_Value - 起始时间]，stopRow是[userId反转][Long.Max_Value - 结束时间]（达到热点分散，查询又可以支持多样性）。</code></pre><h6 id="其他一些建议"><a href="#其他一些建议" class="headerlink" title="其他一些建议"></a>其他一些建议</h6><pre><code>1.尽量减少行和列的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。2.如果你的rowkey和列名很大，甚至可以和具体的值相比较，那么你将会遇到一些有趣的问题。3.HBase storefiles中的索引（有助于随机访问）最终占据了HBase分配的大量内存，因为具体的值和它的key很大。4.可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。5.压缩也有助于更大的索引。6.列族尽可能越短越好，最好是一个字符，冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好</code></pre>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码库-split分割方式</title>
      <link href="/2019/07/25/%E4%BB%A3%E7%A0%81%E5%BA%93-split%E5%88%86%E5%89%B2%E6%96%B9%E5%BC%8F/"/>
      <url>/2019/07/25/%E4%BB%A3%E7%A0%81%E5%BA%93-split%E5%88%86%E5%89%B2%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h4 id="split-案例"><a href="#split-案例" class="headerlink" title="split 案例"></a>split 案例</h4><pre><code>String address=&quot;上海*上海市*闵行区*吴中路&quot;; String[] splitAddress=address.split(&quot;\\*&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海:上海市:闵行区:吴中路&quot;; String[] splitAddress=address.split(&quot;\:&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海.上海市.闵行区.吴中路&quot;; String[] splitAddress=address.split(&quot;\\.&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海^上海市^闵行区^吴中路&quot;; String[] splitAddress=address.split(&quot;\\^&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海@上海市@闵行区@吴中路&quot;; String[] splitAddress=address.split(&quot;@&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);String address=&quot;上海,上海市,闵行区,吴中路&quot;; String[] splitAddress=address.split(&quot;,&quot;);System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);【2】多个符号作为分隔符  String address=&quot;上海^上海市@闵行区#吴中路&quot;; String[] splitAddress=address.split(&quot;\\^|@|#&quot;); System.out.println(splitAddress[0]+splitAddress[1]+splitAddress[2]+splitAddress[3]);</code></pre>]]></content>
      
      
      <categories>
          
          <category> 代码库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> split </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pulsar-学习篇</title>
      <link href="/2019/07/17/pulsar-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/07/17/pulsar-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.infoq.cn/article/1UaxFKWUhUKTY1t_5gPq" target="_blank" rel="noopener"></a></p>]]></content>
      
      
      <categories>
          
          <category> pulsar </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pulsar-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>广告-学习篇</title>
      <link href="/2019/07/12/%E5%B9%BF%E5%91%8A-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/07/12/%E5%B9%BF%E5%91%8A-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="术语理解"><a href="#术语理解" class="headerlink" title="术语理解"></a>术语理解</h4><p>1.<a href="https://cloud.tencent.com/developer/article/1351052" target="_blank" rel="noopener">dsp系统架构</a></p><h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><blockquote><ol><li>dsp 业务</li></ol></blockquote><blockquote><p>1.1 <a href="https://blog.csdn.net/LW_GHY/article/details/71455535" target="_blank" rel="noopener">美团DSP广告策略实践</a></p></blockquote><pre><code>DSP基本概念介绍：     广告平台：将广告素材分发给下游设备、目标用户，然后跟踪分析用户事件行为，并反馈广告主。     DSP：DemandSidePlatform，需求方平台。即将广告平台或者其他来源的广告素材分发给指定的其他广告平台，并收集分析广告的展示激活信息，反馈广告主</code></pre><h4 id="广告术语："><a href="#广告术语：" class="headerlink" title="广告术语："></a>广告术语：</h4><pre><code>通用术语包名：package name，可以唯一确定一个app。CTR：click-through rate，点击率，点击/展现。Campaign：广告计划，某个客户某个广告在某个区域的宣传，投放单元，比如“MT 安卓台湾”。DSP：需求方平台。广告素材Offer：广告组织单元，商务运营层面的一支广告，约等于一个campaign。有时3S系统内部的同一个UUID会拆分成3~4个campaign，分属于ADN1/2/3，这些campaign属于一个offer。直客单：第一跳是3s，下一跳就是第三方（直接跳转到了第三方）的单子。效果评估CVR：安装/点击。归因：简单理解就是把一个安装归功于某一个点击或者展示的过程。因此有点击归因、展示归因等不同细化叫法。VTA：业务上指代透明卖。透明卖是使用展示匹配安装进行结算。使用DSP的设备和M系统的campaign合成展示，与自然安装量进行匹配，同时vta透明卖的ctr和cvr需要控制在合理的范围内。系统名称fVTA：fake的VTA流量。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 广告 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 广告-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-学习篇</title>
      <link href="/2019/07/12/spark-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/07/12/spark-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h4><blockquote><ol><li><a href="https://blog.csdn.net/liangyihuai/article/details/56676878" target="_blank" rel="noopener">Spark运行模式(local standalond,yarn-client,yarn-cluster,mesos-client,mesos-cluster)</a></li></ol></blockquote><blockquote><ol start="2"><li><a href="https://blog.csdn.net/LW_GHY/article/details/52946136" target="_blank" rel="noopener">spark参数总结</a></li><li><a href="https://blog.csdn.net/LW_GHY/article/details/51477065" target="_blank" rel="noopener">spark 常用算子</a></li><li><a href="https://blog.csdn.net/LW_GHY/article/details/51470073" target="_blank" rel="noopener">Spark on YARN客户端模式作业运行全过程分析</a></li><li><a href="https://github.com/JerryLead/SparkInternals" target="_blank" rel="noopener">spark 原理解析（重要）</a></li></ol></blockquote><hr><h4 id="spark-batch"><a href="#spark-batch" class="headerlink" title="spark batch"></a>spark batch</h4><blockquote><ol><li><a href="https://blog.csdn.net/LW_GHY/article/details/51477100" target="_blank" rel="noopener">Spark多文件输出(MultipleOutputFormat)</a></li></ol></blockquote><blockquote><p>2.spark-mysql 操作</p><blockquote><p>2.1 <a href="https://blog.csdn.net/LW_GHY/article/details/50939091" target="_blank" rel="noopener">Spark读取数据库(Mysql)的四种方式讲解</a></p></blockquote></blockquote><blockquote><blockquote><p>2.2 <a href="https://blog.csdn.net/LW_GHY/article/details/51465125" target="_blank" rel="noopener">Spark与Mysql(JdbcRDD)整合开发</a></p></blockquote></blockquote><blockquote><blockquote><p>2.3 <a href="https://blog.csdn.net/LW_GHY/article/details/51477072" target="_blank" rel="noopener">spark 计算结果写入mysql</a></p></blockquote></blockquote><hr><h4 id="spark-Streaming"><a href="#spark-Streaming" class="headerlink" title="spark Streaming"></a>spark Streaming</h4><blockquote><ol><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">官网stream</a></li></ol></blockquote><blockquote><ol start="2"><li><a href="https://blog.csdn.net/LW_GHY/article/details/88867174" target="_blank" rel="noopener">Spark Streaming 实现思路与模块概述</a></li></ol></blockquote><blockquote><ol start="3"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51673081" target="_blank" rel="noopener">SparkStreaming向Hbase中写数据</a></li></ol></blockquote><blockquote><ol start="4"><li><a href="https://blog.csdn.net/LW_GHY/article/details/52143659" target="_blank" rel="noopener">Spark Streaming kafka实现数据零丢失的几种方式</a></li></ol></blockquote><blockquote><ol start="5"><li><a href="https://blog.csdn.net/LW_GHY/article/details/50926956" target="_blank" rel="noopener">Kafka+Spark Streaming+Redis实时系统实践</a></li></ol></blockquote><blockquote><ol start="6"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51477355" target="_blank" rel="noopener">Spark Streaming中空batches处理的两种方法</a></li></ol></blockquote><hr><h4 id="spark-调优"><a href="#spark-调优" class="headerlink" title="spark 调优"></a>spark 调优</h4><blockquote><ol><li><a href="https://blog.csdn.net/LW_GHY/article/details/51419760" target="_blank" rel="noopener">spark性能优化：shuffle调优</a></li></ol></blockquote><blockquote><ol start="2"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51420027" target="_blank" rel="noopener">spark性能调优：开发调优</a></li></ol></blockquote><blockquote><ol start="3"><li><a href="https://blog.csdn.net/LW_GHY/article/details/51419977" target="_blank" rel="noopener">spark性能调优：资源优化</a></li></ol></blockquote><blockquote><ol start="4"><li><a href="https://blog.csdn.net/LW_GHY/article/details/50780940" target="_blank" rel="noopener">Saprk Streaming性能调优</a></li></ol></blockquote><blockquote><ol start="5"><li><a href="https://blog.csdn.net/LW_GHY/article/details/52373749" target="_blank" rel="noopener">GC调优在Spark应用中的实践</a></li></ol></blockquote><blockquote><ol start="6"><li><a href="https://blog.csdn.net/LW_GHY/article/details/88232471" target="_blank" rel="noopener">JVM的GC调优-上</a></li></ol></blockquote><blockquote><ol start="7"><li><a href="https://blog.csdn.net/LW_GHY/article/details/88232638" target="_blank" rel="noopener">JVM的GC调优-下</a></li></ol></blockquote><hr><h4 id="sparkMLib"><a href="#sparkMLib" class="headerlink" title="sparkMLib"></a>sparkMLib</h4><blockquote><p>1.<a href="https://blog.csdn.net/LW_GHY/article/details/54426443" target="_blank" rel="noopener">Spark MLlib训练的广告点击率预测模型</a></p></blockquote><hr><h4 id="案例实践"><a href="#案例实践" class="headerlink" title="案例实践"></a>案例实践</h4><blockquote><ol><li><a href="https://blog.csdn.net/xwc35047/article/details/75309350" target="_blank" rel="noopener">基于Spark streaming的SQL服务实时自动化运维</a></li><li></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka-资料篇</title>
      <link href="/2019/07/12/kafka-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/07/12/kafka-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="官网"><a href="#官网" class="headerlink" title="官网"></a>官网</h4><blockquote><p>1.<a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">官网</a></p></blockquote><h4 id="kafka-技术内幕"><a href="#kafka-技术内幕" class="headerlink" title="kafka 技术内幕"></a>kafka 技术内幕</h4><blockquote><ol><li><a href="https://blog.csdn.net/lw_ghy/article/details/51494292" target="_blank" rel="noopener">Kafka技术内幕-日志压缩</a></li></ol></blockquote><h4 id="中文教程"><a href="#中文教程" class="headerlink" title="中文教程"></a>中文教程</h4><blockquote><p>1.<a href="https://www.orchome.com/kafka/index" target="_blank" rel="noopener">kafka</a></p></blockquote><h4 id="kafka-实践"><a href="#kafka-实践" class="headerlink" title="kafka 实践"></a>kafka 实践</h4><blockquote><p>1.<a href="https://blog.csdn.net/LW_GHY/article/details/72900873" target="_blank" rel="noopener">kafka broker的常用配置</a></p></blockquote><h4 id="kafka-面试问题"><a href="#kafka-面试问题" class="headerlink" title="kafka 面试问题"></a>kafka 面试问题</h4><blockquote><p><a href="https://ruhetouzi.com/t/topic/3821" target="_blank" rel="noopener">21个常见问题</a></p></blockquote><blockquote><p><a href="https://ruhetouzi.com/t/topic/3832" target="_blank" rel="noopener">常见问题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka-问题篇</title>
      <link href="/2019/07/11/kafka-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/07/11/kafka-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="No-resolvable-bootstrap-urls-given-in-bootstrap-servers"><a href="#No-resolvable-bootstrap-urls-given-in-bootstrap-servers" class="headerlink" title="No resolvable bootstrap urls given in bootstrap.servers"></a>No resolvable bootstrap urls given in bootstrap.servers</h4><pre><code>说明当前的bootstrap.servers 填写不正确或者未再配置conf 中设置conf 基础配置val props = parameterTool.getProperties    //判断是线上还是线下if (parameterTool.get(ENV_STATE_FLAG) == &quot;online&quot;) props.put(&quot;bootstrap.servers&quot;, parameterTool.get(KAFKA_ONLINE_BROKERS))else props.put(&quot;bootstrap.servers&quot;, parameterTool.get(KAFKA_OFFLINE_BROKERS))    props.put(&quot;group.id&quot;, parameterTool.get(&quot;groupId&quot;))    props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;)    props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;)    props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;)    props</code></pre><hr><h4 id="写kafka-超时"><a href="#写kafka-超时" class="headerlink" title="写kafka 超时"></a>写kafka 超时</h4><pre><code>Caused by: org.apache.kafka.common.errors.TimeoutException: Expiring 23 record(s) for Topic: 30001 ms has passed since last append</code></pre><p>解决方法：<br>  查看 request.timeout.ms=120000 （defalt 30s）</p><h4 id="Timed-out-waiting-for-connection-while-in-state-CONNECTING"><a href="#Timed-out-waiting-for-connection-while-in-state-CONNECTING" class="headerlink" title="Timed out waiting for connection while in state: CONNECTING"></a>Timed out waiting for connection while in state: CONNECTING</h4><p>1.zookeeper 访问不了或者没有启动。 查看下zookeeper （网络是否通）是否正常启动。<br>2.kafka 的zookeeper （server.properties里面）访问地址不正确，检查一下。<br>3.kafka 的 broker.id （server.properties里面）没有注释掉，这里集群最好注释掉，不要手动指定。<br>4.修改 kafka 配置 连接超时间，这里是以毫秒为单位。</p><pre><code> zookeeper.connection.timeout.ms=60000</code></pre>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo-常用篇</title>
      <link href="/2019/07/10/hexo-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/07/10/hexo-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="创建新文件"><a href="#创建新文件" class="headerlink" title="创建新文件"></a>创建新文件</h4><pre><code> 在当前总目录下 hexo new &quot;hexo之基本操作&quot;</code></pre><h4 id="清除缓存更新部署"><a href="#清除缓存更新部署" class="headerlink" title="清除缓存更新部署"></a>清除缓存更新部署</h4><pre><code> hexo clean &amp;&amp; hexo g -d</code></pre><h4 id="metery"><a href="#metery" class="headerlink" title="metery"></a>metery</h4><pre><code>---title: typora-vue-theme主题介绍date: 2018-09-07 09:25:00author: 赵奇img: /source/images/xxx.jpgtop: truecover: truecoverImg: /images/1.jpgpassword: toc: falsemathjax: falsesummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要categories: Markdowntags:  - Typora  - Markdown---</code></pre><h4 id="相关参考"><a href="#相关参考" class="headerlink" title="相关参考"></a>相关参考</h4><blockquote><p><a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md" target="_blank" rel="noopener">主题代码</a></p></blockquote><blockquote><p><a href="http://luokangyuan.com/" target="_blank" rel="noopener">大佬博客</a></p></blockquote><blockquote><p><a href="https://juejin.im/post/5d81d3036fb9a06b3260a901" target="_blank" rel="noopener">hexo+github+hexo-theme-matery搭建个人博客</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-常用篇</title>
      <link href="/2019/07/10/linux-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/07/10/linux-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="常用小技巧"><a href="#常用小技巧" class="headerlink" title="常用小技巧"></a>常用小技巧</h4><h5 id="查询字符串在文件的位置"><a href="#查询字符串在文件的位置" class="headerlink" title="查询字符串在文件的位置"></a>查询字符串在文件的位置</h5><pre><code>### 在var 目录下查询包含“test.user”字符串的文件 并显示上下5行的信息。grep -C 5 -rn &quot;test.user&quot; /var</code></pre><h5 id="快速清除文件内容"><a href="#快速清除文件内容" class="headerlink" title="快速清除文件内容"></a>快速清除文件内容</h5><pre><code> &gt; filename </code></pre><h5 id="统计文件个数"><a href="#统计文件个数" class="headerlink" title="统计文件个数"></a>统计文件个数</h5><pre><code>统计某文件夹下文件的个数ls -l |grep &quot;^-&quot;|wc -l统计某文件夹下目录的个数ls -l |grep &quot;^ｄ&quot;|wc -l</code></pre><h5 id="快捷键操作"><a href="#快捷键操作" class="headerlink" title="快捷键操作"></a>快捷键操作</h5><pre><code>Ctl-U   删除光标到行首的所有字符,在某些设置下,删除全行Ctl-W   删除当前光标到前边的最近一个空格之间的字符</code></pre><h5 id="查看错误日志"><a href="#查看错误日志" class="headerlink" title="查看错误日志"></a>查看错误日志</h5><pre><code>tail -n 2000 namenode.log | grep -i -A 30 -B 5 -E &quot;error|exception&quot;读取namenode.log文件的末尾2000行，然后通过grep命令，忽略大小写过滤出包含error或者exception字符的行，并打印匹配行前面5行和后面30行的内容。这样不仅可以找出错误，还可以把异常堆栈信息找出来</code></pre><h5 id="shell文本格式化代码："><a href="#shell文本格式化代码：" class="headerlink" title="shell文本格式化代码："></a>shell文本格式化代码：</h5><pre><code> gg=G</code></pre><h5 id="查看shell-脚本执行过程"><a href="#查看shell-脚本执行过程" class="headerlink" title="查看shell 脚本执行过程"></a>查看shell 脚本执行过程</h5><pre><code>bash -x test.sh</code></pre><h4 id="案例1-磁盘清理"><a href="#案例1-磁盘清理" class="headerlink" title="案例1. 磁盘清理"></a>案例1. 磁盘清理</h4><pre><code>第一步查看磁盘具体情况df -h第二步定位大文件du -h —max-depth=1 /第三步删除大文件第四步如果删除磁盘还是没有清理lsof -nP | grep &#39;(deleted)’5.kill 掉你删除文件的pid6.再查看你会发现世界真的很美好</code></pre><hr><h4 id="案例2-工具使用-sort-uniq-tr-cut-paste-split-file-wc-iconv"><a href="#案例2-工具使用-sort-uniq-tr-cut-paste-split-file-wc-iconv" class="headerlink" title="案例2. 工具使用(sort|uniq|tr|cut|paste|split|file|wc|iconv)"></a>案例2. 工具使用(sort|uniq|tr|cut|paste|split|file|wc|iconv)</h4><pre><code>sort - 对内容排序uniq - 去掉相邻重复内容tr - 替换指定内容为新内容cut / paste - 剪切/黏贴内容split - 拆分文件file - 判断文件类型wc - 统计文件行数、单词数、字节数iconv - 编码转换#sort 按第2列排序[root@localhost program_test]# sort -k 2 sort.txt4 bad 50003  linux 501   mac 20002  winxp 100#sort 逆序排序[root@localhost program_test]# sort -r sort.txt4 bad 50003  linux 502  winxp 1001   mac 2000[root ~]# cat foo.txtgrapeapplepitaya[root ~]# cat bar.txt100200300400#paste 粘贴 拼接[root ~]# paste foo.txt bar.txtgrape   100apple   200pitaya  300        400[root ~]# paste foo.txt bar.txt &gt; hello.txt[root ~]# cut -b 4-8 hello.txtpe      10le      20aya     30[root ~]# cat hello.txt | tr &#39;\t&#39; &#39;,&#39;grape,100apple,200pitaya,300,400#split 切分文件第100行[root ~]# split -l 100 sohu.html hello[root ~]# wget https://www.baidu.com/img/bd_logo1.png#file 判断文件格式[root ~]# file bd_logo1.pngbd_logo1.png: PNG image data, 540 x 258, 8-bit colormap, non-interlaced[root ~]# wc sohu.html  2979   6355 212527 sohu.html#wc 单词计数wc -l sohu.html2979 sohu.html#iconv iconv -f gb2312 -t utf-8 qq.html#curl 的使用 结果会被保存到index.htmlcurl -o index.html http://www.codebelief.com  wget http://www.qq.com -O qq.html</code></pre><h4 id="案例3-管道重定向"><a href="#案例3-管道重定向" class="headerlink" title="案例3-管道重定向"></a>案例3-管道重定向</h4><pre><code>查找record.log中包含AAA，但不包含BBB的记录的总数[root ~]# cat record.log | grep AAA | grep -v BBB | wc -l</code></pre><h6 id="输出重定向和错误重定向-gt-gt-gt-2-gt-。"><a href="#输出重定向和错误重定向-gt-gt-gt-2-gt-。" class="headerlink" title="输出重定向和错误重定向 - &gt; / &gt;&gt; / 2&gt;。"></a>输出重定向和错误重定向 - &gt; / &gt;&gt; / 2&gt;。</h6><pre><code>[root ~]# cat readme.txtbananaapplegrapeapplegrapewatermelonpearpitaya[root ~]# cat readme.txt | sort | uniq &gt; result.txt[root ~]# cat result.txtapplebananagrapepearpitayawatermelon</code></pre><h6 id="输入重定向-lt-。"><a href="#输入重定向-lt-。" class="headerlink" title="输入重定向 - &lt;。"></a>输入重定向 - &lt;。</h6><pre><code>[root ~]# echo &#39;hello, world!&#39; &gt; hello.txt[root ~]# wall &lt; hello.txt[root ~]#Broadcast message from root (Wed Jun 20 19:43:05 2018):hello, world![root ~]# echo &#39;I will show you some code.&#39; &gt;&gt; hello.txt[root ~]# wall &lt; hello.txt[root ~]#Broadcast message from root (Wed Jun 20 19:43:55 2018):hello, world!I will show you some code.</code></pre><h4 id="案例4-字符流编辑器-sed。"><a href="#案例4-字符流编辑器-sed。" class="headerlink" title="案例4. 字符流编辑器 - sed。"></a>案例4. 字符流编辑器 - sed。</h4><pre><code>sed是操作、过滤和转换文本内容的工具。假设有一个名为fruit.txt的文件，内容如下所示。[root ~]# cat -n fruit.txt      1  banana     2  grape     3  apple     4  watermelon     5  orange接下来，我们在第2行后面添加一个pitaya。[root ~]# sed &#39;2a pitaya&#39; fruit.txt bananagrapepitayaapplewatermelonorange注意：刚才的命令和之前我们讲过的很多命令一样并没有改变fruit.txt文件，而是将添加了新行的内容输出到终端中，如果想保存到fruit.txt中，可以使用输出重定向操作。在第2行前面插入一个waxberry。[root ~]# sed &#39;2i waxberry&#39; fruit.txtbananawaxberrygrapeapplewatermelonorange删除第3行。[root ~]# sed &#39;3d&#39; fruit.txtbananagrapewatermelonorange删除第2行到第4行。[root ~]# sed &#39;2,4d&#39; fruit.txtbananaorange将文本中的字符a替换为@。[root ~]# sed &#39;s#a#@#&#39; fruit.txt b@nanagr@pe@pplew@termelonor@nge将文本中的字符a替换为@，使用全局模式。[root ~]# sed &#39;s#a#@#g&#39; fruit.txt b@n@n@gr@pe@pplew@termelonor@nge</code></pre><h4 id="案例5-awk-使用"><a href="#案例5-awk-使用" class="headerlink" title="案例5. awk-使用"></a>案例5. awk-使用</h4><pre><code>[root ~]# cat fruit2.txt 1       banana      1202       grape       5003       apple       12304       watermelon  805       orange      400</code></pre><h5 id="显示文件的第3行。"><a href="#显示文件的第3行。" class="headerlink" title="显示文件的第3行。"></a>显示文件的第3行。</h5><pre><code>[root ~]# awk &#39;NR==3&#39; fruit2.txt 3       apple       1230</code></pre><h5 id="显示文件的第2列。"><a href="#显示文件的第2列。" class="headerlink" title="显示文件的第2列。"></a>显示文件的第2列。</h5><pre><code>[root ~]# awk &#39;{print $2}&#39; fruit2.txt bananagrapeapplewatermelonorange</code></pre><h5 id="显示文件的最后一列。"><a href="#显示文件的最后一列。" class="headerlink" title="显示文件的最后一列。"></a>显示文件的最后一列。</h5><pre><code>[root ~]# awk &#39;{print $NF}&#39; fruit2.txt 120500123080400</code></pre><h5 id="输出末尾数字大于等于300的行。"><a href="#输出末尾数字大于等于300的行。" class="headerlink" title="输出末尾数字大于等于300的行。"></a>输出末尾数字大于等于300的行。</h5><pre><code>[root ~]# awk &#39;{if($3 &gt;= 300) {print $0}}&#39; fruit2.txt 2       grape       5003       apple       12305       orange      400</code></pre><h5 id="进行行的过滤"><a href="#进行行的过滤" class="headerlink" title="进行行的过滤"></a>进行行的过滤</h5><pre><code>awk &#39;NR &lt; 5&#39; #行号小于5awk &#39;NR==1,NR==4 {print}&#39; file #行号等于1和4的打印出来awk &#39;/linux/&#39; #包含linux文本的行（可以用正则表达式来指定，超级强大）awk &#39;!/linux/&#39; #不包含linux文本的行</code></pre><h4 id="案例6-统计文件的行数"><a href="#案例6-统计文件的行数" class="headerlink" title="案例6. 统计文件的行数"></a>案例6. 统计文件的行数</h4><pre><code>awk &#39; END {print NR}&#39;</code></pre><h4 id="案例7-将外部shell命令的输出读入到变量中"><a href="#案例7-将外部shell命令的输出读入到变量中" class="headerlink" title="案例7. 将外部shell命令的输出读入到变量中"></a>案例7. 将外部shell命令的输出读入到变量中</h4><pre><code>使用getline，将外部shell命令的输出读入到变量cmdout中:echo | awk &#39;{&quot;grep root /etc/passwd&quot; | getline cmdout; print cmdout }&#39;</code></pre><h4 id="案例8-替换jar-或war-中的文件"><a href="#案例8-替换jar-或war-中的文件" class="headerlink" title="案例8.替换jar 或war 中的文件"></a>案例8.替换jar 或war 中的文件</h4><pre><code>1.首先查找出文件的绝对路径jar tvf test.war | grep application.yml2.解压文件jar xvf test.war3.然后将绝对路径下文件进行替换，然后添加到war 里jar uvf test.war WEB-INF/classes/application.yml#jar 常见用法列出 .jar/.war 文件内容jar -tf file.jar-v 在标准输出中生成详细输出-u 更新现有文件-c 创建新归档文件-f 指定归档文件名-x 解档文件-t 列出归档文件内容-0  仅存储; 不使用任何 ZIP 压缩（把jar包放进war必须把这个参数加上）</code></pre><h4 id="案例9-按占用内存由高到低排序"><a href="#案例9-按占用内存由高到低排序" class="headerlink" title="案例9.按占用内存由高到低排序"></a>案例9.按占用内存由高到低排序</h4><h6 id="内存使用前10"><a href="#内存使用前10" class="headerlink" title="内存使用前10:"></a>内存使用前10:</h6><pre><code>ps auxw|head -1;ps auxw|sort -rn -k4|head -10</code></pre><h6 id="内存整体情况"><a href="#内存整体情况" class="headerlink" title="内存整体情况"></a>内存整体情况</h6><pre><code>free -m</code></pre><h6 id="含义："><a href="#含义：" class="headerlink" title="含义："></a>含义：</h6><pre><code>USER    进程所属用户    PID    进程ID    %CPU    进程占用CPU百分比    %MEM    进程占用内存百分比    VSZ    虚拟内存占用大小    单位：kb（killobytes）RSS    实际内存占用大小    单位：kb（killobytes）TTY    终端类型    STAT    进程状态    START    进程启动时刻    TIME    进程运行时长    COMMAND    启动进程的命令</code></pre><h4 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h4><h5 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h5><pre><code> cat -s //多个空白行压缩成一个  cat *.txt | tr -s &#39;\n&#39;   //移除空白行 cat -n //加行号</code></pre><h5 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h5><pre><code>1. 删除文件： rm -rf  *log  等加于$ find ./ -name “*log” -exec rm {} 2. 搜寻文件或目录: $find ./ -name &quot;core*&quot; | xargs file3. 查找所有非txt文本: find . ! -name &quot;*.txt&quot; -print4. 统计文本中123 出现的个数： grep -c “123” filename5. sort 排序：-n 按数字进行排序 VS -d 按字典序进行排序 -r 逆序排序  -k N 指定按第N列排序 ：sort -n -1k -2k    sort -bd data // 忽略像空格之类的前导空白字符6. 拼接文本 paste file1 file2</code></pre><h5 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h5><pre><code># uname -a               # 查看内核/操作系统/CPU信息# head -n 1 /etc/issue   # 查看操作系统版本# cat /proc/cpuinfo      # 查看CPU信息# hostname               # 查看计算机名# lspci -tv              # 列出所有PCI设备# lsusb -tv              # 列出所有USB设备# lsmod                  # 列出加载的内核模块# env                    # 查看环境变量</code></pre><h5 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h5><pre><code># free -m                # 查看内存使用量和交换区使用量# df -h                  # 查看各分区使用情况# du -sh &lt;目录名&gt;        # 查看指定目录的大小# grep MemTotal /proc/meminfo   # 查看内存总量# grep MemFree /proc/meminfo    # 查看空闲内存量# uptime                 # 查看系统运行时间、用户数、负载# cat /proc/loadavg      # 查看系统负载</code></pre><h5 id="磁盘和分区"><a href="#磁盘和分区" class="headerlink" title="磁盘和分区"></a>磁盘和分区</h5><pre><code># mount | column -t      # 查看挂接的分区状态# fdisk -l               # 查看所有分区# swapon -s              # 查看所有交换分区# hdparm -i /dev/hda     # 查看磁盘参数(仅适用于IDE设备)# dmesg | grep IDE       # 查看启动时IDE设备检测状况</code></pre><h5 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h5><pre><code># ifconfig               # 查看所有网络接口的属性# iptables -L            # 查看防火墙设置# route -n               # 查看路由表# netstat -lntp          # 查看所有监听端口# netstat -antp          # 查看所有已经建立的连接# netstat -s             # 查看网络统计信息# ip route show</code></pre><h5 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h5><pre><code># ps -ef                 # 查看所有进程# top                    # 实时显示进程状态#根据 top 中作业pid 查看作业进程cd /proc/pid/#查询单独一个指定pid 的内存和cpu 使用情况ps -aux | grep pidtop -p pid</code></pre><h5 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h5><pre><code># w                      # 查看活动用户# id &lt;用户名&gt;            # 查看指定用户信息# last                   # 查看用户登录日志# cut -d: -f1 /etc/passwd   # 查看系统所有用户# cut -d: -f1 /etc/group    # 查看系统所有组# crontab -l             # 查看当前用户的计划任务</code></pre><h5 id="服务"><a href="#服务" class="headerlink" title="服务"></a>服务</h5><pre><code># chkconfig --list       # 列出所有系统服务# chkconfig --list | grep on    # 列出所有启动的系统服务</code></pre><h5 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h5><pre><code># rpm -qa                # 查看所有安装的软件包</code></pre><h4 id="小技巧："><a href="#小技巧：" class="headerlink" title="小技巧："></a>小技巧：</h4><h5 id="查看变量是否被声明："><a href="#查看变量是否被声明：" class="headerlink" title="查看变量是否被声明："></a>查看变量是否被声明：</h5><pre><code>使用 :- 来测试是否一个变量是否被声明过。如：if [ &quot;${NAME:-}&quot; = &quot;Kevin&quot; ] 如果 ${NAME}变量未声明则会变为空字符，你也可以设置为其他默认值.例如：如果不存在，默认值设为：noname ，if [ &quot;${NAME:-noname}&quot; = &quot;Kevin” ]</code></pre><h5 id="自定义-linux-命令："><a href="#自定义-linux-命令：" class="headerlink" title="自定义 linux 命令："></a>自定义 linux 命令：</h5><pre><code>vim ~/.bashrc 中设置命令别名:alias lsl=&#39;ls -lrt&#39;alias lm=&#39;ls -al|more’</code></pre><hr><h4 id="vi-快捷建"><a href="#vi-快捷建" class="headerlink" title="vi 快捷建"></a>vi 快捷建</h4><p>A               移动光标到当前行尾，并进入 insert 状态<br>a               在当前位置后进入 insert 状态<br>dd              删除当前行<br>D               删除光标之后的内容<br>p               粘贴刚删除的文本<br>ctrl+r          搜索历史命令<br>ctrl+X Ctrl+E   调用默认编辑器去编辑一个特别长的命令</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cassandra-优化篇</title>
      <link href="/2019/05/17/cassandra-%E4%BC%98%E5%8C%96%E7%AF%87/"/>
      <url>/2019/05/17/cassandra-%E4%BC%98%E5%8C%96%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>Cassandra不同于普通的应用程序，它是分布式数据库，它要大口吃内存，吃磁盘，吃CPU，所以机器要进行特殊的配置，以适应其需要。</p><h4 id="使用最新的64位的jdk8的最新发布版本。"><a href="#使用最新的64位的jdk8的最新发布版本。" class="headerlink" title="使用最新的64位的jdk8的最新发布版本。"></a>使用最新的64位的jdk8的最新发布版本。</h4><hr><h4 id="时钟同步，开启NTP服务"><a href="#时钟同步，开启NTP服务" class="headerlink" title="时钟同步，开启NTP服务."></a>时钟同步，开启NTP服务.</h4><blockquote><p>cassandra是分布式存储，就靠时间戳解决数据冲突，所以始终必须同步</p></blockquote><hr><h4 id="TCP参数设置"><a href="#TCP参数设置" class="headerlink" title="TCP参数设置"></a>TCP参数设置</h4><blockquote><p>在低带宽环境下，防火墙会检测闲置的连接并关闭，为了保护节点之间，或者多个DC节点之间的连接，建议如下配置系统参数</p></blockquote><pre><code>sudo sysctl -wnet.ipv4.tcp_keepalive_time=60net.ipv4.tcp_keepalive_probes=3net.ipv4.tcp_keepalive_intvl=10</code></pre><blockquote><p>设置这个就是可以快速的发现底层的TCP连接是否已经关闭，它间隔60秒开始探测3次，每次探测间隔10秒，也就是说最多在60+3*10=90秒内就可以检测到连接被中断。<br>为了支撑上千个数据库连接，还建议修改以下参数</p></blockquote><pre><code>sudo sysctl -wnet.core.rmem_max=16777216net.core.wmem_max=16777216net.core.rmem_default=16777216net.core.wmem_default=16777216net.core.optmem_max=40960net.ipv4.tcp_rmem=4096 87380 16777216net.ipv4.tcp_wmem=4096 65536 16777216</code></pre><blockquote><p>为了让参数永久生效，记得把它们写入系统配置文件/etc/sysctl.conf里</p></blockquote><hr><h4 id="禁用CPU动态跳频功能。"><a href="#禁用CPU动态跳频功能。" class="headerlink" title="禁用CPU动态跳频功能。"></a>禁用CPU动态跳频功能。</h4><blockquote><p>最近的linux系统增加了一个新特性，就是可以动态调整CPU频率，就是在机器低负载的时候，可以降低CPU频率，以达到降低功耗的目的。<br>这种动态调频功能会影响cassandra数据库的吞吐量。建议禁用，让CPU一直维持恒定的频率输出，尽管这很耗电，但是保证你的数据库的吞吐量。</p></blockquote><pre><code>禁用方式：for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governordo[ -f $CPUFREQ ] || continueecho -n performance &gt; $CPUFREQdone</code></pre><hr><h4 id="禁用zone-reclaim-mode"><a href="#禁用zone-reclaim-mode" class="headerlink" title="禁用zone_reclaim_mode"></a>禁用zone_reclaim_mode</h4><blockquote><p>官方建议禁用，这个是关于多核CPU使用NUMA架构，分别访问内存，内存回收方面的一个参数<br>这个参数的解释，可以参考：<br><a href="http://linuxinsight.com/proc_sys_vm_zone_reclaim_mode.html" target="_blank" rel="noopener">http://linuxinsight.com/proc_sys_vm_zone_reclaim_mode.html</a><br>这里面有一句话，当你的机器用作文件服务器，或者你的大部分内存需要用于系统文件缓存的时候，你需要禁用这个功能。<br>我们的Cassandra就相当于文件服务器，它对IO是依赖的，它需要系统内存用于大量缓存DB文件。所以要禁用这个功能。<br>echo 0 &gt; /proc/sys/vm/zone_reclaim_mode<br>Cassandra官方描述了如果不禁用这个参数带来的后果：</p></blockquote><pre><code>1、随机CPU尖峰带来时延增加，吞吐量增加。2、程序假死，什么也不做。3、一些突然发生又消失的莫名异常。4、重启机器，可能在一段时间内不再出现异常。</code></pre><hr><h4 id="资源限制放开。"><a href="#资源限制放开。" class="headerlink" title="资源限制放开。"></a>资源限制放开。</h4><blockquote><p>cassandra会使用很多内存，很多连接，很多文件，所以一律放开。</p></blockquote><pre><code>&lt;cassandra_user&gt; – memlock unlimited&lt;cassandra_user&gt; – nofile 100000&lt;cassandra_user&gt; – nproc 32768&lt;cassandra_user&gt; – as unlimited</code></pre><blockquote><p>这个加入到/etc/security/limits.conf 里（不同操作系统，可能不同，后面不再注明）<br>vm.max_map_count = 1048575   将这个加入到 /etc/sysctl.conf 里</p></blockquote><hr><h4 id="禁用swap"><a href="#禁用swap" class="headerlink" title="禁用swap"></a>禁用swap</h4><blockquote><p>关闭 sudo swapoff –all<br>修改 /etc/fstab. 去掉swap挂载。<br>swap文件内存交换区，当你的内存不够的时候，使用文件内存，这会让你的数据库卡成狗的，一律禁用。</p></blockquote><hr><h4 id="文件预"><a href="#文件预" class="headerlink" title="文件预"></a>文件预</h4><blockquote><p>默认保持64k即可</p></blockquote><pre><code>echo 64 &gt; /sys/class/block/{sda}/queue/read_ahead_kb</code></pre><blockquote><p>如果是ssd，设置为8k</p></blockquote><pre><code>echo 8 &gt; /sys/class/block/{sda}/queue/read_ahead_kb</code></pre><blockquote><p>如果是ssd，还要设置下面的参数进行优化。</p></blockquote><pre><code>echo deadline &gt; /sys/block/sda/queue/scheduler#OR…#echo noop &gt; /sys/block/sda/queue/schedulertouch /var/lock/subsys/localecho 0 &gt; /sys/class/block/sda/queue/rotational</code></pre><hr><h4 id="确保以上参数重启机器后仍然有效。"><a href="#确保以上参数重启机器后仍然有效。" class="headerlink" title="确保以上参数重启机器后仍然有效。"></a>确保以上参数重启机器后仍然有效。</h4><hr><h4 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h4><p><a href="https://zhaoyanblog.com/archives/1005.html" target="_blank" rel="noopener">赵岩的博客</a></p>]]></content>
      
      
      <categories>
          
          <category> cassandra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cassandra-优化篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-问题篇</title>
      <link href="/2019/05/17/linux-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/05/17/linux-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="报错信息：run-npm-audit-fix-to-fix-them-or-npm-audit-for-details"><a href="#报错信息：run-npm-audit-fix-to-fix-them-or-npm-audit-for-details" class="headerlink" title="报错信息：run npm audit fix to fix them, or npm audit for details"></a>报错信息：run <code>npm audit fix</code> to fix them, or <code>npm audit</code> for details</h4><pre><code>解决：npm audit fix --force</code></pre>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cassandra-问题篇</title>
      <link href="/2019/05/17/cassandra-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/05/17/cassandra-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="为什么不可以设置listen-address为0-0-0-0（意思是监听所有地址）？"><a href="#为什么不可以设置listen-address为0-0-0-0（意思是监听所有地址）？" class="headerlink" title="为什么不可以设置listen_address为0.0.0.0（意思是监听所有地址）？"></a>为什么不可以设置listen_address为0.0.0.0（意思是监听所有地址）？</h4><blockquote><p>Cassandra是一个基于gossip协议的分布式系统，监听地址是用来告诉其它节点来访问的，告诉别的节点说“连接我任何地址都可以”，是一个糟糕的想法，如果集群中不同的节点使用了不同方式的地址，悲剧的事情就要发生了。<br>如果你不想为你集群中的每个节点单独配置ip（非常可以理解）,你可以不配，空着它，Cassandra将会使用InetAddress.getLocalHost()来选择地址，然后只要你或者你的运维团队保证这个是正确的(/etc/hosts/,dns 等等要配置对)。<br>一个例外是JMX,他默认监听的地址是0.0.0.0（这个是java的bug 6425769）<br>请看CASSANDRA-256 和 CASSANDRA-43获取这方面更多的细节。</p></blockquote><hr><h4 id="cassandra用了哪些端口？"><a href="#cassandra用了哪些端口？" class="headerlink" title="cassandra用了哪些端口？"></a>cassandra用了哪些端口？</h4><blockquote><p>默认7000作为集群通信端口（如果开启了SSL就是7001端口）。<br>9042端口用于native协议的客户端连接。<br>7199端口用于JMX，<br>9160端口用于废弃的Thrift接口。<br>内部节点通信以及native协议的端口在cassandra配置文件里可以配置。JMX端口可以在cassandra-env.sh配置（通过JVM的参数)。所有端口都是TCP的。</p></blockquote><hr><h4 id="当往集群中增加新节点的时候，对于存在的数据发生了什么？"><a href="#当往集群中增加新节点的时候，对于存在的数据发生了什么？" class="headerlink" title="当往集群中增加新节点的时候，对于存在的数据发生了什么？"></a>当往集群中增加新节点的时候，对于存在的数据发生了什么？</h4><blockquote><p>当一个新节点加入到集群，它将会自动连接集群中的其它节点，并且去复制正确的数据到本地，同样的增加、替换、移动、删除节点都是这样的。</p></blockquote><hr><h4 id="我删除了数据，但是磁盘使用率没有变化，这是为什么？"><a href="#我删除了数据，但是磁盘使用率没有变化，这是为什么？" class="headerlink" title="我删除了数据，但是磁盘使用率没有变化，这是为什么？"></a>我删除了数据，但是磁盘使用率没有变化，这是为什么？</h4><blockquote><p>写入到cassandra里的数据会被持久化到SSTable文件里，SSTable文件是不可改变的，也就是说当你执行删除的时候，数据不会从文件中被去除掉的。<br>相反，一个标记（也叫tombstone)会被写入用于标记对应记录的新状态。不用担心，当数据和tombstone发生第一次compaction的时候，数据会被删除掉，相应的磁盘空间也被回收，你可以了解关于Compaction的更多细节。</p></blockquote><hr><h4 id="为什么用nodetool-ring只能看到一条记录？-即便所有节点输出的日志里可以看出，他们都发现彼此加入到了这个ring。"><a href="#为什么用nodetool-ring只能看到一条记录？-即便所有节点输出的日志里可以看出，他们都发现彼此加入到了这个ring。" class="headerlink" title="为什么用nodetool ring只能看到一条记录？ 即便所有节点输出的日志里可以看出，他们都发现彼此加入到了这个ring。"></a>为什么用nodetool ring只能看到一条记录？ 即便所有节点输出的日志里可以看出，他们都发现彼此加入到了这个ring。</h4><blockquote><p>这个发生于你的所有节点都配了通用的token，不要这么做。<br>这经常发生于哪些使用VM部署cassandra的用户，（特别是使用Debian package，它会在安装完自动启动cassandra，所以会生成token并保存它。），安装好后就把VM整个克隆出另外的节点。<br>增很容易修复，只要把数据目录以及commitlog目录删除，然后保证每个节点是随机生成的token，再启动就可以了。</p></blockquote><hr><h4 id="我可以修改一个正在运行中的集群中的keyspace的副本因子吗？"><a href="#我可以修改一个正在运行中的集群中的keyspace的副本因子吗？" class="headerlink" title="我可以修改一个正在运行中的集群中的keyspace的副本因子吗？"></a>我可以修改一个正在运行中的集群中的keyspace的副本因子吗？</h4><blockquote><p>可以，但是修改后需要执行repair或者cleanup来改变已存数据的副本个数。<br>首先使用cqlsh修改目标keyspace的副本因子。<br>如果你是减少副本因子，你可以执行nodetool cleanup去删除多余的副本数据，对每个节点都要执行。<br>如果你是增加副本因子，你需要执行nodetool repair来保证数据的副本个数满足当前的配置。 Repair只要对每个副本集执行一次即可。这是个敏感的操作，这会影响集群的性能。强烈建议执行rolling repair，因为试图一次修复整个集群的话，那可能是个坑。</p></blockquote><hr><h4 id="可以使用cassandra存储大的二进制字段吗？"><a href="#可以使用cassandra存储大的二进制字段吗？" class="headerlink" title="可以使用cassandra存储大的二进制字段吗？"></a>可以使用cassandra存储大的二进制字段吗？</h4><blockquote><p>Cassandra并没有对存储大文件或者二进制，以及这样一个二进制数据被经常读，也就是整个发送到客户端的情况进行优化。因为存储小的二进制数据（小于1MB)应该不是问题。但是还是建议把大的二进制数据分隔成小块。<br>需要特别注意的是，任何大于16MB的值，将被Cassandra拒绝掉，这是由max_mutation_size_in_kb配置项决定的（这个配置项默认是commitlog_segment_size_in_mb的一半，commitlog_segment_size_in_mb默认是32M)。</p></blockquote><hr><h4 id="Nodetool连接远程服务器的时候，提示“Connection-refused-to-host-127-0-1-1”-，这是为什么？"><a href="#Nodetool连接远程服务器的时候，提示“Connection-refused-to-host-127-0-1-1”-，这是为什么？" class="headerlink" title="Nodetool连接远程服务器的时候，提示“Connection refused to host: 127.0.1.1” ，这是为什么？"></a>Nodetool连接远程服务器的时候，提示“Connection refused to host: 127.0.1.1” ，这是为什么？</h4><blockquote><p>nodetool依赖JMX，JMX依赖RMI。RMI在两端通信的时候会根据需要创建自己的listenners和connectors。通常，这些都是底层透明的，但是不正确的hostname解析，无论是在连接方还是被连接方，都会导致错乱和这样的拒绝异常。<br>如果你在使用DNS。确保两端机器的/etc/hosts文件是正确的。如果还是失败的，你可以尝试通过jvm选项-Djava.rmi.server.hostname=指定你要连接的远程机器名称给JMX接口，配置项大体在cassandra-env.sh文件的靠下的位置。</p></blockquote><hr><h4 id="端口占用"><a href="#端口占用" class="headerlink" title="端口占用"></a>端口占用</h4><blockquote><p>错误：<br> Cassandra关闭后，重启，提示，7199端口被占用，分析原因是关闭时使用的ctrl+c，实际上并没有关闭cassandra服务进程，所以提示端口已被使用； </p></blockquote><pre><code>yxcdeMacBook-Pro:3.11.4 yxc$ cqlshConnection error: (&#39;Unable to connect to any servers&#39;, {&#39;127.0.0.1&#39;: error(61, &quot;Tried connecting to [(&#39;127.0.0.1&#39;, 9042)]. Last error: Connection refused&quot;)})</code></pre><blockquote><p>解决：<br>找出使用7199端口的进程<br>lsof -i:7199<br>杀死残留进程<br>kill direct_pid</p></blockquote><hr><h4 id="版本不一致"><a href="#版本不一致" class="headerlink" title="版本不一致"></a>版本不一致</h4><blockquote><p>错误： </p></blockquote><pre><code>Connection error: (‘Unable to connect to any servers’, {‘127.0.0.1’: ProtocolError(“cql_version ‘3.3.0’ is not supported by remote (w/ native protocol). Supported versions: [u’3.3.1’]”,)}) </code></pre><blockquote><p>解决： </p></blockquote><pre><code>修改cassandra_home/bin/cqlsh.py: DEFAULT_CQLVER = ‘3.3.0’为DEFAULT_CQLVER = ‘3.3.1’；</code></pre><hr><h4 id="All-host-s-tried-for-query-failed"><a href="#All-host-s-tried-for-query-failed" class="headerlink" title="All host(s) tried for query failed"></a>All host(s) tried for query failed</h4><blockquote><p>错误： </p></blockquote><pre><code>message: ‘All host(s) tried for query failed. First host tried, 127.0.0.1:9042: Error: connect ECONNREFUSED 127.0.0.1:9042. See innerErrors.’ }</code></pre><blockquote><p>解决： </p></blockquote><pre><code>修改cassandra_home/conf/cassandra.yaml: start_native_transport=false改为start_native_transport=true;</code></pre><hr><h4 id="Cannot-build-a-cluster-without-contact-points"><a href="#Cannot-build-a-cluster-without-contact-points" class="headerlink" title="Cannot build a cluster without contact points"></a>Cannot build a cluster without contact points</h4><blockquote><p>解决：</p></blockquote><pre><code> 不能构建cassandra 的集群，查看是否ip 没有写对</code></pre><hr><h4 id="caching-增加导致负载升高"><a href="#caching-增加导致负载升高" class="headerlink" title="caching 增加导致负载升高"></a>caching 增加导致负载升高</h4><blockquote><p>解决：</p></blockquote><pre><code>Not enough replica available for query at consistency ONE (1 required but only 0 alive)https://issues.apache.org/jira/browse/CASSANDRA-7905https://issues.apache.org/jira/browse/CASSANDRA-11815https://stackoverflow.com/questions/27974911/not-enough-replica-available-for-query-at-consistency-one-1-required-but-only-0复制数据的官网解释：https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.htmlhttps://docs.datastax.com/en/archived/cassandra/2.1/cassandra/operations/ops_add_dc_to_cluster_t.html</code></pre><h4 id="Cassandra-received-an-invalid-gossip-generation-for-peer"><a href="#Cassandra-received-an-invalid-gossip-generation-for-peer" class="headerlink" title="Cassandra received an invalid gossip generation for peer"></a>Cassandra received an invalid gossip generation for peer</h4><blockquote><p>解决：<br><a href="https://stackoverflow.com/questions/40676589/cassandra-received-an-invalid-gossip-generation-for-peer" target="_blank" rel="noopener">链接</a></p></blockquote><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://zhaoyanblog.com/archives/957.html" target="_blank" rel="noopener">赵岩</a></p>]]></content>
      
      
      <categories>
          
          <category> cassandra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cassandra-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>druid-学习篇</title>
      <link href="/2019/05/17/druid-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/05/17/druid-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<ol><li><a href="https://blog.csdn.net/bigtree_3721/article/category/6956082" target="_blank" rel="noopener">druid 系列csdn</a></li><li></li></ol>]]></content>
      
      
      <categories>
          
          <category> druid </category>
          
      </categories>
      
      
        <tags>
            
            <tag> druid-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>brew-使用篇</title>
      <link href="/2019/05/17/brew-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
      <url>/2019/05/17/brew-%E4%BD%BF%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="安装软件存放路径"><a href="#安装软件存放路径" class="headerlink" title="安装软件存放路径"></a>安装软件存放路径</h4><pre><code>/usr/local/Cellar/</code></pre><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><pre><code>brew --help #简洁命令帮助man brew #完整命令帮助brew install git #安装软件包(这里是示例安装的Git版本控制)brew uninstall git #卸载软件包brew search git #搜索软件包brew list #显示已经安装的所有软件包brew update #同步远程最新更新情况，对本机已经安装并有更新的软件用*标明brew outdated #查看已安装的哪些软件包需要更新brew upgrade git #更新单个软件包brew info git #查看软件包信息brew home git #访问软件包官方站brew cleanup #清理所有已安装软件包的历史老版本brew cleanup git #清理单个已安装软件包的历史版本</code></pre><h4 id="服务管理"><a href="#服务管理" class="headerlink" title="服务管理"></a>服务管理</h4><pre><code>brew services list  # 查看使用brew安装的服务列表brew services run formula|--all  # 启动服务（仅启动不注册）brew services start formula|--all  # 启动服务，并注册brew services stop formula|--all   # 停止服务，并取消注册brew services restart formula|--all  # 重启服务，并注册brew services cleanup  # 清除已卸载应用的无用的配置</code></pre><h4 id="问题及解决"><a href="#问题及解决" class="headerlink" title="问题及解决"></a>问题及解决</h4><h5 id="安装或卸载-问题-Error-No-such-keg-usr-local-Cellar"><a href="#安装或卸载-问题-Error-No-such-keg-usr-local-Cellar" class="headerlink" title="安装或卸载 问题 Error: No such keg: /usr/local/Cellar/****"></a>安装或卸载 问题 Error: No such keg: /usr/local/Cellar/****</h5><pre><code>方法一：  brew cleanup   brew uninstall ***  brew install ***方法二：    如果方法一不生效，则查看是否有版本号，或者软件名错误</code></pre>]]></content>
      
      
      <categories>
          
          <category> brew </category>
          
      </categories>
      
      
        <tags>
            
            <tag> brew-使用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cassandra-常用篇</title>
      <link href="/2019/05/15/cassandra-%E5%B8%B8%E7%94%A8%E7%AF%87/"/>
      <url>/2019/05/15/cassandra-%E5%B8%B8%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="http://www.myoak.info/post/12/" target="_blank" rel="noopener">cassandra mac os 安装</a></p></blockquote><h4 id="shell-命令"><a href="#shell-命令" class="headerlink" title="shell 命令"></a>shell 命令</h4><h5 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h5><pre><code>cqlsh -u cassandra -p cassandra --connect-timeout 300</code></pre><h5 id="查看集群信息"><a href="#查看集群信息" class="headerlink" title="查看集群信息"></a>查看集群信息</h5><pre><code>nodetool  describecluster</code></pre><h5 id="查看节点状态"><a href="#查看节点状态" class="headerlink" title="查看节点状态"></a>查看节点状态</h5><pre><code>nodetool    status</code></pre><h5 id="重启节点"><a href="#重启节点" class="headerlink" title="重启节点"></a>重启节点</h5><pre><code>sudo systemctl restart scylla-server</code></pre><h5 id="查看系统相关日志信息"><a href="#查看系统相关日志信息" class="headerlink" title="查看系统相关日志信息"></a>查看系统相关日志信息</h5><pre><code>sudo journalctl --follow _UID=`id -u scylla`</code></pre><hr><h4 id="Cql操作"><a href="#Cql操作" class="headerlink" title="Cql操作"></a>Cql操作</h4><h5 id="查看版本"><a href="#查看版本" class="headerlink" title="查看版本"></a>查看版本</h5><pre><code>SHOW VERSIONOR SELECT release_version from system.local;</code></pre><h5 id="查看Schema-信息"><a href="#查看Schema-信息" class="headerlink" title="查看Schema 信息"></a>查看Schema 信息</h5><pre><code> SELECT * FROM system_schema.keyspaces;</code></pre><h5 id="keyspaces-操作"><a href="#keyspaces-操作" class="headerlink" title="keyspaces 操作"></a>keyspaces 操作</h5><pre><code>#创建CREATE KEYSPACE tutorialspointWITH replication = {&#39;class&#39;:&#39;SimpleStrategy&#39;, &#39;replication_factor&#39; : 3};#修改ALTER KEYSPACE tutorialspointWITH replication = {&#39;class&#39;:&#39;NetworkTopologyStrategy&#39;, &#39;replication_factor&#39; : 3};#删除DROP KEYSPACE “KeySpace name”#查看desc keyspaces;#使用use keyspaceName</code></pre><h5 id="table-操作"><a href="#table-操作" class="headerlink" title="table 操作"></a>table 操作</h5><pre><code>#创建表#1CREATE TABLE user_action.user_history (   device_id text PRIMARY KEY,   user_history text,   dmp_tag text,   install_pkg text) WITH bloom_filter_fp_chance = 0.01   AND caching = {&#39;keys&#39;: &#39;ALL&#39;, &#39;rows_per_partition&#39;: &#39;ALL&#39;}   AND comment = &#39;m user history action, tag, pkg  records&#39;   AND compaction = {&#39;class&#39;: &#39;SizeTieredCompactionStrategy&#39;, &#39;max_threshold&#39;: &#39;32&#39;, &#39;min_threshold&#39;: &#39;4&#39;}   AND compression = {&#39;chunk_length_kb&#39;: &#39;1&#39;, &#39;crc_check_chance&#39;: &#39;0.000010&#39;, &#39;sstable_compression&#39;: &#39;org.apache.cassandra.io.compress.LZ4Compressor&#39;}   AND crc_check_chance = 1.0   AND dclocal_read_repair_chance = 0.1   AND default_time_to_live = 15552000   AND gc_grace_seconds = 864000   AND max_index_interval = 2048   AND memtable_flush_period_in_ms = 0   AND min_index_interval = 128   AND read_repair_chance = 0.0   AND speculative_retry = &#39;99.0PERCENTILE&#39;;#2CREATE TABLE test.test_user_action_history (    device_id text,    user_history text,    dmp_tag text,    install_pkg text,    PRIMARY KEY (device_id)) WITH comment=&#39;test user history action records&#39;    AND caching = {&#39;keys&#39;: &#39;ALL&#39;, &#39;rows_per_partition&#39;: &#39;ALL&#39;}    AND compaction = {&#39;class&#39;: &#39;SizeTieredCompactionStrategy&#39;, &#39;max_threshold&#39;: &#39;32&#39;, &#39;min_threshold&#39;: &#39;4&#39;}    AND compression = {&#39;chunk_length_kb&#39;: &#39;1&#39;, &#39;crc_check_chance&#39;: &#39;0.000010&#39;, &#39;sstable_compression&#39;: &#39;org.apache.cassandra.io.compress.LZ4Compressor&#39;};##添加列ALTER TABLE emp ADD emp_email text;##删除列ALTER TABLE emp DROP emp_email;  # 删除表DROP TABLE emp;#截断表（删除所有数据）表结构保留TRUNCATE student;#创建索引CREATE INDEX name ON emp1 (emp_name);#删除索引drop index name;#bacth 处理BEGIN BATCH... INSERT INTO emp (emp_id, emp_city, emp_name, emp_phone, emp_sal) values(  4,&#39;Pune&#39;,&#39;rajeev&#39;,9848022331, 30000);... UPDATE emp SET emp_sal = 50000 WHERE emp_id =3;... DELETE emp_city FROM emp WHERE emp_id = 2;... APPLY BATCH;#table listdesc tables;#查看表结构desc table &#39;table_name&#39;;</code></pre><h4 id="CURD-操作"><a href="#CURD-操作" class="headerlink" title="CURD 操作"></a>CURD 操作</h4><pre><code>#插入INSERT INTO emp (emp_id, emp_name, emp_city,   emp_phone, emp_sal) VALUES(1,&#39;ram&#39;, &#39;Hyderabad&#39;, 9848022338, 50000);#修改UPDATE emp SET emp_city=&#39;Delhi&#39;,emp_sal=50000   WHERE emp_id=2;#读取select #删除DELETE emp_sal FROM emp WHERE emp_id=3;</code></pre><h5 id="修改keyspaces-的副本"><a href="#修改keyspaces-的副本" class="headerlink" title="修改keyspaces 的副本"></a>修改keyspaces 的副本</h5><pre><code> ALTER KEYSPACE library WITH REPLICATION = { &#39;class&#39; : &#39;SimpleStrategy&#39;, &#39;replication_factor&#39; : 1 };</code></pre><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><table><thead><tr><th>数据类型</th><th>常量</th><th>描述</th></tr></thead><tbody><tr><td>ascii</td><td>strings</td><td>表示ASCII字符串</td></tr><tr><td>bigint</td><td>bigint</td><td>表示64位有符号长</td></tr><tr><td>blob</td><td>blobs</td><td>表示任意字节</td></tr><tr><td>Boolean</td><td>booleans</td><td>表示true或false</td></tr><tr><td>counter</td><td>integers</td><td>表示计数器列</td></tr><tr><td>decimal</td><td>integers, floats</td><td>表示变量精度十进制</td></tr><tr><td>double</td><td>integers</td><td>表示64位IEEE-754浮点</td></tr><tr><td>float</td><td>integers, floats</td><td>表示32位IEEE-754浮点</td></tr><tr><td>inet</td><td>strings</td><td>表示一个IP地址，IPv4或IPv6</td></tr><tr><td>int</td><td>integers</td><td>表示32位有符号整数</td></tr><tr><td>text</td><td>strings</td><td>表示UTF8编码的字符串</td></tr><tr><td>timestamp</td><td>integers, strings</td><td>表示时间戳</td></tr><tr><td>timeuuid</td><td>uuids</td><td>表示类型1 UUID</td></tr><tr><td>uuid</td><td>uuids</td><td>表示类型1或类型4</td></tr><tr><td></td><td></td><td>UUID</td></tr><tr><td>varchar</td><td>strings</td><td>表示uTF8编码的字符串</td></tr><tr><td>varint</td><td>integers</td><td>表示任意精度整数</td></tr></tbody></table><h4 id="集合类型"><a href="#集合类型" class="headerlink" title="集合类型"></a>集合类型</h4><blockquote><p>Cassandra查询语言还提供了一个集合数据类型。下表提供了CQL中可用的集合的列表。</p></blockquote><table><thead><tr><th>集合</th><th>描述</th><th>备注</th></tr></thead><tbody><tr><td>list</td><td>列表是一个或多个有序元素的集合。</td><td>将保持元素的顺序，并且值将被多次存储。</td></tr><tr><td>map</td><td>map是键值对的集合。</td><td></td></tr><tr><td>set</td><td>集合是一个或多个元素的集合。</td><td></td></tr></tbody></table><h5 id="list"><a href="#list" class="headerlink" title="list"></a>list</h5><pre><code>CREATE TABLE data(name text PRIMARY KEY, email list&lt;text&gt;);INSERT INTO data(name, email) VALUES (&#39;ramu&#39;,[&#39;abc@gmail.com&#39;,&#39;cba@yahoo.com&#39;])UPDATE data SET email = email +[&#39;xyz@tutorialspoint.com&#39;] where name = &#39;ramu&#39;;SELECT * FROM data;name | email------+-------------------------------------------------------------- ramu | [&#39;abc@gmail.com&#39;, &#39;cba@yahoo.com&#39;, &#39;xyz@tutorialspoint.com&#39;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> cassandra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cassandra-常用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cassandra-学习篇</title>
      <link href="/2019/05/15/cassandra-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/05/15/cassandra-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h3><blockquote><p>Apache Cassandra 是一个开源的、分布式、无中心、弹性可扩展、高可用、容错、一致性可调、面向行的数据库，它基于 Amazon Dynamo 的分布式设计和 Google Bigtable 的数据模型，Cassandra 其协议是 P2P 的，并使用 gossip 来维护存活或死亡节点的列表（gossip 协议介绍：<a href="https://www.iteblog.com/archives/2505.html）由" target="_blank" rel="noopener">https://www.iteblog.com/archives/2505.html）由</a> Facebook 创建，在一些最流行的网站中得到应用.</p></blockquote><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><h4 id="分布式和去中心化（Distributed-and-Decentralized）："><a href="#分布式和去中心化（Distributed-and-Decentralized）：" class="headerlink" title="分布式和去中心化（Distributed and Decentralized）："></a>分布式和去中心化（Distributed and Decentralized）：</h4><blockquote><p>可以运行在多台机器上，并呈现给用户一个一致的整体；</p></blockquote><blockquote><p>Cassandra 是无中心的，也就是说每个节点都是一样的，协议是 P2P 的，并使用 gossip 来维护存活或死亡节点的列表。</p></blockquote><h4 id="弹性可扩展："><a href="#弹性可扩展：" class="headerlink" title="弹性可扩展："></a>弹性可扩展：</h4><blockquote><p>集群可以不间断的情况下，方便扩展或缩减服务的规模，不需要重新启动进程，不必修改应用的查询，也无需自己手工重新均衡数据分布，只要加入新的计算机，Cassandra 就会自动地发现它并让它开始工作。</p></blockquote><h4 id="高可用和容错："><a href="#高可用和容错：" class="headerlink" title="高可用和容错："></a>高可用和容错：</h4><blockquote><p>可以在不中断系统的情况下替换故障节点，还可以把数据分布到多个数据中心里，从而提供更好的本地访问性能，并且在某一数据中心发生火灾、洪水等不可抗灾难的时候防止系统彻底瘫痪</p></blockquote><h4 id="可调节一致性："><a href="#可调节一致性：" class="headerlink" title="可调节一致性："></a>可调节一致性：</h4><blockquote><ol><li>通过副本因子（replication factor），你可以决定准备牺牲多少性能来换取一致性。 副本因子是你要求更新在集群中传播到的节点数（注意，更新包括所有增加、删除和更新操作）。</li></ol></blockquote><blockquote><ol start="2"><li>一致性级别（consistency level）参数，这个参数决定了多少个副本写入成功才可以认定写操作是成功的，或者读取过程中读到多少个副本正确就可以认定是读成功的。这里 Cassandra 把决定一致性程度的权利留给了客户自己。</li></ol></blockquote><blockquote><ol start="3"><li>所以，如果需要的话，你可以设定一致性级别和副本因子相等，从而达到一个较高的一致性水平，不过这样就必须付出同步阻塞操作的代价，只有所有节点都被更新完成才能成功返回一次更新。而实际上，Cassandra 一般都不会这么来用，原因显而易见（这样就丧失了可用性目标，影响性能，而且这不是你选择 Cassandra 的初衷）。而如果一个客户端设置一致性级别低于副本因子的话，即使有节点宕机了，仍然可以写成功。</li></ol></blockquote><blockquote><ol start="4"><li>总体来说，Cassandra 更倾向于 CP，虽然它也可以通过调节一致性水平达到 AP；但是不推荐你这么设置。其CAP 定律的详细介绍可参见<a href="https://www.iteblog.com/archives/2352.html" target="_blank" rel="noopener">《分布式系统一致性问题、CAP定律以及 BASE 理论》</a>以及<a href="https://www.iteblog.com/archives/2390.html" target="_blank" rel="noopener">《一篇文章搞清楚什么是分布式系统 CAP 定理》</a>。</li></ol></blockquote><h4 id="面向行"><a href="#面向行" class="headerlink" title="面向行"></a>面向行</h4><blockquote><p>它的数据结构不是关系型的，而是一个多维稀疏哈希表。稀疏（Sparse）意味着任何一行都可能会有一列或者几列。更确切地说，应该把 Cassandra 看做是一个有索引的、面向行的存储系统。</p></blockquote><h4 id="灵活的模式（Flexible-Schema）"><a href="#灵活的模式（Flexible-Schema）" class="headerlink" title="灵活的模式（Flexible Schema）"></a>灵活的模式（Flexible Schema）</h4><blockquote><p>从 3.0 版本开始，不推荐使用基于 Thrift API 的动态列创建的 API，并且 Cassandra 底层存储已经重新实现了，以更紧密地与 CQL 保持一致。 Cassandra 并没有完全限制动态扩展架构的能力，但它的工作方式却截然不同。 CQL 集合（比如 list、set、尤其是 map）提供了在无结构化的格式里面添加内容的能力，从而能扩展现有的模式。CQL 还提供了改变列的类型的能力，以支持 JSON 格式的文本的存储。</p></blockquote><h4 id="高性能-High-Performance"><a href="#高性能-High-Performance" class="headerlink" title="高性能(High Performance)"></a>高性能(High Performance)</h4><blockquote><p>设计之初就特别考虑了要充分利用多处理器和多核计算机的性能，并考虑在分布于多个数据中心的大量这类服务器上运行。它可以一致而且无缝地扩展到数百台机器，存储数 TB 的数据。Cassandra 已经显示出了高负载下的良好表现，在一个非常普通的工作站上，Cassandra 也可以提供非常高的写吞吐量。而如果你增加更多的服务器，你还可以继续保持 Cassandra 所有的特性而无需牺牲性能</p></blockquote><hr><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><h4 id="大规模部署"><a href="#大规模部署" class="headerlink" title="大规模部署"></a>大规模部署</h4><blockquote><p>单节点不易发挥它的性能，多个节点部署cassandra才是最佳选择</p></blockquote><h4 id="写密集、统计和分析型工作"><a href="#写密集、统计和分析型工作" class="headerlink" title="写密集、统计和分析型工作"></a>写密集、统计和分析型工作</h4><blockquote><p>Cassandra 是为优异的写吞吐量而特别优化的。 早期用于存储用户状态更新、社交网络、建议/评价以及应用统计都是很好的应用场景。现又适用于窗口化的时间序列数据库，用于文档搜索的反向索引，以及分布式任务优先级队列。</p></blockquote><h4 id="地区分布"><a href="#地区分布" class="headerlink" title="地区分布"></a>地区分布</h4><blockquote><p>支持多地分布的数据存储，Cassandra 可以很容易配置成将数据分布到多个数据中心的存储方式。如果你有一个全球部署的应用，那么让数据贴近用户会获得不错的性能收益，Cassandra 正适合这种应用场合。</p></blockquote><h4 id="变化的应用"><a href="#变化的应用" class="headerlink" title="变化的应用"></a>变化的应用</h4><blockquote><p>正在“初创阶段”，业务会不断改进，Cassandra 这种灵活的模式的数据模型可能更适合你。这让你的数据库能更快地跟上业务改进的步伐。</p></blockquote><blockquote><p>参考地址：<a href="https://www.iteblog.com/archives/2530.html" target="_blank" rel="noopener">cassandra 简介</a></p></blockquote><hr><h4 id="Single-column-Primary-Key"><a href="#Single-column-Primary-Key" class="headerlink" title="Single column Primary Key"></a>Single column Primary Key</h4><blockquote><p>Primary Key 可以由一列或多列组成,用于从表中检索数据，如果 Primary Key 由一列组成，那么称为 Single column Primary Key&gt; 如下：</p></blockquote><pre><code>CREATE TABLE iteblog_user (first_name text , last_name text, PRIMARY KEY (first_name)) ;</code></pre><blockquote><p>我们在检索数据的时候需要指定 Primary Key,不指定查询数据会抛以下异常：</p></blockquote><pre><code> InvalidRequest: Error from server: code=2200 [Invalid query]  message=&quot;Cannot execute this query as it might involve data filtering and thus may have unpredictable performance.  If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING&quot;</code></pre><h4 id="Composite-Primary-Key"><a href="#Composite-Primary-Key" class="headerlink" title="Composite Primary Key"></a>Composite Primary Key</h4><blockquote><p>如果 Primary Key 由多列组成，那么这种情况称为 Compound Primary Key 或 Composite Primary Key  如下：</p></blockquote><pre><code>CREATE TABLE iteblog_user_composite (first_name text , last_name text, PRIMARY KEY (first_name, last_name)) ;</code></pre><blockquote><p>其中 first_name 称为 Partition key，last_name 称为 Clustering key（也可以称为 Clustering column）。在这种情况下，下面查询的前三条都是合法的，最后一条是非法的。</p></blockquote><pre><code>cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite;cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite where first_name = &#39;iteblog&#39;;cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite where first_name = &#39;iteblog&#39; and last_name = &#39;hadoop&#39;;//非法查询cqlsh:iteblog_keyspace&gt; select * from iteblog_user_composite where last_name = &#39;hadoop&#39;;</code></pre><blockquote><p>Partition key 和 Clustering key(查询的时候不可以仅指定，需要和partition key 组合) 也可以由多个字段组成，如果 Partition key 由多个字段组成，称之为 Composite partition key：</p></blockquote><pre><code>create table iteblog_multiple (      k_part_one text,      k_part_two int,      k_clust_one text,      k_clust_two int      data text,      PRIMARY KEY((k_part_one, k_part_two), k_clust_one, k_clust_two)        );</code></pre><blockquote><p>小知识：使用 Composite partition key 的一个原因<br>其实一个 Partition 对应的 Cell 个数在 Cassandra 里面是有限制的。理论上来说，一个 Partition 的 Cell 个数大约在20亿个（231）。所以采用了 Composite partition key，我们可以将数据分散到不同的 Partition，这样有利于将同一个 Partition 的 Cell 个数减少。</p></blockquote><h4 id="Partition-key-amp-Clustering-key-amp-Primary-Key-作用"><a href="#Partition-key-amp-Clustering-key-amp-Primary-Key-作用" class="headerlink" title="Partition key &amp; Clustering key &amp; Primary Key 作用"></a>Partition key &amp; Clustering key &amp; Primary Key 作用</h4><blockquote><p>Partition Key：将数据分散到集群的 node 上</p></blockquote><blockquote><p>Primary Key：在 Single column Primary Key 情况下作用和 Partition Key 一样；在 Composite Primary Key 情况下，组合 Partition key 字段决定数据的分发的节点；</p></blockquote><blockquote><p>Clustering Key：决定同一个分区内相同 Partition Key 数据的排序，默认为升序，我们可以在建表语句里面手动设置排序的方式（DESC 或 ASC）</p></blockquote><p>参考链接:<a href="https://www.iteblog.com/archives/2534.html" target="_blank" rel="noopener">Apache Cassandra Composite KeyPartition keyClustering key 介绍</a></p><hr>]]></content>
      
      
      <categories>
          
          <category> cassandra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cassandra-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码库-邮件发送</title>
      <link href="/2019/05/06/%E4%BB%A3%E7%A0%81%E5%BA%93-%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81/"/>
      <url>/2019/05/06/%E4%BB%A3%E7%A0%81%E5%BA%93-%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81/</url>
      
        <content type="html"><![CDATA[<h4 id="scala-版本示例："><a href="#scala-版本示例：" class="headerlink" title="scala 版本示例："></a>scala 版本示例：</h4><pre><code>import org.slf4j.LoggerFactoryimport javax.mail._import javax.mail.internet.InternetAddressimport javax.mail.internet.MimeMessageimport java.util.Propertiesobject Mail {  val logger = LoggerFactory.getLogger(Mail.getClass)  val bodyHtml = &quot;&lt;!DOCTYPE html PUBLIC -//W3C//DTD HTML 4.01 Transitional//ENhttp://www.w3.org/TR/html4/loose.dtd&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=Content-Type content=text/html; charset=utf-8 pageEncoding=UTF-8&gt;&lt;/head&gt;&lt;body&gt;%s&lt;/body&gt;&lt;/html&gt;&quot;  val prop = new Properties()  prop.put(&quot;mail.smtp.host&quot;,&quot;smtp.exmail.qq.com&quot;)  prop.put(&quot;mail.smtp.auth&quot;,&quot;true&quot;)  prop.put(&quot;mail.smtp.connectiontimeout&quot;,&quot;10000&quot;)  prop.put(&quot;mail.smtp.timeout&quot;,&quot;20000&quot;)  def send(address: String, title: String, content: String) {    try {      val addresses = address.split(&quot;,&quot;).map(new InternetAddress(_).asInstanceOf[Address])      val authenticator = new SMTPAuthenticator(&quot;username@qq.com&quot;, &quot;password&quot;)      val sendMailSession = Session.getDefaultInstance(prop, authenticator)      val newMessage = new MimeMessage(sendMailSession)      newMessage.setFrom(new InternetAddress(&quot;username@qq.com&quot;))      newMessage.setRecipients(Message.RecipientType.TO, addresses)      newMessage.setSubject(title)      val html = String.format(bodyHtml, content)      newMessage.setContent(html, &quot;text/html;charset=utf-8&quot;)      Transport.send(newMessage)      logger.info(&quot;send an email to address[{}] title[{}] content[{}]&quot;, addresses, title, content);    } catch {      case e: MessagingException =&gt; logger.info(&quot;error occur when mail&quot;, e)    }  }  class SMTPAuthenticator(username: String, password: String) extends Authenticator {    override def getPasswordAuthentication: PasswordAuthentication = new PasswordAuthentication(username, password)  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 代码库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代码库-邮件发送 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-问题篇</title>
      <link href="/2019/04/29/spark-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/04/29/spark-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h5 id="org-apache-spark-SparkException-Could-not-find-CoarseGrainedScheduler"><a href="#org-apache-spark-SparkException-Could-not-find-CoarseGrainedScheduler" class="headerlink" title="org.apache.spark.SparkException: Could not find CoarseGrainedScheduler."></a>org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.</h5><blockquote><ol><li>这个可能是一个资源问题，应该给任务分配更多的 cores 和Executors，并且分配更多的内存。并且需要给RDD分配更多的分区 </li><li>在配置资源中加入这句话也许能解决你的问题：<br> –conf spark.dynamicAllocation.enabled=false</li><li>经过一般调试，发现原来是因为spark任务生成task任务过少，而任务提交时所指定的Excutor 数过多导致，故调小 –num-executors 参数问题得以解决。</li></ol></blockquote><hr><h5 id="心跳超时"><a href="#心跳超时" class="headerlink" title="心跳超时"></a>心跳超时</h5><blockquote><p>报错信息：</p></blockquote><pre><code>WARN executor.Executor: Issue communicating with driver in heartbeaterorg.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval    at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)    at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:785)    at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:814)    at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)    at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)    at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:814)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]    at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)    at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)    ... 14 more19/10/21 07:00:29 ERROR executor.Executor: Exit as unable to send heartbeats to driver more than 60 times</code></pre><blockquote><p>解决方法：</p></blockquote><pre><code>      --conf spark.executor.heartbeatInterval=15s       --conf spark.network.timeout=120s       --conf spark.core.connection.ack.wait.timeout=300</code></pre><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="sg"><a href="#sg" class="headerlink" title="sg"></a>sg</h4>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>macOS-使用篇</title>
      <link href="/2019/04/29/macOS-%E4%BD%BF%E7%94%A8%E7%AF%87/"/>
      <url>/2019/04/29/macOS-%E4%BD%BF%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="快捷键："><a href="#快捷键：" class="headerlink" title="快捷键："></a>快捷键：</h4><pre><code>Command（或 Cmd）⌘    Shift ⇧Option（或 Alt）⌥     Control（或 Ctrl）⌃Caps Lock ⇪          Fn#窗口管理退出关闭应用：Command+Q最小化当前窗口: command+m  #文本管理打开文件系统： Command+O 新建文件夹：Command-Shift-N将文件移至废纸篓： Command-Delete清倒废纸篓： Command-Shift-Delete***#### 目录地址：&gt; 软件的安装目录</code></pre><p>~/Library/Application Support</p><pre><code>&gt; brew 安装路径</code></pre><p>/usr/local/Cellar/</p><pre><code>&gt; 系统环境加载顺序</code></pre><p>/etc/profile<br>/etc/paths<br>~/.bash_profile<br>~/.bash_login<br>~/.profile<br>~/.bashrc</p><p>```</p><hr><h4 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h4><blockquote><p><a href="https://blog.csdn.net/k12104/article/details/84104261" target="_blank" rel="noopener">Mac Beyond Compare4 破解方法</a></p></blockquote><blockquote><p><a href="https://learnku.com/articles/23834" target="_blank" rel="noopener">tldr 命令帮助</a></p></blockquote><h4 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h4><blockquote><p><a href="lovesofttech.com/mac/macPort/">mac 的netstat 与 linux netstat 不同</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> macOS-使用篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>技术文章</title>
      <link href="/2019/04/29/%E7%BB%8F%E5%85%B8%E6%96%87%E7%AB%A0/"/>
      <url>/2019/04/29/%E7%BB%8F%E5%85%B8%E6%96%87%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h4 id="技术文章"><a href="#技术文章" class="headerlink" title="技术文章"></a>技术文章</h4><blockquote><ol><li><a href="https://tech.meituan.com/2014/06/30/mysql-index.html" target="_blank" rel="noopener">MySQL索引原理及慢查询优化</a></li><li><a href="https://tech.meituan.com/2018/10/18/meishi-data-flink.html" target="_blank" rel="noopener">美团点评基于 Flink 的实时数仓建设实践</a></li><li><a href="https://mp.weixin.qq.com/s/lhP4B6jA0CQpjT1PdlYmQw" target="_blank" rel="noopener">从零开始入门推荐算法工程师</a></li><li><a href="https://yq.aliyun.com/articles/691816?spm=a2c4e.11153940.blogcont691499.9.2be51d53KvnU8f" target="_blank" rel="noopener">计算广告与流处理技术综述</a></li></ol></blockquote><h4 id="学习规划文章"><a href="#学习规划文章" class="headerlink" title="学习规划文章"></a>学习规划文章</h4><p>1.<a href="https://zhuanlan.zhihu.com/p/29608190" target="_blank" rel="noopener">面试过阿里等互联网大公司，我知道了这些套路</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术文章 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-调优篇</title>
      <link href="/2019/04/28/spark-%E8%B0%83%E4%BC%98%E7%AF%87/"/>
      <url>/2019/04/28/spark-%E8%B0%83%E4%BC%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h3><h4 id="分配资源："><a href="#分配资源：" class="headerlink" title="分配资源："></a>分配资源：</h4><pre><code>spark-submit \--class cn.spark.sparktest.core.WordCountCluster \--num-executors 80 \  配置executor的数量--driver-memory 6g \  配置driver的内存（影响不大）--executor-memory 6g \  配置每个executor的内存大小--executor-cores 3 \  配置每个executor的cpu core数量(RDD cache/shuffle/task执行)--master yarn-cluster \--queue root.default \--conf spark.yarn.executor.memoryOverhead=2048 \  executor堆外内存--conf spark.core.connection.ack.wait.timeout=300 \ 连接的超时时长/usr/local/spark/spark.jar \${1}spark-submit \  --master yarn \  --deploy-mode cluster \  --executor-cores 3 \  --executor-memory 10G \  --driver-memory 4G \  --conf spark.dynamicAllocation.enabled=true \  --conf spark.shuffle.service.enabled=true \  --conf spark.dynamicAllocation.initialExecutors=5 \  --conf spark.dynamicAllocation.maxExecutors=40 \  --conf spark.dynamicAllocation.minExecutors=0 \  --conf spark.dynamicAllocation.executorIdleTimeout=30s \  --conf spark.dynamicAllocation.schedulerBacklogTimeout=10s \</code></pre><hr><h4 id="SparkStreaming-优雅退出"><a href="#SparkStreaming-优雅退出" class="headerlink" title="SparkStreaming 优雅退出"></a>SparkStreaming 优雅退出</h4><pre><code>    public static void main(String[] args) throws Exception{        Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR);//String checkpointPath = PropertiesUtil.getProperty(&quot;streaming.checkpoint.path&quot;);        JavaStreamingContext javaStreamingContext = JavaStreamingContext.getOrCreate(&quot;hdfs://Master:9000/streaming_checkpoint&quot;, createContext());        javaStreamingContext.start();        每隔20秒钟监控是否有停止指令,如果有则优雅退出streaming        final Properties serverProps = PropertiesUtil.properties;        Thread thread = new Thread(new MonitorStopThread(javaStreamingContext,serverProps));        thread.start();        javaStreamingContext.awaitTermination();       }    }</code></pre><hr><h4 id="调节并行度："><a href="#调节并行度：" class="headerlink" title="调节并行度："></a>调节并行度：</h4><blockquote><ol><li>并行度：其实就是指的是，Spark作业中，各个stage的task数量，也就代表了Spark作业的在各个阶段（stage）的并行度。</li><li>官方是推荐，task数量，设置成spark application总cpu core数量的2<del>3倍，比如150个cpu core，基本要设置task数量为300</del>500；</li><li>SparkConf conf = new SparkConf().set(“spark.default.parallelism”, “500”)</li></ol></blockquote><hr><h4 id="InputDStream并行化数据接收"><a href="#InputDStream并行化数据接收" class="headerlink" title="InputDStream并行化数据接收"></a>InputDStream并行化数据接收</h4><blockquote><p>创建多个InputDStream来接收同一数据源,把多个topic数据细化为单一的kafkaStream来接收</p><blockquote><ol><li>创建kafkaStream</li></ol></blockquote></blockquote><pre><code>  Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;();   kafkaParams.put(&quot;metadata.broker.list&quot;, &quot;192.168.1.164:9092,192.168.1.165:9092,192.168.1.166:9092&quot;);   kafkaParams.put(&quot;zookeeper.connect&quot;,&quot;master:2181,data1:2181,data2:2181&quot;);   构建topic set   String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);   String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);   Set&lt;String&gt; topics = new HashSet&lt;String&gt;();   for(String kafkaTopic : kafkaTopicsSplited) {       topics.add(kafkaTopic);   JavaPairInputDStream&lt;String, String&gt; kafkaStream = KafkaUtils.createDirectStream(       jssc,        String.class,        String.class,        StringDecoder.class,        StringDecoder.class,        kafkaParams,        topics);</code></pre><blockquote><blockquote><ol start="2"><li>InputDStream并行化数据接收</li></ol></blockquote></blockquote><pre><code> ```   int numStreams = 5;   List&lt;JavaPairDStream&lt;String, String&gt;&gt; kafkaStreams = new   ArrayList&lt;JavaPairDStream&lt;String,String&gt;&gt;(numStreams);   for (int i = 0; i &lt; numStreams; i++) {       kafkaStreams.add(KafkaUtils.createStream(...));       }   JavaPairDStream&lt;String, String&gt; unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));  unifiedStream.print();```</code></pre><hr><h4 id="增加block数量，增加每个batch-rdd的partition数量，增加处理并行度"><a href="#增加block数量，增加每个batch-rdd的partition数量，增加处理并行度" class="headerlink" title="增加block数量，增加每个batch rdd的partition数量，增加处理并行度"></a>增加block数量，增加每个batch rdd的partition数量，增加处理并行度</h4><pre><code>第一步：receiver从数据源源源不断地获取到数据，首先是会按照block interval，将指定时间间隔的数据，收集为一个block；默认时间是200ms，官方推荐不要小于50ms；第二步：根据指定batch interval时间间隔合并为一个batch，创建为一个rdd，第三步：启动一个job，去处理这个batch rdd中的数据。第四步：batch rdd 的partition数量是多少呢？一个batch有多少个block，就有多少个partition；就意味着并行度是多少；就意味着每个batch rdd有多少个task会并行计算和处理。调优：如果希望可以比默认的task数量和并行度再多一些，可以手动调节blockinterval，减少block interval。每个batch可以包含更多的block。因此也就有更多的partition，因此就会有更多的task并行处理每个batch rdd。</code></pre><h4 id="重分区，增加每个batch-rdd的partition数量"><a href="#重分区，增加每个batch-rdd的partition数量" class="headerlink" title="重分区，增加每个batch rdd的partition数量"></a>重分区，增加每个batch rdd的partition数量</h4><p>inputStream.repartition()：重分区，增加每个batch rdd的partition数量<br>对dstream中的rdd进行重分区为指定数量的分区，就可以提高指定dstream的rdd的计算并行度<br>调节并行度</p><hr><h4 id="重构RDD架构以及RDD持久化："><a href="#重构RDD架构以及RDD持久化：" class="headerlink" title="重构RDD架构以及RDD持久化："></a>重构RDD架构以及RDD持久化：</h4><blockquote><ol><li>RDD架构重构与优化</li><li>公共RDD一定要实现持久化,对于要多次计算和使用的公共RDD，一定要进行持久化。</li><li>持久化，是可以进行序列化的<br>sessionid2actionRDD=sessionid2actionRDD.persist(StorageLevel.MEMORY_ONLY());<pre><code>```  MEMORY_ONLY    直接以Java对象的形式存储于JVM的内存中 MYMORY_AND_DISK    存储于JVM的内存+磁盘 MEMORY_ONLY_SER    序列化存储于内存中 MEMORY_AND_DISK_SER    序列化存储于内存+磁盘 ```</code></pre></li><li>为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化</li></ol></blockquote><hr><h4 id="实现RDD高可用性：启动WAL预写日志机制"><a href="#实现RDD高可用性：启动WAL预写日志机制" class="headerlink" title="实现RDD高可用性：启动WAL预写日志机制"></a>实现RDD高可用性：启动WAL预写日志机制</h4><blockquote><p>spark streaming，从原理上来说，是通过receiver来进行数据接收的；接收到的数据，会被划分成一个一个的block；block会被组合成一个batch；针对一个batch，会创建一个rdd；<br>receiver接收到数据后，就会立即将数据写入一份到容错文件系统（比如hdfs）上的checkpoint目录中的，另一份写入到磁盘文件中去；作为数据的冗余副本。无论你的程序怎么挂掉，或者是数据丢失，那么数据都不肯能会永久性的丢失；因为肯定有副本。</p></blockquote><pre><code>  SparkConf conf = new SparkConf()                     .setMaster(&quot;local[2]&quot;)              .setAppName(&quot;StreamingSpark&quot;);            .set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;);             .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);              .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;);                  .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);     JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5));   jssc.checkpoint(&quot;hdfs://192.168.1.164:9000/checkpoint&quot;);</code></pre><hr><h4 id="广播大变量（1m-100m）："><a href="#广播大变量（1m-100m）：" class="headerlink" title="广播大变量（1m~100m）："></a>广播大变量（1m~100m）：</h4><blockquote><ol><li>默认的情况下，task执行的算子中，使用了外部的变量，每个task都会获取一份变量的副本，有什么缺点呢？<pre><code> 网络传输的开销、耗费内存、RDD持久化到内存（内存不够，持续到磁盘）、task创建对象导致gc；</code></pre></li></ol></blockquote><blockquote><ol start="2"><li>广播变量，初始的时候，就在Drvier上有一份副本。</li></ol></blockquote><pre><code>task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中；此后这个executor上的task，都会直接使用本地的BlockManager中的副本。executor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，举例越近越好。sc.boradcast();</code></pre><hr><h4 id="使用Kryo序列化"><a href="#使用Kryo序列化" class="headerlink" title="使用Kryo序列化:"></a>使用Kryo序列化:</h4><blockquote><ol><li>默认情况下，Spark内部是使用Java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。</li><li>Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。</li><li>Kryo序列化机制，一旦启用以后，会生效的几个地方：<pre><code> ``` 1、算子函数中使用到的外部变量 2、持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER 3、shuffle    .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)    .set(&quot;spark.default.parallelism&quot;, &quot;1000&quot;);       .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;);           .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);       .registerKryoClasses(new Class[]{CategorySortKey.class})</code></pre></li></ol></blockquote><pre><code>  &gt; 4. 序列化       ``` 1、在SparkConf中设置一个属性，spark.serializer，org.apache.spark.serializer.KryoSerializer类；        2、注册你使用到的，需要通过Kryo序列化的，一些自定义类，SparkConf.registerKryoClasses() </code></pre><hr><h4 id="使用fastutil优化数据格式"><a href="#使用fastutil优化数据格式" class="headerlink" title="使用fastutil优化数据格式:"></a>使用fastutil优化数据格式:</h4><blockquote><ol><li>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；</li><li>fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度；</li></ol></blockquote><p>fastutil也提供了64位的array、set和list，以及高性能快速的，以及实用的IO类，来处理二进制和文本类型的文件；fastutil最新版本要求Java 7以及以上版本；</p><p>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</p><p>fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。<br>fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。</p><blockquote><ol start="3"><li>maven 依赖</li></ol></blockquote><pre><code>  &lt;dependency&gt;    &lt;groupId&gt;fastutil&lt;/groupId&gt;    &lt;artifactId&gt;fastutil&lt;/artifactId&gt;    &lt;version&gt;5.0.9&lt;/version&gt;  &lt;/dependency&gt;</code></pre><hr><h4 id="调节数据本地化等待时长："><a href="#调节数据本地化等待时长：" class="headerlink" title="调节数据本地化等待时长："></a>调节数据本地化等待时长：</h4><blockquote><ol><li>PROCESS_LOCAL：进程本地化；NODE_LOCAL：节点本地化；NO_PREF：对于task来说，没有好坏之分；RACK_LOCAL：机架本地化；ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差；</li><li>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL（不用调节）、NODE LOCAL、ANY（调节一下数据本地化的等待时长），反复调节，每次调节完以后，再来运行，观察日志</li><li>怎么调节？</li></ol></blockquote><pre><code>spark.locality.wait，默认是3s；6s，10s默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3sspark.locality.wait.processspark.locality.wait.nodespark.locality.wait.racknew SparkConf()  .set(&quot;spark.locality.wait&quot;, &quot;10&quot;)</code></pre><hr><h4 id="定时清除不需要的数据"><a href="#定时清除不需要的数据" class="headerlink" title="定时清除不需要的数据"></a>定时清除不需要的数据</h4><blockquote><ol><li>通过配置spark.cleaner.ttl为一个合理的值，但是这个值不能过小，因为如果后面计算需要用的数据被清除会带来不必要的麻烦。</li><li>另外通过配置spark.streaming.unpersist为true(默认就是true)来更智能地去持久化（unpersist）RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为。</li></ol></blockquote><hr><h4 id="去除压缩-内存充足的情况下"><a href="#去除压缩-内存充足的情况下" class="headerlink" title="去除压缩 (内存充足的情况下)"></a>去除压缩 (内存充足的情况下)</h4><p>在内存充足的情况下，可以设置spark.rdd.compress 设置为false.</p><hr><h3 id="Yarn-优化"><a href="#Yarn-优化" class="headerlink" title="Yarn 优化"></a>Yarn 优化</h3><h4 id="Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配"><a href="#Executors和cpu核心数设置和Spark-On-Yarn-动态资源分配" class="headerlink" title="Executors和cpu核心数设置和Spark On Yarn 动态资源分配"></a>Executors和cpu核心数设置和Spark On Yarn 动态资源分配</h4><blockquote><p>首先需要对YARN的NodeManager进行配置，使其支持Spark的Shuffle Service。</p></blockquote><pre><code>  #修改  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;  &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;  &lt;/property&gt;  #增加  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;  &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;  &lt;name&gt;spark.shuffle.service.port&lt;/name&gt;  &lt;value&gt;7337&lt;/value&gt;  &lt;/property&gt;</code></pre><blockquote><p>将spark中对应jar包拷贝到hadoop的目录下：</p></blockquote><pre><code>  首先找到spark版本的spark-&lt;version&gt;-yarn-shuffle.jar  shuffle包，并将该包放到集群所有NodeManager的classpath下，  比如放到HADOOP_HOME/share/hadoop/yarn/lib</code></pre><hr><h3 id="JVM-调优"><a href="#JVM-调优" class="headerlink" title="JVM 调优"></a>JVM 调优</h3><h4 id="原理概述以及降低cache操作的内存占比"><a href="#原理概述以及降低cache操作的内存占比" class="headerlink" title="原理概述以及降低cache操作的内存占比:"></a>原理概述以及降低cache操作的内存占比:</h4><blockquote><ol><li>full gc / minor gc，无论是快，还是慢，都会导致jvm的工作线程停止工作，stop the world。简而言之，就是说，gc的时候，spark停止工作了。等着垃圾回收结束。</li><li>spark中，堆内存又被划分成了两块儿，存储内存和执行内存；<pre><code> 一句话，让task执行算子函数时，有更多的内存可以使用。</code></pre></li></ol></blockquote><hr><h4 id="GC优化策略-暂时不确定"><a href="#GC优化策略-暂时不确定" class="headerlink" title="GC优化策略(暂时不确定)"></a>GC优化策略(暂时不确定)</h4><p>建议用并行Mark-Sweep垃圾回收机制，虽然它消耗更多的资源，但是我们还是建议开启。<br>在spark-submit中使用<br>–driver-java-options “-XX:+UseConcMarkSweepGC”<br>–conf “spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC”</p><h4 id="调节executor堆外内存与连接等待时长-在spark-sbmit中修改-："><a href="#调节executor堆外内存与连接等待时长-在spark-sbmit中修改-：" class="headerlink" title="调节executor堆外内存与连接等待时长(在spark-sbmit中修改)："></a>调节executor堆外内存与连接等待时长(在spark-sbmit中修改)：</h4><blockquote><ol><li>有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行，时不时的报错，shuffle file cannot find，executor、task lost，out of memory（内存溢出）；–conf spark.yarn.executor.memoryOverhead=2048</li><li>有时候，无法建立网络连接；会卡住；ok，spark默认的网络连接的超时时长，是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了:一串file id。uuid（dsfsfd-2342vs–sdf–sdfsd）。not found。file lost。<br> –conf spark.core.connection.ack.wait.timeout=300</li></ol></blockquote><hr><h3 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h3><h4 id="原理概述："><a href="#原理概述：" class="headerlink" title="原理概述："></a>原理概述：</h4><blockquote><ol><li>在spark中，主要是以下几个算子：groupByKey、reduceByKey、countByKey、join，等等。</li><li>shuffle，一定是分为两个stage来完成的。因为这其实是个逆向的过程，不是stage决定shuffle，是shuffle决定stage。</li><li>shuffle前半部分的task在写入数据到磁盘文件之前，都会先写入一个一个的内存缓冲，内存缓冲满溢之后，再spill溢写到磁盘文件中。</li></ol></blockquote><hr><h4 id="合并map端输出文件："><a href="#合并map端输出文件：" class="headerlink" title="合并map端输出文件："></a>合并map端输出文件：</h4><blockquote><ol><li>开启shuffle map端输出文件合并的机制；默认情况下，是不开启的，就是会发生如上所述的大量map端输出文件的操作，严重影响性能。</li><li>new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)<br>new SparkConf().set(“spark.shuffle.consolidateFiles”, “true”)</li></ol></blockquote><hr><h4 id="合并map端输出文件：-1"><a href="#合并map端输出文件：-1" class="headerlink" title="合并map端输出文件："></a>合并map端输出文件：</h4><blockquote><ol><li>map端内存缓冲：spark.shuffle.file.buffer，默认32k  <pre><code> reduce端内存占比：spark.shuffle.memoryFraction，0.2</code></pre></li><li>调节的时候的原则。spark.shuffle.file.buffer，每次扩大一倍，然后看看效果，64，128；<br>spark.shuffle.memoryFraction，每次提高0.1，看看效果。<br>不能调节的太大，太大了以后过犹不及，因为内存资源是有限的，你这里调节的太大了，其他环节的内存使用就会有问题了。</li></ol></blockquote><pre><code>new SparkConf().set(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)new SparkConf().set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.3&quot;)</code></pre><hr><h4 id="HashShuffleManager与SortShuffleManager"><a href="#HashShuffleManager与SortShuffleManager" class="headerlink" title="HashShuffleManager与SortShuffleManager"></a>HashShuffleManager与SortShuffleManager</h4><blockquote><ol><li>spark.shuffle.manager：hash、sort、tungsten-sort（自己实现内存管理），spark 1.2.x版本以后，默认的shuffle manager，是SortShuffleManager。<br>   spark.shuffle.sort.bypassMergeThreshold：200（默认值为200）</li><li>SortShuffleManager会避免像HashShuffleManager那样，默认就去创建多份磁盘文件。一个task，只会写入一个磁盘文件，不同reduce task的数据，用offset来划分界定。</li></ol></blockquote><pre><code>new SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)new SparkConf().set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;)</code></pre><hr><h3 id="算子调优"><a href="#算子调优" class="headerlink" title="算子调优"></a>算子调优</h3><h4 id="MapPartitions提升Map类操作性能"><a href="#MapPartitions提升Map类操作性能" class="headerlink" title="MapPartitions提升Map类操作性能:"></a>MapPartitions提升Map类操作性能:</h4><blockquote><ol><li>如果是普通的map，比如一个partition中有1万条数据；function要执行和计算1万次。但是，使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。</li><li>但是，可能就OOM，内存溢出。   </li></ol></blockquote><hr><h4 id="filter过后使用coalesce减少分区数量："><a href="#filter过后使用coalesce减少分区数量：" class="headerlink" title="filter过后使用coalesce减少分区数量："></a>filter过后使用coalesce减少分区数量：</h4><blockquote><ol><li>就会导致有些task运行的速度很快；有些task运行的速度很慢。这就是数据倾斜。</li><li>coalesce算子：主要就是用于在filter操作之后，针对每个partition的数据量各不相同的情况，来压缩partition的数量。减少partition的数量，而且让每个partition的数据量都尽量均匀紧凑。</li></ol></blockquote><hr><h4 id="foreachPartition优化写数据库性能："><a href="#foreachPartition优化写数据库性能：" class="headerlink" title="foreachPartition优化写数据库性能："></a>foreachPartition优化写数据库性能：</h4><pre><code>&gt; 1. 用了foreachPartition算子之后，好处在哪里？    1、对于我们写的function函数，就调用一次，一次传入一个partition所有数据；    2、主要创建或者获取一个数据库连接就可以；    3、只要向数据库发送一次SQL语句和多组参数即可；&gt; 2. 很有可能会发生OOM，内存溢出的问题。    一个partition大概是1千条左右用foreach，跟用foreachPartition，性能的提升达到了2~3分钟。</code></pre><hr><h4 id="repartition解决Spark-SQL低并行度的性能问题："><a href="#repartition解决Spark-SQL低并行度的性能问题：" class="headerlink" title="repartition解决Spark SQL低并行度的性能问题："></a>repartition解决Spark SQL低并行度的性能问题：</h4><pre><code>repartition算子，你用Spark SQL这一步的并行度和task数量，肯定是没有办法去改变了。但是呢，可以将你用Spark SQL查询出来的RDD，使用repartition算子，去重新进行分区，此时可以分区成多个partition，比如从20个partition，分区成100个。</code></pre><hr><h4 id="reduceByKey本地聚合介绍："><a href="#reduceByKey本地聚合介绍：" class="headerlink" title="reduceByKey本地聚合介绍："></a>reduceByKey本地聚合介绍：</h4><pre><code>reduceByKey，相较于普通的shuffle操作（比如groupByKey），它的一个特点，就是说，会进行map端的本地聚合</code></pre><h3 id="代码-调优"><a href="#代码-调优" class="headerlink" title="代码 调优"></a>代码 调优</h3><h4 id="进行HA机制处理-针对Driver高可用性"><a href="#进行HA机制处理-针对Driver高可用性" class="headerlink" title="进行HA机制处理-针对Driver高可用性"></a>进行HA机制处理-针对Driver高可用性</h4><blockquote><p>在创建和启动StreamingContext的时候，将元数据写入容错的文件系统（比如hdfs）。保证在driver挂掉之后，spark集群可以自己将driver重新启动起来；而且driver在启动的时候，不会重新创建一个streaming context，而是从容错文件系统（比如hdfs）中读取之前的元数据信息，包括job的执行进度，继续接着之前的进度，继续执行。使用这种机制，就必须使用cluster模式提交，确保driver运行在某个worker上面；</p></blockquote><pre><code>  JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() {      @Override     public JavaStreamingContext create() {           JavaStreamingContext jssc = new JavaStreamingContext(...);          JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...);          jssc.checkpoint(checkpointDirectory);          return jssc;        }      }; JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory); context.start(); context.awaitTermination();</code></pre><pre><code>JavaStreamingContext.getOrCreate 基于Function0&lt; JavaStreamingContext &gt; 进行Driver高可用 Function0&lt;JavaStreamingContext&gt; createContextFunc = new Function0&lt;JavaStreamingContext&gt;(){     @Override     public JavaStreamingContext call() throws Exception     {         conf = new SparkConf()                 .setMaster(&quot;local[4]&quot;)                 .setAppName(&quot;java/RealTimeStreaming&quot;)                 .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)                 .set(&quot;spark.default.parallelism&quot;, &quot;10&quot;)                 .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;)                 .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);         Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();         kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);         kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);         kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);         kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);         kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);         kafkaParams.put(&quot;enable.auto.commit&quot;,true);         JavaStreamingContext jssc = new JavaStreamingContext(                 conf, Durations.seconds(30));         jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;);         // 构建topic set         String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);         String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);         Set&lt;String&gt; topics = new HashSet&lt;String&gt;();         for(String kafkaTopic : kafkaTopicsSplited) {             topics.add(kafkaTopic);         }         JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc,                 LocationStrategies.PreferConsistent(),                 ConsumerStrategies.Subscribe(topics, kafkaParams));         hostMap = adRealTimeLogDStream.mapToPair(record -&gt; new Tuple2&lt;String, String&gt;(record.key(), record.value()));         logPeakDstream  = hostMap.mapToPair(new PairFunction&lt;Tuple2&lt;String, String&gt;, String, Long&gt;() {             @Override             public Tuple2&lt;String,Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception {                 String log = tuple._2;                 String[] logSplited = log.split(&quot;\\|&quot;);                 String eventTime= logSplited[1];                 String todayDate = DATE_FORMAT.format(new Date()).trim();                 String cutTime= eventTime.substring(13,eventTime.length()-7);                 String ip = logSplited[0].trim();                 String host = logSplited[14].trim();                 return new Tuple2&lt;String, Long&gt;(host+&quot;-&quot;+ip, 1L);             }         });         hostReduce = logPeakDstream.reduceByKeyAndWindow(new Function2&lt;Long, Long, Long&gt;() {             @Override             public Long call(Long v1, Long v2) throws Exception {                 return v1 + v2;             }         }, Durations.minutes(10),Durations.seconds(30));         JavaPairDStream&lt;String, Long&gt; topNPairRdd = hostReduce.transformToPair(new Function&lt;JavaPairRDD&lt;String, Long&gt;, JavaPairRDD&lt;String, Long&gt;&gt;() {             @Override             public JavaPairRDD&lt;String, Long&gt; call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception {                 JavaPairRDD&lt;Long, String&gt; sortRDD = (JavaPairRDD&lt;Long, String&gt;) rdd.mapToPair(record -&gt; new Tuple2&lt;Long, String&gt;(record._2, record._1));                 JavaPairRDD&lt;String, Long&gt; sortedRdd = (JavaPairRDD&lt;String, Long&gt;) sortRDD.sortByKey(false).mapToPair(record -&gt; new Tuple2&lt;String, Long&gt;(record._2, record._1));                 List&lt;Tuple2&lt;String, Long&gt;&gt; topNs = sortedRdd.take(5);//取前5个输出                 System.out.println(&quot;                                                 &quot;);                 System.out.println(&quot;*****************峰值访问窗统计*******************&quot;);                 for (Tuple2&lt;String, Long&gt; topN : topNs) {                     System.out.println(topN);                 }                 System.out.println(&quot;**********************END***********************&quot;);                 System.out.println(&quot;                                                 &quot;);                 return sortedRdd;             }         });         topNPairRdd.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Long&gt;&gt;() {             @Override             public void call(JavaPairRDD&lt;String, Long&gt; rdd) throws Exception {             }         });         logDetailDstream = hostMap.map(new Function&lt;Tuple2&lt;String,String&gt;, String&gt;() {             @Override             public String call(Tuple2&lt;String, String&gt; tuple) throws Exception {                 String log = tuple._2;                 String[] logSplited = log.split(&quot;\\|&quot;);                 String eventTime= logSplited[1];                 String todayDate = DATE_FORMAT.format(new Date()).trim();                 String cutTime= eventTime.substring(13,eventTime.length()-7);                 String[] urlDetails = logSplited[7].split(&quot;/&quot;);                 String ip = logSplited[0].trim();                 String url =&quot;&quot;;                 if(urlDetails.length==4){                     url = urlDetails[3];                 }else if(urlDetails.length==5){                     url = urlDetails[3] + &quot;/&quot; + urlDetails[4];                 }else if(urlDetails.length&gt;=6){                     url = urlDetails[3] + &quot;/&quot; + urlDetails[4]+ &quot;/&quot; + urlDetails[5];                 }                 String host = logSplited[14].trim();                 String dataTime =todayDate +&quot; &quot;+ cutTime;                 String bytesSent = logSplited[5].trim();                 return  dataTime+&quot; &quot;+host+&quot; &quot;+ip+&quot; &quot;+url+&quot; &quot;+bytesSent;             }         });         //logDetailDstream.print();         return jssc;     } }; return createContextFunc;</code></pre><pre><code> 提交方式  spark-submit          --deploy-mode cluster          --supervise</code></pre><hr><h4 id="SparkStreaming-与kafka整合调优"><a href="#SparkStreaming-与kafka整合调优" class="headerlink" title="SparkStreaming 与kafka整合调优"></a>SparkStreaming 与kafka整合调优</h4><blockquote><p>LocationStrategies 位置策略：</p></blockquote><pre><code>The new Kafka consumer API will pre-fetch messages into buffers. Therefore it is important for performance reasons that the Spark integration keep cached consumers on executors (rather than recreating them for each batch), and prefer to schedule partitions on the host locations that have the appropriate consumers.</code></pre><blockquote><p>新的Kafka消费者API可以预获取消息缓存到缓冲区，因此Spark整合Kafka让消费者在executor上进行缓存对性能是非常有助的，可以调度消费者所在主机位置的分区。</p></blockquote><pre><code>In most cases, you should use LocationStrategies.PreferConsistent as shown above. This will distribute partitions evenly across available executors. If your executors are on the same hosts as your Kafka brokers, use PreferBrokers,which will prefer to schedule partitions on the Kafka leader for that partition. Finally, if you have a significant skew in load among partitions, use PreferFixed. This allows you to specify an explicit mapping of partitions to hosts (any unspecified partitions will use a consistent location).</code></pre><blockquote><p>通常，你可以使用 LocationStrategies.PreferConsistent，这个策略会将分区分布到所有可获得的executor上。如果你的executor和kafkabroker在同一主机上的话，可以使用PreferBrokers，这样kafka leader会为此分区进行调度。最后，如果你加载数据有倾斜的话可以使用PreferFixed，这将允许你制定一个分区和主机的映射（没有指定的分区将使用PreferConsistent 策略）</p></blockquote><pre><code>The cache for consumers has a default maximum size of 64. If you expect to be handling more than (64 * number of executors) Kafka partitions, you can change this settingvia spark.streaming.kafka.consumer.cache.maxCapacity</code></pre><blockquote><p>消费者默认缓存大小是64，如果你期望处理较大的Kafka分区的话，你可以使用</p></blockquote><pre><code>spark.streaming.kafka.consumer.cache.maxCapacity设置大小。The cache is keyed by topicpartition and group.id, so use a separate group.id for each call to createDirectStream.</code></pre><blockquote><p>缓存是使用key为topic partition 和组id的，因此对于每一次调用 createDirectStream 可以使用不同的 group . id</p></blockquote><pre><code>public  static SparkConf  conf = new SparkConf()                .setMaster(&quot;local[4]&quot;)                .setAppName(&quot;java/RealTimeStreaming&quot;)                .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)                .set(&quot;spark.default.parallelism&quot;, &quot;10&quot;)                .set(&quot;spark.streaming.blockInterval&quot;, &quot;50&quot;)                .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;, &quot;true&quot;);Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;Master:9092,Worker1:9092,Worker2:9092&quot;);kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);kafkaParams.put(&quot;group.id&quot;, &quot;TestGroup&quot;);kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);kafkaParams.put(&quot;enable.auto.commit&quot;,true);JavaStreamingContext jssc = new JavaStreamingContext(        conf, Durations.seconds(30));jssc.checkpoint(&quot;hdfs://Master:9000/checkpoint&quot;);</code></pre><blockquote><p>构建topic set</p></blockquote><pre><code>String kafkaTopics = ConfigurationManager.getProperty(Constants.KAFKA_TOPICS);String[] kafkaTopicsSplited = kafkaTopics.split(&quot;,&quot;);Set&lt;String&gt; topics = new HashSet&lt;String&gt;();for(String kafkaTopic : kafkaTopicsSplited) {    topics.add(kafkaTopic);        }JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; adRealTimeLogDStream = KafkaUtils.createDirectStream(jssc,        LocationStrategies.PreferConsistent(),        ConsumerStrategies.Subscribe(topics, kafkaParams));</code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark-调优篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>grafana-资料篇</title>
      <link href="/2019/04/28/grafana-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/04/28/grafana-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h5 id="参考地址："><a href="#参考地址：" class="headerlink" title="参考地址："></a>参考地址：</h5><blockquote><ol><li><a href="https://grafana.com" target="_blank" rel="noopener">官网</a></li><li><a href="https://grafana.com/docs/tutorials/screencasts/" target="_blank" rel="noopener">官网视频</a></li></ol></blockquote><h5 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h5><blockquote><ol><li><a href="https://grafana.com/docs/plugins/installation/" target="_blank" rel="noopener">插件安装步骤</a></li><li><a href="https://grafana.com/plugins" target="_blank" rel="noopener">插件搜索</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> grafana </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grafana-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink-资料篇</title>
      <link href="/2019/04/26/flink-%E8%B5%84%E6%96%99%E7%AF%87/"/>
      <url>/2019/04/26/flink-%E8%B5%84%E6%96%99%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="flink-学习（系列）"><a href="#flink-学习（系列）" class="headerlink" title="flink 学习（系列）"></a>flink 学习（系列）</h4><blockquote><ol><li><a href="https://flink.apache.org/" target="_blank" rel="noopener">flink官网</a></li><li><a href="https://training.ververica.com" target="_blank" rel="noopener">training</a></li><li><a href="https://github.com/flink-china/flink-training-course/blob/master/%E8%AF%BE%E7%A8%8B%E8%A1%A8%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89.md" target="_blank" rel="noopener">flink-阿里巴巴课程</a></li><li><a href="https://blog.csdn.net/yanghua_kobe/column/info/apacheflink" target="_blank" rel="noopener">flink-vinoYang博客</a></li><li><a href="http://wuchong.me/" target="_blank" rel="noopener">flink-云邪博客</a></li><li><a href="https://blog.csdn.net/liguohuabigdata/article/category/7279020" target="_blank" rel="noopener">云星数据-大数据团队</a></li><li><a href="https://www.cnblogs.com/niutao/category/1421715.html" target="_blank" rel="noopener">flink-归来朝歌博客</a></li><li><a href="https://yq.aliyun.com/teams/67/type_blog?spm=a2c4e.11153940.0.0.2f6557a6hSnfer" target="_blank" rel="noopener">阿里云实时计算</a></li></ol></blockquote><hr><h4 id="flink-代码"><a href="#flink-代码" class="headerlink" title="flink 代码"></a>flink 代码</h4><blockquote><ol><li><a href="https://github.com/apache/flink" target="_blank" rel="noopener">flink</a></li><li><a href="https://github.com/zhisheng17/flink-learning/" target="_blank" rel="noopener">flink-learning</a></li><li><a href="https://github.com/ververica" target="_blank" rel="noopener">flink-training-exercises</a></li></ol></blockquote><hr><h4 id="flink-书籍"><a href="#flink-书籍" class="headerlink" title="flink 书籍"></a>flink 书籍</h4><blockquote><ol><li>flink基础教程</li><li>[#]</li></ol></blockquote><hr><h4 id="博客-单篇"><a href="#博客-单篇" class="headerlink" title="博客(单篇)"></a>博客(单篇)</h4><blockquote><ol><li><a href="https://www.ververica.com/what-is-stream-processing" target="_blank" rel="noopener">什么是流处理</a></li></ol></blockquote><h4 id="理解Flink的设计原则"><a href="#理解Flink的设计原则" class="headerlink" title="理解Flink的设计原则"></a>理解Flink的设计原则</h4><blockquote><p><a href="https://blog.csdn.net/ffjl1985/article/details/77961714" target="_blank" rel="noopener">Google Stream 101越了批处理的流处理世界</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/ffjl1985/article/details/78018475" target="_blank" rel="noopener">Google Stream 102超越了批处理的流处理世界</a></p></blockquote><h4 id="Flink原理和实现"><a href="#Flink原理和实现" class="headerlink" title="Flink原理和实现"></a>Flink原理和实现</h4><blockquote><p><a href="https://cloud.tencent.com/developer/article/1172583" target="_blank" rel="noopener">追源索骥：透过源码看懂Flink核心框架的执行流程</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/180418" target="_blank" rel="noopener">Flink的架构和拓扑概览</a></p></blockquote><blockquote><p><a href="https://cloud.tencent.com/developer/article/1172583" target="_blank" rel="noopener">Flink核心框架的执行流程</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/64819" target="_blank" rel="noopener">理解 Flink 中的计算资源</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/225618" target="_blank" rel="noopener">Flink如何生成ExecutionGraph及物理执行图</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/259125" target="_blank" rel="noopener">Flink 生成StreamGraph</a></p></blockquote><h4 id="Flink-Window"><a href="#Flink-Window" class="headerlink" title="Flink Window"></a>Flink Window</h4><blockquote><p><a href="https://yq.aliyun.com/articles/225624" target="_blank" rel="noopener">Flink Window的实现原理</a></p></blockquote><blockquote><p><a href="https://www.jianshu.com/p/a69811b41bc3" target="_blank" rel="noopener">Flink Window的实现原理：Session Window</a></p></blockquote><blockquote><p><a href="https://tech.youzan.com/flink-sliding-window/" target="_blank" rel="noopener">Flink 滑动窗口优化</a></p></blockquote><h4 id="Flink-State"><a href="#Flink-State" class="headerlink" title="Flink State"></a>Flink State</h4><blockquote><p><a href="https://yq.aliyun.com/articles/225623" target="_blank" rel="noopener">Flink中的状态管理</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/64821" target="_blank" rel="noopener">Flink中的反压Back-Pressure</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/225621" target="_blank" rel="noopener">Flink Operator Chain原理</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/57815" target="_blank" rel="noopener">Flink内存管理</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/lmalds/article/details/54925877" target="_blank" rel="noopener">Flink异步快照机制-Failover</a></p></blockquote><blockquote><p><a href="http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/" target="_blank" rel="noopener">数据流的类型和操作</a></p></blockquote><blockquote><p><a href="https://www.jianshu.com/p/d8f99d94b761" target="_blank" rel="noopener">Flink Async IO</a></p></blockquote><blockquote><p><a href="https://www.jianshu.com/p/2e379f242272" target="_blank" rel="noopener">Flink源码解析 Stream Operator</a></p></blockquote><h4 id="Flink-SQL"><a href="#Flink-SQL" class="headerlink" title="Flink SQL"></a>Flink SQL</h4><blockquote><p><a href>Flink SQL的大部分代码实现是阿里巴巴的Blink团队贡献给Apache的</a>。</p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/457438" target="_blank" rel="noopener">Flink SQL 核心功能解密</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/457385" target="_blank" rel="noopener">Flink SQL维表Join和异步优化</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/457370" target="_blank" rel="noopener">Flink SQL 异步IO设计</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/457356" target="_blank" rel="noopener">Flink SQL数据去重的技巧和思考</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/457445" target="_blank" rel="noopener">Flink SQL TOP N的挑战与实现</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/457392" target="_blank" rel="noopener">Flink SQL 流计算“撤回(Retraction)”案例分析</a></p></blockquote><blockquote><p><a href="https://yq.aliyun.com/articles/448853" target="_blank" rel="noopener">Flink SQL 解决热点问题的大杀器MiniBatch</a></p></blockquote><blockquote><p><a href="https://www.jianshu.com/p/6bc5b4e6f163" target="_blank" rel="noopener">Flink Table API&amp;SQL的概念和通用API</a></p></blockquote><h4 id="Flink-CEP复杂事件处理"><a href="#Flink-CEP复杂事件处理" class="headerlink" title="Flink CEP复杂事件处理"></a>Flink CEP复杂事件处理</h4><h5 id="系列1"><a href="#系列1" class="headerlink" title="系列1"></a>系列1</h5><blockquote><p><a href="https://blog.csdn.net/yanghua_kobe/article/details/60151617" target="_blank" rel="noopener">Flink-CEP论文与源码解读之状态与状态转换</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/yanghua_kobe/article/details/58714692" target="_blank" rel="noopener">Flink之CEP-API简介</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/yanghua_kobe/article/details/59169473" target="_blank" rel="noopener">Flink之CEP案例分析-网络攻击检测</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/yanghua_kobe/article/details/60986433" target="_blank" rel="noopener">Flink-CEP之NFA</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/yanghua_kobe/article/details/61430843" target="_blank" rel="noopener">Flink-CEP之NFA编译器</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/yanghua_kobe/article/details/62477733" target="_blank" rel="noopener">Flink-CEP之模式流与运算符</a></p></blockquote><h5 id="系列2"><a href="#系列2" class="headerlink" title="系列2"></a>系列2</h5><blockquote><p><a href="http://www.liaojiayi.com/CEP-In-Flink-1/" target="_blank" rel="noopener">CEP In Flink (1) - CEP规则解析</a></p></blockquote><blockquote><p><a href="http://www.liaojiayi.com/CEP-In-Flink-2/" target="_blank" rel="noopener">CEP In Flink (2) - CEP规则匹配</a></p></blockquote><blockquote><p><a href="http://www.liaojiayi.com/CEP-In-Flink-3/" target="_blank" rel="noopener">CEP In Flink (3) - 匹配事件提取</a></p></blockquote><blockquote><p><a href="http://www.liaojiayi.com/CEP-In-Flink-4/" target="_blank" rel="noopener">CEP In Flink (4) - 使用瓶颈</a></p></blockquote><h4 id="Flink事务"><a href="#Flink事务" class="headerlink" title="Flink事务"></a>Flink事务</h4><blockquote><p><a href="https://www.itcodemonkey.com/article/8997.html" target="_blank" rel="noopener">Flink Streaming Ledger 支持流式处理ACID事务</a></p></blockquote><h4 id="Flink源码解析"><a href="#Flink源码解析" class="headerlink" title="Flink源码解析"></a>Flink源码解析</h4><blockquote><p><a href="https://www.jianshu.com/p/a3f43f861a42" target="_blank" rel="noopener">Apache Flink源码解析 DataStream API</a></p></blockquote><blockquote><p><a href>Flink Exactly Once语义</a><br><a href="https://data-artisans.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka" target="_blank" rel="noopener">flink的两阶段提交协议-实现端到端的Exactly Once语义</a></p></blockquote><h4 id="Flink案例"><a href="#Flink案例" class="headerlink" title="Flink案例"></a>Flink案例</h4><blockquote><p><a href="https://doc.flink-china.org/blog/apacheflinkinvipcom" target="_blank" rel="noopener">Flink在唯品会的实践</a></p></blockquote><blockquote><p><a href="https://doc.flink-china.org/blog/meituan.html" target="_blank" rel="noopener">Flink在美团的实践应用</a></p></blockquote><blockquote><p><a href="https://doc.flink-china.org/blog/g7onflink.html" target="_blank" rel="noopener">Flink在G7的实践</a></p></blockquote><blockquote><p><a href="https://doc.flink-china.org/blog/eleme.html" target="_blank" rel="noopener">Flink在饿了么的应用</a></p></blockquote><blockquote><p><a href="https://www.sohu.com/a/246673917_315839" target="_blank" rel="noopener">基于Flink的实时特征平台在Flink的应用</a></p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/BghNofoU6cPRn7XfdHR83w" target="_blank" rel="noopener">日均处理万亿数据！Flink在快手的应用实践与技术演进之路</a></p></blockquote><h4 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h4><blockquote><p><a href="https://tech.meituan.com/2017/11/17/flink-benchmark.html" target="_blank" rel="noopener">流计算框架 Flink 与 Storm 的性能对比</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink-资料篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink-问题篇</title>
      <link href="/2019/04/26/flink-%E9%97%AE%E9%A2%98%E7%AF%87/"/>
      <url>/2019/04/26/flink-%E9%97%AE%E9%A2%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="资源管理："><a href="#资源管理：" class="headerlink" title="资源管理："></a>资源管理：</h4><p>flink<br>vcore:<br>提示：并行度一定要小于solt数，也就是提交命令里 yn<em>ys的值，最好保持一致，这样可以将申请的资源充分利用。<br>计算方式：如果并行度小于solt数（yn</em>ys），则vocer默认是：并行度 +1 ；如果并行度和solt数(yn*ys)一致，则vcore数为yn * ys + 1<br>memory:<br>计算方式：<br>并行度和solt数保持一致时：    yn * yjm + ytm<br>并行度小于solt 数是：并行度 * yjm + ytm</p><h4 id="隐式转换器的异常："><a href="#隐式转换器的异常：" class="headerlink" title="隐式转换器的异常："></a>隐式转换器的异常：</h4><pre><code>错误信息：value foreach is not a member of java.util.ArrayList解决：import scala.collection.JavaConversions._错误信息：could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation解决：import org.apache.flink.streaming.api.scala._（DataStream API）     import org.apache.flink.api.scala._（DataSet API）</code></pre><hr><h4 id="如果在使用泛型参数的函数或类中使用Flink操作，则TypeInformation必须可用于该参数。这可以通过使用上下文绑定来实现"><a href="#如果在使用泛型参数的函数或类中使用Flink操作，则TypeInformation必须可用于该参数。这可以通过使用上下文绑定来实现" class="headerlink" title="如果在使用泛型参数的函数或类中使用Flink操作，则TypeInformation必须可用于该参数。这可以通过使用上下文绑定来实现"></a>如果在使用泛型参数的函数或类中使用Flink操作，则TypeInformation必须可用于该参数。这可以通过使用上下文绑定来实现</h4><pre><code>def myFunction[T: TypeInformation](input: DataSet[T]): DataSet[Seq[T]] = {  input.reduceGroup( i =&gt; i.toSeq )}</code></pre><hr><h4 id="ClassCastException：X无法强制转换为X"><a href="#ClassCastException：X无法强制转换为X" class="headerlink" title="ClassCastException：X无法强制转换为X"></a>ClassCastException：X无法强制转换为X</h4><pre><code>1.尝试classloader.resolve-order: parent-first在配置中进行设置2.从不同的执行尝试缓存类3.通过child-first类加载进行类复制</code></pre><hr><h4 id="AbstractMethodError或NoSuchFieldError"><a href="#AbstractMethodError或NoSuchFieldError" class="headerlink" title="AbstractMethodError或NoSuchFieldError"></a>AbstractMethodError或NoSuchFieldError</h4><pre><code>    存在依赖项版本冲突,确保所有依赖项版本都一致。</code></pre><hr><h4 id="事件正在进行，DataStream应用程序不产生任何输出"><a href="#事件正在进行，DataStream应用程序不产生任何输出" class="headerlink" title="事件正在进行，DataStream应用程序不产生任何输出"></a>事件正在进行，DataStream应用程序不产生任何输出</h4><pre><code>1.如果您的DataStream应用程序使用事件时间，请检查您的水印是否已更新。  如果没有产生水印，事件时间窗口可能永远不会触发，应用程序将不会产生任何结果。2.您可以在Flink的Web UI（水印部分）中查看水印是否正在取得进展。</code></pre><hr><h4 id="exception-reporting-“Insufficient-number-of-network-buffers”"><a href="#exception-reporting-“Insufficient-number-of-network-buffers”" class="headerlink" title="exception reporting “Insufficient number of network buffers”"></a>exception reporting “Insufficient number of network buffers”</h4><pre><code>1.原因可能：以很高的并发来执行flink ,增加了网络缓冲区的数量。  默认情况下，Flink占用网络缓冲区的JVM堆大小的10％，最小为64MB，最大为1GB。2.可以通过修改一下以下参数来调整：taskmanager.network.memory.fractiontaskmanager.network.memory.mintaskmanager.network.memory.max</code></pre><p><a href="https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#configuring-the-network-buffers" title="flink 配置信息" target="_blank" rel="noopener">flink 配置信息</a></p><hr><h4 id="由于javadoc错误无法构建maven项目"><a href="#由于javadoc错误无法构建maven项目" class="headerlink" title="由于javadoc错误无法构建maven项目"></a>由于javadoc错误无法构建maven项目</h4><pre><code>三个选择：1.修复错误2.禁用严格检查3.在建造时跳过Javadoc&lt;plugins&gt;  &lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt;    &lt;configuration&gt;      &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;    &lt;/configuration&gt;  &lt;/plugin&gt;&lt;/plugins&gt;构建时跳过javadocmvn -Dmaven.javadoc.skip=true verify</code></pre><h4 id="java-lang-ClassNotFoundException-scala-Product-class"><a href="#java-lang-ClassNotFoundException-scala-Product-class" class="headerlink" title="java.lang.ClassNotFoundException: scala.Product$class"></a>java.lang.ClassNotFoundException: scala.Product$class</h4><pre><code>原因：构建文件不对。您的scala版本是2.12.x但您使用的是scala版本2.11中编译的库。解决：查看构建文件的lib 中是否包含2.12的版本。</code></pre><h4 id="Flink-BucketingSink-crashes-with-NoClassDefFoundError-Lorg-apache-hadoop-fs-FileSystem"><a href="#Flink-BucketingSink-crashes-with-NoClassDefFoundError-Lorg-apache-hadoop-fs-FileSystem" class="headerlink" title="Flink BucketingSink crashes with NoClassDefFoundError: Lorg/apache/hadoop/fs/FileSystem"></a>Flink BucketingSink crashes with NoClassDefFoundError: Lorg/apache/hadoop/fs/FileSystem</h4><pre><code>    缺少hadoop 依赖：（ 版本根据自己的来）        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;            &lt;version&gt;2.7.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;            &lt;version&gt;2.7.1&lt;/version&gt;        &lt;/dependency&gt;</code></pre><hr><h4 id="Name-node-is-in-safe"><a href="#Name-node-is-in-safe" class="headerlink" title="Name node is in safe"></a>Name node is in safe</h4><pre><code>modeorg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create file/user/test. Name node is in safe mode.解决方法：bin/hadoop dfsadmin -safemode leave 参考：https://blog.csdn.net/hongweigg/article/details/7185328</code></pre><hr><h4 id="No-implicit-arguments-of-type-XXXXXX"><a href="#No-implicit-arguments-of-type-XXXXXX" class="headerlink" title="No implicit arguments of type: XXXXXX"></a>No implicit arguments of type: XXXXXX</h4><p>错误：could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation[Int]<br>解决办法：import org.apache.flink.api.scala._<br>        或者<br>        import org.apache.flink.streaming.api.scala._</p><hr><h4 id="Stack-trace-ExitCodeException-exitCode-1"><a href="#Stack-trace-ExitCodeException-exitCode-1" class="headerlink" title="Stack trace: ExitCodeException exitCode=1:"></a>Stack trace: ExitCodeException exitCode=1:</h4><blockquote><p>报错信息</p></blockquote><pre><code>Application application_1556525638125_0015 failed 1 times (global limit =2; local limit is =1) due to AM Container for appattempt_1556525638125_0015_000001 exited with exitCode: 1Failing this attempt.Diagnostics: Exception from container-launch.Container id: container_e04_1556525638125_0015_01_000001Exit code: 1Stack trace: ExitCodeException exitCode=1:</code></pre><blockquote><p>解决方式：</p></blockquote><pre><code>  1.查看提交flink 作业的版本是否和提交环境版本一致。</code></pre><h4 id="Checkpoint失败：Checkpoint-expired-before-completing"><a href="#Checkpoint失败：Checkpoint-expired-before-completing" class="headerlink" title="Checkpoint失败：Checkpoint expired before completing"></a>Checkpoint失败：Checkpoint expired before completing</h4><pre><code>        env.enableCheckpointing(1000L)        val checkpointConf = env.getCheckpointConfig        checkpointConf.setMinPauseBetweenCheckpoints(30000L)        checkpointConf.setCheckpointTimeout(8000L)</code></pre><blockquote><p> 解决方式：<br> 主要因为checkpointConf.setCheckpointTimeout(8000L)设置的太小了，默认是10min。<br> 这里只设置了8sec。当一个Flink App背压的时候（例如由外部组件异常引起），Barrier会流动的非常缓慢，导致Checkpoint时长飙升。</p></blockquote><h4 id="FlinkException：The-assigned-slot-container-XXX-was-removed-异常"><a href="#FlinkException：The-assigned-slot-container-XXX-was-removed-异常" class="headerlink" title="FlinkException：The assigned slot container_XXX was removed 异常"></a>FlinkException：The assigned slot container_XXX was removed 异常</h4><blockquote><p>问题原因：<br>一般就是某一个Flink App内存占用大，导致TaskManager（在Yarn上就是Container）被Kill掉。如果代码写的没问题，就确实是资源不够了，其实1G Slot跑多个Task（Slot Group Share）其实挺容易出现的。因此有两种选择。可以根据具体情况，权衡选择一个。<br>解决方式：</p><ol><li>将该Flink App调度在Per Slot内存更大的集群上。</li><li>通过slotSharingGroup(“xxx”)，减少Slot中共享Task的个数</li></ol></blockquote><h4 id="map-和list-声明方式"><a href="#map-和list-声明方式" class="headerlink" title="map 和list 声明方式"></a>map 和list 声明方式</h4><pre><code>val seMap: java.util.Map[String, java.util.Map[String, Object]] =      new java.util.HashMap[String, java.util.Map[String, Object]]()val topList: java.util.List[String] =      new java.util.ArrayList[String]()</code></pre><h4 id="Flink参数设置slot数量增加-导致作业无法启动"><a href="#Flink参数设置slot数量增加-导致作业无法启动" class="headerlink" title="Flink参数设置slot数量增加,导致作业无法启动"></a>Flink参数设置slot数量增加,导致作业无法启动</h4><p>报错信息：</p><pre><code>Insufficient number of network buffers: required 96, but only 25 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys &#39;taskmanager.network.memory.fraction&#39;, &#39;taskmanager.network.memory.min&#39;, and &#39;taskmanager.network.memory.max&#39;</code></pre><p>解决方法：</p><pre><code>调整Flink里面flink-conf.yaml里面的新增参数增加可支持的slot数量taskmanager.network.memory.fraction: 0.1taskmanager.network.memory.min: 268435456taskmanager.network.memory.max: 4294967296</code></pre><h4 id="flink-任务物理内存溢出问题定位"><a href="#flink-任务物理内存溢出问题定位" class="headerlink" title="flink 任务物理内存溢出问题定位"></a>flink 任务物理内存溢出问题定位</h4><pre><code>org.apache.flink.yarn.YarnTaskExecutorRunner - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.</code></pre><p>问题定位：<a href="https://www.jianshu.com/p/f18f0494e8ab" target="_blank" rel="noopener">物理内存溢出问题定位</a></p><h4 id="某个TaskManger丢失的问题排查"><a href="#某个TaskManger丢失的问题排查" class="headerlink" title="某个TaskManger丢失的问题排查"></a>某个TaskManger丢失的问题排查</h4><pre><code>ERROR [flink-akka.remote.default-remote-dispatcher-5] org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandler - Fetching TaskManager log failed.java.util.concurrent.CompletionException: java.io.IOException: TaskManager log files are unavailable. Log file could not be found at /mnt/disk1/log/hadoop-yarn/containers/application_1556227576661_152377/container_1556227576661_152377_01_000012/taskmanager.log</code></pre><p>地址：<a href="https://blog.csdn.net/u012164361/article/details/95317945" target="_blank" rel="noopener">问题解决地址</a></p><h4 id="flink-设置uid-以及savepoint"><a href="#flink-设置uid-以及savepoint" class="headerlink" title="flink 设置uid 以及savepoint"></a>flink 设置uid 以及savepoint</h4><pre><code>DataStream&lt;String&gt; stream = env.  // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(&quot;source-id&quot;) // ID for the source operator .shuffle() // Stateful mapper with ID .map(new StatefulMapper()) .uid(&quot;mapper-id&quot;)不人为指定ID，它们会被自动生成。只要ID不变化，则程序可以自动的从savepoint恢复。ID的生成依赖于程序的结构，并且对程序变化敏感。因此强烈建议人为分配ID可以将savepoint想象成持有每个有状态的操作的Operator ID到State的映射关系：Operator ID | State------------+------------------------source-id   | State of StatefulSourcemapper-id   | State of StatefulMapper在上面的例子中，print sink是无状态的，因此不是savepoint的一部分。默认情况下，会尝试映射savepoint的每条记录到新的程序中</code></pre><p>参考：</p><blockquote><p>flink官网：<a href="https://flink.apache.org/gettinghelp.html" target="_blank" rel="noopener">https://flink.apache.org/gettinghelp.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink-问题篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.flink-学习篇</title>
      <link href="/2019/04/26/flink-%E5%AD%A6%E4%B9%A0%E7%AF%87/"/>
      <url>/2019/04/26/flink-%E5%AD%A6%E4%B9%A0%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h4 id="什么是flink"><a href="#什么是flink" class="headerlink" title="什么是flink"></a>什么是flink</h4><blockquote><p>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算。</p></blockquote><h4 id="flink-特点"><a href="#flink-特点" class="headerlink" title="flink 特点:"></a>flink 特点:</h4><blockquote><ol><li>其异步和增量检查点算法确保对处理延迟的影响最小，同时保证一次性状态一致性。<ol start="2"><li>随处部署应用程序: 常见的集群资源管理器（如Hadoop YARN，Apache Mesos和Kubernetes）集成，但也可以设置为作为独立集群运行</li></ol></li><li>处理无界和有界数据</li><li>充分利用内存性能: 任务状态始终保留在内存中，如果状态大小超过可用内存，则保存在访问高效的磁盘上数据结构中<br><img src="https://flink.apache.org/img/local-state.png" alt title="flink 使用内存保存状态"></li></ol></blockquote><ul><li>性能：每天处理数万亿个事件；维护多个TB的状态；在数千个内核的运行</li></ul><hr><h4 id="flink丰富状态功能："><a href="#flink丰富状态功能：" class="headerlink" title="flink丰富状态功能："></a>flink丰富状态功能：</h4><blockquote><ol><li>多状态基元：Flink为不同的数据结构提供状态基元，例如原子值，列表或映射。</li><li>可插拔状态后端：应用程序状态由可插拔状态后端管理和检查点。(内存或RocksDB 存储或自定义)</li><li>完全一次的状态一致性。</li><li>非常大的状态：由于其异步和增量检查点算法，Flink能够维持几兆兆字节的应用程序状态。</li><li>可扩展的应用程序：Flink通过将状态重新分配给更多或更少的工作人员来支持有状态应用程序的扩展。</li></ol></blockquote><hr><h4 id="flink-的容错机制："><a href="#flink-的容错机制：" class="headerlink" title="flink 的容错机制："></a>flink 的容错机制：</h4><blockquote><ol><li>一致的检查点: 如果发生故障，将重新启动应用程序并从最新检查点加载其状态,此功能可以保证一次性状态一致性。</li><li>高效检查点: Flink可以执行异步和增量检查点，以便将检查点对应用程序的延迟SLA的影响保持在非常小的水平。</li><li>端到端完全一次: Flink为特定存储系统提供事务接收器，保证数据只写出一次，即使出现故障</li><li>与集群管理器集成: Flink与集群管理器紧密集成，例如Hadoop YARN，Mesos或Kubernetes。当进程失败时，将自动启动一个新进程来接管其工作。</li><li>高可用性设置: Flink具有高可用性模式，可消除所有单点故障。HA模式基于Apache ZooKeeper，这是一种经过验证的可靠分布式协调服务。</li><li>Savepoints操作：用于启动状态兼容的应用程序并初始化其状态。<blockquote><p>适用场景：</p><ol><li>程序版本升级。</li><li>程序迁移集群。</li><li>flink 版本更新。</li><li>暂停和恢复、存档。 </li><li>A/B测试：启动同一保存点的所有版本来比较两个（或更多）不同版本的应用程序的性能或质量。</li></ol></blockquote></li></ol></blockquote><hr><h4 id="flink-丰富time功能"><a href="#flink-丰富time功能" class="headerlink" title="flink 丰富time功能:"></a>flink 丰富time功能:</h4><blockquote><ol><li>Event-time Mode(事件时间):根据事件的时间戳计算结果</li><li>Watermark Support（水印支持）:使用水印来推断事件时间应用中的时间，也可以使用水印来推断事件时间应用中的时间。</li><li>Late Data Handling(延迟数据处理):当使用水印在事件 - 时间模式下处理流时，可能会在所有相关事件到达之前完成计算.</li><li>Processing-time Mode(处理时间):由处理机器的挂钟时间触发的计算,处理时间模式适用于具有严格的低延迟要求的某些应用，这些要求可以容忍近似结果.</li></ol></blockquote><hr><h4 id="flink-分层API"><a href="#flink-分层API" class="headerlink" title="flink 分层API:"></a>flink 分层API:</h4><blockquote><p><img src="https://flink.apache.org/img/api-stack.png" alt title="flink 分层API"></p></blockquote><blockquote><ol><li>ProcessFunctions:提供对时间和状态的细粒度控制,可以任意修改其状态并注册将在未来触发回调函数的定时器,因此，它可以根据许多有状态事件驱动的应用程序的需要实现复杂的事件业务逻辑:open -&gt; processElement -&gt; onTimer（<a href="https://flink.apache.org/usecases.html#eventDrivenApps" target="_blank" rel="noopener">相关介绍</a>）</li><li>DataStream API: 提供了许多常见的流处理操作，如窗口，记录在-A-时间变换，并丰富事件原语</li><li>SQL和Table API: Table API和SQL利用Apache Calcite进行解析，验证和查询优化,它们可以与DataStream和DataSet API无缝集成，并支持用户定义的标量，聚合和表值函数。</li></ol></blockquote><hr><h4 id="flink-Libraries"><a href="#flink-Libraries" class="headerlink" title="flink Libraries:"></a>flink Libraries:</h4><blockquote><ol><li>复杂事件处理（CEP）:CEP库的应用包括网络入侵检测，业务流程监控和欺诈检测。</li><li>DataSet API: 用于批处理应用程序的核心API。</li><li>Gelly：Gelly是一个可扩展的图形处理和分析库。</li></ol></blockquote><hr><h4 id="flink-监控方式："><a href="#flink-监控方式：" class="headerlink" title="flink 监控方式："></a>flink 监控方式：</h4><blockquote><ol><li>Web UI:可以检查，监视和调试正在运行的应用程序。</li><li>日志记录：Flink实现了流行的slf4j日志记录界面，并与日志框架log4j或logback集成。</li><li>指标：Flink具有复杂的指标系统，可收集和报告系统和用户定义的指标。</li><li>REST API：Flink公开REST API以提交新应用程序，获取正在运行的应用程序的保存点或取消应用程序。</li></ol></blockquote><hr><h4 id="flink-框架学习"><a href="#flink-框架学习" class="headerlink" title="flink 框架学习"></a>flink 框架学习</h4><h5 id="flink-datasource"><a href="#flink-datasource" class="headerlink" title="flink-datasource"></a>flink-datasource</h5>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink-学习篇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markDown-使用</title>
      <link href="/2019/04/25/markDown-%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/04/25/markDown-%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h6 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h6><pre><code># h1  最大## h2### h3#### h4##### h5###### h6 最小</code></pre><hr><h6 id="段落及区块引用"><a href="#段落及区块引用" class="headerlink" title="段落及区块引用"></a>段落及区块引用</h6><pre><code>  &gt;</code></pre><hr><h6 id="插入链接和图片"><a href="#插入链接和图片" class="headerlink" title="插入链接和图片"></a>插入链接和图片</h6><pre><code>链接 []()[点击跳转至百度](http://www.baidu.com)图片 ![图片alt](图片地址 &#39;&#39;图片title&#39;&#39;)![图片](https://user-gold-cdn.xitu.io/2018/4/18/162d75d959444389?w=1240&amp;h=703&amp;f=jpeg&amp;s=56927)</code></pre><hr><h6 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h6><pre><code>* | + | - 是无序列表1. 数字点加空格 是有序列表列表中加入了区块引用，区域引用标记符也需要缩进4个空格上一级和下一级之间敲三个空格即可示例：* 段落一    &gt; 区块标记一      &gt;&gt;区块标记二* 段落二    &gt; 区块标记二</code></pre><hr><h6 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h6><pre><code>  ***</code></pre><hr><h6 id="强调"><a href="#强调" class="headerlink" title="强调"></a>强调</h6><pre><code> *这里是斜体* **这里是加粗** ***这里是斜线加粗*** ～～这里是删除线～～</code></pre><hr><h6 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h6><pre><code> 单行代码：单反引号包裹 代码块：三个反引号包裹。 代码块: 第一个三点可以加语言类型</code></pre><hr><h6 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h6><pre><code>表头|条目一|条目二:---:|:---:|:---:项目|项目一|项目二注意：第二行分割表头和内容。- 有一个就行，为了对齐，多加了几个文字默认居左-两边加：表示文字居中-右边加：表示文字居右</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markDown-使用 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
